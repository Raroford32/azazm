
================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_amd_platform.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/amd_platform.html"
title: "12. How to get best performance on AMD platform — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html)
- 12\. How to get best performance on AMD platform
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/amd_platform.rst.txt)

* * *

# 12\. How to get best performance on AMD platform

This document provides a detailed, step-by-step guide
on configuring AMD EPYC System-on-Chip (SoC) for optimal performance
in DPDK applications across different SoC families.

The overall performance is influenced by factors such as BIOS settings,
NUMA per socket configuration, memory per NUMA allocation,
and proximity to IO devices.

These are covered in various sections of tuning guides shared below.

## 12.1. Tuning Guides for AMD EPYC SoC

1. [MILAN](https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/tuning-guides/data-plane-development-kit-tuning-guide-amd-epyc7003-series-processors.pdf)

2. [GENOA](https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/tuning-guides/58017-amd-epyc-9004-tg-data-plane-dpdk.pdf)

3. [BERGAMO\|SIENNA](https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/tuning-guides/58310_amd-epyc-8004-tg-data-plane-dpdk.pdf)


## 12.2. General Requirements

### 12.2.1. Memory

Refer to the Memory Configuration section for specific details related to the System-on-Chip (SoC).

Note

As a general guideline, it is recommended to populate
at least one memory DIMM in each memory channel.
The optimal memory size for each DIMM is at least 8, 16, or 32 GB,
utilizing ECC modules.

## 12.3. BIOS

Refer to the BIOS Performance section in tuning guide for recommended settings.

## 12.4. Linux GRUB

Refer to the Linux OS & Kernel in tuning guide for recommended settings.

## 12.5. NIC and Accelerator

AMD EPYC supports PCIe Generation of 1\|2\|3\|4\|5 depending upon SoC families.
For best performance ensure the right slots are used which provides adequate bandwidth.

Use `lspci` to check the speed of a PCI slot:

```
lspci -s 41:00.0 -vv | grep LnkSta

LnkSta: Speed 16GT/s, Width x16, TrErr- Train- SlotClk+ DLActive- ...
LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete+ ...

```

## 12.6. Compiler

Refer to the Compiler Flags in tuning guide for recommended version and -march flags.

## 12.7. Max LCores

Refer to the Compilation in tuning guide for allowing more threads to run as DPDK lcores.

## 12.8. Power

### 12.8.1. Core

AMD EPYC SoC supports CPU power functions via `rte_power` API from 23.11 LTS onwards.
These are tested and validated on MILAN, GENOA, BERGAMO and SIENA
using Linux kernel 6.4 and above with `amd_pstate` power driver.

Note

- Power libraries are supported on Linux only.

- DPDK uncore support on Linux is work in progress.


================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_build_dpdk.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/build_dpdk.html"
title: "3. Compiling the DPDK Target from Source — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html)
- 3\. Compiling the DPDK Target from Source
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/build_dpdk.rst.txt)

* * *

# 3\. Compiling the DPDK Target from Source

## 3.1. Uncompress DPDK and Browse Sources

First, uncompress the archive and move to the uncompressed DPDK source directory:

```
tar xJf dpdk-<version>.tar.xz
cd dpdk-<version>

```

The DPDK is composed of several directories, including:

- doc: DPDK Documentation

- license: DPDK license information

- lib: Source code of DPDK libraries

- drivers: Source code of DPDK poll-mode drivers

- app: Source code of DPDK applications (automatic tests)

- examples: Source code of DPDK application examples

- config, buildtools: Framework-related scripts and configuration

- usertools: Utility scripts for end-users of DPDK applications

- devtools: Scripts for use by DPDK developers

- kernel: Kernel modules needed for some operating systems


## 3.2. Compiling and Installing DPDK System-wide

DPDK can be configured, built and installed on your system using the tools
`meson` and `ninja`.

### 3.2.1. DPDK Configuration

To configure a DPDK build use:

```
meson setup <options> build

```

where “build” is the desired output build directory, and “<options>” can be
empty or one of a number of meson or DPDK-specific build options, described
later in this section. The configuration process will finish with a summary
of what DPDK libraries and drivers are to be built and installed, and for
each item disabled, a reason why that is the case. This information can be
used, for example, to identify any missing required packages for a driver.

Once configured, to build and then install DPDK system-wide use:

```
cd build
ninja
meson install
ldconfig

```

The last two commands above generally need to be run as root,
with the meson install step copying the built objects to their final system-wide locations,
and the last step causing the dynamic loader ld.so to update its cache to take account of the new objects.

Note

On some linux distributions, such as Fedora or Redhat, paths in /usr/local are
not in the default paths for the loader. Therefore, on these
distributions, /usr/local/lib and /usr/local/lib64 should be added
to a file in /etc/ld.so.conf.d/ before running ldconfig.

### 3.2.2. Adjusting Build Options

DPDK has a number of options that can be adjusted as part of the build configuration process.
These options can be listed by running `meson configure` inside a configured build folder.
Many of these options come from the “meson” tool itself and can be seen documented on the
[Meson Website](https://mesonbuild.com/Builtin-options.html).

For example, to change the build-type from the default, “debugoptimized”,
to a regular “debug” build, you can either:

- pass `-Dbuildtype=debug` or `--buildtype=debug` to meson when configuring the build folder initially

- run `meson configure -Dbuildtype=debug` inside the build folder after the initial meson run.


Other options are specific to the DPDK project but can be adjusted similarly.
The “platform” option specifies a set a configuration parameters that will be used.
The valid values are:

- `-Dplatform=native` will tailor the configuration to the build machine.

- `-Dplatform=generic` will use configuration that works on all machines
of the same architecture as the build machine.

- `-Dplatform=<SoC>` will use configuration optimized for a particular SoC.
Consult the “socs” dictionary in `config/arm/meson.build` to see which
SoCs are supported.


The instruction set will be set automatically by default according to these rules:

- `-Dplatform=native` sets `cpu_instruction_set` to `native`,
which configures `-march` (x86\_64), `-mcpu` (ppc), `-mtune` (ppc) to `native`.

- `-Dplatform=generic` sets `cpu_instruction_set` to `generic`,
which configures `-march` (x86\_64), `-mcpu` (ppc), `-mtune` (ppc) to
a common minimal baseline needed for DPDK.


To override what instruction set will be used, set the `cpu_instruction_set`
parameter to the instruction set of your choice (such as `corei7`, `power8`, etc.).

`cpu_instruction_set` is not used in Arm builds, as setting the instruction set
without other parameters leads to inferior builds. The way to tailor Arm builds
is to build for a SoC using `-Dplatform=<SoC>` mentioned above.

The values determined by the `platform` parameter may be overwritten.
For example, to set the `max_lcores` value to 256, you can either:

- pass `-Dmax_lcores=256` to meson when configuring the build folder initially

- run `meson configure -Dmax_lcores=256` inside the build folder after the initial meson run.


Some of the DPDK sample applications in the examples directory can be
automatically built as part of a meson build too.
To do so, pass a comma-separated list of the examples to build to the
-Dexamples meson option as below:

```
meson setup -Dexamples=l2fwd,l3fwd build

```

As with other meson options, this can also be set post-initial-config using meson configure in the build directory.
There is also a special value “all” to request that all example applications whose
dependencies are met on the current system are built.
When -Dexamples=all is set as a meson option, meson will check each example application to see if it can be built,
and add all which can be built to the list of tasks in the ninja build configuration file.

### 3.2.3. Building 32-bit DPDK on 64-bit Systems

To build a 32-bit copy of DPDK on a 64-bit OS,
the `-m32` flag should be passed to the compiler and linker
to force the generation of 32-bit objects and binaries.
This can be done either by setting `CFLAGS` and `LDFLAGS` in the environment,
or by passing the value to meson using `-Dc_args=-m32` and `-Dc_link_args=-m32`.
For correctly identifying and using any dependency packages,
the `pkg-config` tool must also be configured
to look in the appropriate directory for .pc files for 32-bit libraries.
This is done by setting `PKG_CONFIG_LIBDIR` to the appropriate path.

The following meson command can be used on RHEL/Fedora systems to configure a 32-bit build,
assuming the relevant 32-bit development packages, such as a 32-bit libc, are installed:

```
PKG_CONFIG_LIBDIR=/usr/lib/pkgconfig \
    meson setup -Dc_args='-m32' -Dc_link_args='-m32' build

```

For Debian/Ubuntu systems, the equivalent command is:

```
PKG_CONFIG_LIBDIR=/usr/lib/i386-linux-gnu/pkgconfig \
    meson setup -Dc_args='-m32' -Dc_link_args='-m32' build

```

Once the build directory has been configured,
DPDK can be compiled using `ninja` as described above.

### 3.2.4. Building Applications Using Installed DPDK

When installed system-wide, DPDK provides a pkg-config file `libdpdk.pc` for applications to query as part of their build.
It’s recommended that the pkg-config file be used, rather than hard-coding the parameters (cflags/ldflags)
for DPDK into the application build process.

An example of how to query and use the pkg-config file can be found in the `Makefile` of each of the example applications included with DPDK.
A simplified example snippet is shown below, where the target binary name has been stored in the variable `$(APP)`
and the sources for that build are stored in `$(SRCS-y)`.

```
PKGCONF = pkg-config

CFLAGS += -O3 $(shell $(PKGCONF) --cflags libdpdk)
LDFLAGS += $(shell $(PKGCONF) --libs libdpdk)

$(APP): $(SRCS-y) Makefile
        $(CC) $(CFLAGS) $(SRCS-y) -o $@ $(LDFLAGS)

```

Note

Unlike with the make build system present in older DPDK releases,
the meson system is not
designed to be used directly from a build directory. Instead it is
recommended that it be installed either system-wide or to a known
location in the user’s home directory. The install location can be set
using the –prefix meson option (default: /usr/local).

an equivalent build recipe for a simple DPDK application using meson as a
build system is shown below:

```
project('dpdk-app', 'c')

dpdk = dependency('libdpdk')
sources = files('main.c')
executable('dpdk-app', sources, dependencies: dpdk)

```


================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_build_sample_apps.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/build_sample_apps.html"
title: "8. Running Sample Applications — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html)
- 8\. Running Sample Applications
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/build_sample_apps.rst.txt)

* * *

# 8\. Running Sample Applications

The chapter describes how to compile and run applications in a DPDK environment.
It also provides a pointer to where sample applications are stored.

## 8.1. Compiling a Sample Application

Please refer to [Building Applications Using Installed DPDK](https://doc.dpdk.org/guides-25.03/linux_gsg/build_dpdk.html#building-app-using-installed-dpdk) for detail on compiling sample apps.

## 8.2. Running a Sample Application

Warning

Before running the application make sure:

- Hugepages setup is done.

- Any kernel driver being used is loaded.

- In case needed, ports being used by the application should be
bound to the corresponding kernel driver.


refer to [Linux Drivers](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#linux-gsg-linux-drivers) for more details.

The application is linked with the DPDK target environment’s Environmental Abstraction Layer (EAL) library,
which provides some options that are generic to every DPDK application.

The following is the list of options that can be given to the EAL:

```
./rte-app [-c COREMASK | -l CORELIST] [-n NUM] [-b <domain:bus:devid.func>] \
          [--socket-mem=MB,...] [-d LIB.so|DIR] [-m MB] [-r NUM] [-v] [--file-prefix] \
          [--proc-type <primary|secondary|auto>]

```

The EAL options are as follows:

- `-c COREMASK` or `-l CORELIST`:
An hexadecimal bit mask of the cores to run on. Note that core numbering can
change between platforms and should be determined beforehand. The corelist is
a set of core numbers instead of a bitmap core mask.

- `-n NUM`:
Number of memory channels per processor socket.

- `-b <domain:bus:devid.func>`:
Blocklisting of ports; prevent EAL from using specified PCI device
(multiple `-b` options are allowed).

- `--use-device`:
use the specified Ethernet device(s) only. Use comma-separate
`[domain:]bus:devid.func` values. Cannot be used with `-b` option.

- `--socket-mem`:
Memory to allocate from hugepages on specific sockets. In dynamic memory mode,
this memory will also be pinned (i.e. not released back to the system until
application closes).

- `--socket-limit`:
Limit maximum memory available for allocation on each socket. Does not support
legacy memory mode.

- `-d`:
Add a driver or driver directory to be loaded.
The application should use this option to load the PMDs
that are built as shared libraries.

- `-m MB`:
Memory to allocate from hugepages, regardless of processor socket. It is
recommended that `--socket-mem` be used instead of this option.

- `-r NUM`:
Number of memory ranks.

- `-v`:
Display version information on startup.

- `--huge-dir`:
The directory where hugetlbfs is mounted.

- `--mbuf-pool-ops-name`:
Pool ops name for mbuf to use.

- `--file-prefix`:
The prefix text used for hugepage filenames.

- `--proc-type`:
The type of process instance.

- `--vmware-tsc-map`:
Use VMware TSC map instead of native RDTSC.

- `--base-virtaddr`:
Specify base virtual address.

- `--vfio-intr`:
Specify interrupt type to be used by VFIO (has no effect if VFIO is not used).

- `--legacy-mem`:
Run DPDK in legacy memory mode (disable memory reserve/unreserve at runtime,
but provide more IOVA-contiguous memory).

- `--single-file-segments`:
Store memory segments in fewer files (dynamic memory mode only - does not
affect legacy memory mode).


The `-c` or `-l` and option is mandatory; the others are optional.

Copy the DPDK application binary to your target, then run the application as follows
(assuming the platform has four memory channels per processor socket,
and that cores 0-3 are present and are to be used for running the application):

```
./dpdk-helloworld -l 0-3 -n 4

```

Note

The `--proc-type` and `--file-prefix` EAL options are used for running
multiple DPDK processes. See the “Multi-process Sample Application”
chapter in the _DPDK Sample Applications User Guide_ and the _DPDK_
_Programmers Guide_ for more details.

### 8.2.1. Logical Core Use by Applications

The coremask (-c 0x0f) or corelist (-l 0-3) parameter is always mandatory for DPDK applications.
Each bit of the mask corresponds to the equivalent logical core number as reported by Linux. The preferred corelist option is a cleaner method to define cores to be used.
Since these logical core numbers, and their mapping to specific cores on specific NUMA sockets, can vary from platform to platform,
it is recommended that the core layout for each platform be considered when choosing the coremask/corelist to use in each case.

On initialization of the EAL layer by a DPDK application, the logical cores to be used and their socket location are displayed.
This information can also be determined for all cores on the system by examining the `/proc/cpuinfo` file, for example, by running cat `/proc/cpuinfo`.
The physical id attribute listed for each processor indicates the CPU socket to which it belongs.
This can be useful when using other processors to understand the mapping of the logical cores to the sockets.

Note

A more graphical view of the logical core layout
may be obtained using the `lstopo` Linux utility.
On Fedora, this may be installed and run using the following commands:

```
sudo yum install hwloc
lstopo

```

This command produces a quite short textual output:

```
lstopo-no-graphics --merge

```

Warning

The logical core layout can change between different board layouts and should be checked before selecting an application coremask/corelist.

### 8.2.2. Hugepage Memory Use by Applications

When running an application, it is recommended to use the same amount of memory as that allocated for hugepages.
This is done automatically by the DPDK application at startup,
if no `-m` or `--socket-mem` parameter is passed to it when run.

If more memory is requested by explicitly passing a `-m` or `--socket-mem` value, the application fails.
However, the application itself can also fail if the user requests less memory than the reserved amount of hugepage-memory, particularly if using the `-m` option.
The reason is as follows.
Suppose the system has 1024 reserved 2 MB pages in socket 0 and 1024 in socket 1.
If the user requests 128 MB of memory, the 64 pages may not match the constraints:

- The hugepage memory by be given to the application by the kernel in socket 1 only.
In this case, if the application attempts to create an object, such as a ring or memory pool in socket 0, it fails.
To avoid this issue, it is recommended that the `--socket-mem` option be used instead of the `-m` option.

- These pages can be located anywhere in physical memory, and, although the DPDK EAL will attempt to allocate memory in contiguous blocks,
it is possible that the pages will not be contiguous. In this case, the application is not able to allocate big memory pools.


The socket-mem option can be used to request specific amounts of memory for specific sockets.
This is accomplished by supplying the `--socket-mem` flag followed by amounts of memory requested on each socket,
for example, supply `--socket-mem=0,512` to try and reserve 512 MB for socket 1 only.
Similarly, on a four socket system, to allocate 1 GB memory on each of sockets 0 and 2 only, the parameter `--socket-mem=1024,0,1024` can be used.
No memory will be reserved on any CPU socket that is not explicitly referenced, for example, socket 3 in this case.
If the DPDK cannot allocate enough memory on each socket, the EAL initialization fails.

Whether hugepages are included in core dump is controlled by `/proc/<pid>/coredump_filter`.
It is `33` (hexadecimal) by default, which means that hugepages are excluded from core dump.
This setting is per-process and is inherited.
Refer to `core(5)` for details.
To include mapped hugepages in core dump, set bit 6 ( `0x40`) in the parent process
or shell before running a DPDK application:

```
echo 0x73 > /proc/self/coredump_filter
./dpdk-application ...

```

Note

Including hugepages in core dump file increases its size,
which may fill the storage or overload the transport.
Hugepages typically hold data processed by the application,
like network packets, which may contain sensitive information.

## 8.3. Additional Sample Applications

Additional sample applications are included in the DPDK examples directory.
These sample applications may be built and run in a manner similar to that described in earlier sections in this manual.
In addition, see the _DPDK Sample Applications User Guide_ for a description of the application,
specific instructions on compilation and execution and some explanation of the code.


================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_cross_build_dpdk_for_arm64.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_arm64.html"
title: "4. Cross compiling DPDK for aarch64 and aarch32 — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html)
- 4\. Cross compiling DPDK for aarch64 and aarch32
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/cross_build_dpdk_for_arm64.rst.txt)

* * *

# 4\. Cross compiling DPDK for aarch64 and aarch32

This chapter describes how to cross compile DPDK for aarch64 on x86 build
machine and compile 32-bit aarch32 DPDK on aarch64 build machine.

Note

Whilst it is recommended to natively build DPDK on aarch64 (just
like with x86), it is also possible to cross compile DPDK for aarch64.
An aarch64 cross compiler GNU toolchain or an LLVM/clang toolchain
may be used for cross-compilation.

## 4.1. Prerequisites

### 4.1.1. NUMA library

NUMA is required by most modern machines, not needed for non-NUMA architectures.

Note

For compiling the NUMA lib, run libtool –version to ensure the libtool version >= 2.2,
otherwise the compilation will fail with errors.

```
git clone https://github.com/numactl/numactl.git
cd numactl
git checkout v2.0.13 -b v2.0.13
./autogen.sh
autoconf -i
./configure --host=aarch64-linux-gnu CC=aarch64-none-linux-gnu-gcc --prefix=<numa install dir>
make install

```

Note

The compiler is `aarch64-none-linux-gnu-gcc` if you download GCC
using the below guide. If you’re using a different compiler,
make sure you’re using the proper executable name.

The numa header files and lib file is generated in the include and lib folder
respectively under `<numa install dir>`.

### 4.1.2. Meson prerequisites

Meson depends on pkgconfig to find the dependencies.
The package `pkg-config-aarch64-linux-gnu` is required for aarch64.
To install it in Ubuntu:

```
sudo apt install pkg-config-aarch64-linux-gnu

```

For aarch32, install `pkg-config-arm-linux-gnueabihf`:

```
sudo apt install pkg-config-arm-linux-gnueabihf

```

## 4.2. GNU toolchain

### 4.2.1. Get the cross toolchain

The latest GNU cross compiler toolchain can be downloaded from:
[https://developer.arm.com/open-source/gnu-toolchain/gnu-a/downloads](https://developer.arm.com/open-source/gnu-toolchain/gnu-a/downloads).

It is always recommended to check and get the latest compiler tool
from the page and use it to generate better code.
As of this writing 9.2-2019.12 is the newest,
the following description is an example of this version.

For aarch64:

```
wget https://developer.arm.com/-/media/Files/downloads/gnu-a/9.2-2019.12/binrel/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu.tar.xz
tar -xvf gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu.tar.xz
export PATH=$PATH:<cross_install_dir>/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin

```

For aarch32:

```
wget https://developer.arm.com/-/media/Files/downloads/gnu-a/9.2-2019.12/binrel/gcc-arm-9.2-2019.12-x86_64-arm-none-linux-gnueabihf.tar.xz
tar -xvf gcc-arm-9.2-2019.12-x86_64-arm-none-linux-gnueabihf.tar.xz
export PATH=$PATH:<cross_install_dir>/gcc-arm-9.2-2019.12-x86_64-arm-none-linux-gnueabihf/bin

```

Note

For the host requirements and other info, refer to the release note section:
[https://releases.linaro.org/components/toolchain/binaries/](https://releases.linaro.org/components/toolchain/binaries/)

### 4.2.2. Augment the GNU toolchain with NUMA support

Copy the NUMA header files and lib to the cross compiler’s directories:

```
cp <numa_install_dir>/include/numa*.h <cross_install_dir>/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/libc/usr/include/
cp <numa_install_dir>/lib/libnuma.a <cross_install_dir>/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/lib/gcc/aarch64-none-linux-gnu/9.2.1/
cp <numa_install_dir>/lib/libnuma.so <cross_install_dir>/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/lib/gcc/aarch64-none-linux-gnu/9.2.1/

```

Note

Using LDFLAGS and CFLAGS is not a viable alternative to copying the files.
The Meson docs say it is not recommended, as there are many caveats
to their use with Meson, especially when rebuilding the project.
A viable alternative would be to use the `c_args` and `c_link_args`
options with Meson 0.51.0 and higher:

```
-Dc_args=-I<numa_install_dir>/include -Dc_link_args=-L<numa_install_dir>/lib

```

For Meson versions lower than 0.51.0, the `c_args` and `c_link_args`
options do not apply to cross compilation.
However, the compiler/linker flags may be added to cross files under \[properties\]:

```
c_args = ['-I<numa_install_dir>/include']
c_link_args = ['-L<numa_install_dir>/lib']

```

### 4.2.3. Cross Compiling DPDK with GNU toolchain using Meson

Note

The names of GCC binaries in cross files differ from the downloaded ones,
which have an extra `-none-` in their name.
Please modify the cross file binaries accordingly
when using the downloaded cross compilers.

An example cross file with modified names and added NUMA paths
would look like this:

```
[binaries]
c = 'aarch64-none-linux-gnu-gcc'
cpp = 'aarch64-none-linux-gnu-cpp'
ar = 'aarch64-none-linux-gnu-gcc-ar'
strip = 'aarch64-none-linux-gnu-strip'
pkgconfig = 'aarch64-linux-gnu-pkg-config' # the downloaded binaries
   # do not contain a pkgconfig binary, so it is not modified
pcap-config = ''

[host_machine]
system = 'linux'
cpu_family = 'aarch64'
cpu = 'armv8-a'
endian = 'little'

[properties]
# Generate binaries that are portable across all Armv8 machines
platform = 'generic'
c_args = ['-I<numa_install_dir>/include']  # replace <numa_install_dir>
c_link_args = ['-L<numa_install_dir>/lib'] # with your path

```

To cross-compile DPDK on a desired target machine we can use the following
command:

```
meson setup cross-build --cross-file <target_machine_configuration>
ninja -C cross-build

```

For example if the target machine is aarch64 we can use the following
command, provided the cross file has been modified accordingly:

```
meson setup aarch64-build-gcc --cross-file config/arm/arm64_armv8_linux_gcc
ninja -C aarch64-build-gcc

```

If the target machine is aarch32 we can use the following command,
provided the cross file has been modified accordingly:

```
meson setup aarch32-build --cross-file config/arm/arm32_armv8_linux_gcc
ninja -C aarch32-build

```

## 4.3. LLVM/Clang toolchain

### 4.3.1. Obtain the cross tool chain

The latest LLVM/Clang cross compiler toolchain can be downloaded from:
[https://developer.arm.com/tools-and-software/open-source-software/developer-tools/llvm-toolchain](https://developer.arm.com/tools-and-software/open-source-software/developer-tools/llvm-toolchain).

```
# Ubuntu binaries
wget https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz

```

The LLVM/Clang toolchain does not implement the standard c library.
The GNU toolchain ships an implementation we can use.
Refer to [obtain\_GNU\_toolchain](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_arm64.html#obtain-gnu-toolchain) to get the GNU toolchain.

### 4.3.2. Unzip and add into the PATH

```
tar -xvf clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04.tar.xz
export PATH=$PATH:<cross_install_dir>/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/bin

```

### 4.3.3. Cross Compiling DPDK with LLVM/Clang toolchain using Meson

Note

To use the NUMA library follow the same steps as for
[augment\_the\_gnu\_toolchain\_with\_numa\_support](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_arm64.html#augment-the-gnu-toolchain-with-numa-support).

The paths to GNU stdlib must be specified in a cross file.
Augmenting the default cross-file’s `c_args` and `c_link_args` `config/arm/arm64_armv8_linux_clang_ubuntu1804` would look like this:

```
...
c_args = ['-target', 'aarch64-linux-gnu', '--sysroot', '<cross_install_dir>/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/libc']
c_link_args = ['-target', 'aarch64-linux-gnu', '-fuse-ld=lld', '--sysroot', '<cross_install_dir>/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/libc', '--gcc-toolchain=<cross_install_dir>/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu']

```

Assuming the file with augmented `c_args` and `c_link_args`
is named `arm64_armv8_linux_clang`,
use the following command to cross-compile DPDK for the target machine:

```
meson setup aarch64-build-clang --cross-file config/arm/arm64_armv8_linux_clang
ninja -C aarch64-build-clang

```

### 4.3.4. Cross Compiling DPDK with LLVM/Clang toolchain using Meson on Ubuntu 18.04

On most popular Linux distribution it is not necessary to download
the toolchains, but rather use the packages provided by said distributions.
On Ubuntu 18.04, these packages are needed:

```
sudo apt-get install pkg-config-aarch64-linux-gnu clang llvm llvm-dev lld
libc6-dev-arm64-cross libatomic1-arm64-cross libgcc-8-dev-arm64-cross

```

Use the following command to cross-compile DPDK for the target machine:

```
meson setup aarch64-build-clang --cross-file config/arm/arm64_armv8_linux_clang_ubuntu1804
ninja -C aarch64-build-clang

```

## 4.4. Building for an aarch64 SoC on an aarch64 build machine

If you wish to build on an aarch64 build machine for a different aarch64 SoC,
you don’t need a separate cross toolchain, just a different set of
configuration options. To build for an aarch64 SoC, use the -Dplatform meson
option:

```
meson setup soc_build -Dplatform=<target_soc>

```

Substitute <target\_soc> with one of the supported SoCs

```
generic:         Generic un-optimized build for armv8 aarch64 execution mode.
generic_aarch32: Generic un-optimized build for armv8 aarch32 execution mode.
altra:           Ampere Altra/AltraMax
ampereone:       Ampere AmpereOne
ampereoneac04:   Ampere AmpereOneAC04
armada:          Marvell ARMADA
bluefield:       NVIDIA BlueField
bluefield3:      NVIDIA BlueField-3
capri:           AMD Pensando Capri
cdx:             AMD CDX
centriq2400:     Qualcomm Centriq 2400
cn9k:            Marvell OCTEON 9
cn10k:           Marvell OCTEON 10
dpaa:            NXP DPAA
elba:            AMD Pensando Elba
emag:            Ampere eMAG
ft2000plus:      Phytium FT-2000+
grace:           NVIDIA Grace
graviton2:       AWS Graviton2
graviton3:       AWS Graviton3
graviton4:       AWS Graviton4
hip10:           HiSilicon HIP10
kunpeng920:      HiSilicon Kunpeng 920
kunpeng930:      HiSilicon Kunpeng 930
n1sdp:           Arm Neoverse N1SDP
n2:              Arm Neoverse N2
odyssey:         Marvell Odyssey
stingray:        Broadcom Stingray
thunderx2:       Marvell ThunderX2 T99
thunderxt83:     Marvell ThunderX T83
thunderxt88:     Marvell ThunderX T88
tys2500:         Phytium TengYun S2500
tys5000c:        Phytium TengYun S5000c
v2:              Arm Neoverse V2

```

These SoCs are also used in cross files, e.g.:

```
[properties]
# Generate binaries that are portable across all Armv8 machines
platform = 'generic'

```

## 4.5. Supported SoC configuration

The SoC configuration is a combination of implementer and CPU part number
configuration and SoC-specific configuration:

```
soc_<name> = {
   'description': 'SoC Description',  # mandatory
   'implementer': <implementer_id>,   # mandatory
   'part_number': <part_number>,      # mandatory
   'numa': false,  # optional, specify for non-NUMA SoCs
   'enable_drivers': 'common/*,bus/*',  # optional, comma-separated list of
                           # drivers to build, wildcards are accepted
   'disable_drivers': 'crypto/*',       # optional, comma-separated list of
                           # drivers to disable, wildcards are accepted
   'flags': [\
      ['RTE_MAX_LCORE', '16'],\
      ['RTE_MAX_NUMA_NODES', '1']\
   ]               # optional, list of DPDK options that will be added
                   # or overwritten
}

```

Where <implementer\_id> is a key defined in the implementers dictionary
in config/arm/meson.build (e.g. 0x41) and part\_number is a key defined
in implementers\[<implementer\_id>\]\[‘part\_number\_config’\] dictionary
(i.e. the part number must be defined for the implementer,
e.g. for 0x41, a valid value is 0xd49, which is the neoverse-n2 SoC).


================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_enable_func.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/enable_func.html"
title: "10. Enabling Additional Functionality — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html)
- 10\. Enabling Additional Functionality
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/enable_func.rst.txt)

* * *

# 10\. Enabling Additional Functionality

## 10.1. Running DPDK Applications Without Root Privileges

The following sections describe generic requirements and configuration
for running DPDK applications as non-root.
There may be additional requirements documented for some drivers.

### 10.1.1. Hugepages

Hugepages must be reserved as root before running the application as non-root,
for example:

```
sudo dpdk-hugepages.py --reserve 1G

```

If multi-process is not required, running with `--in-memory`
bypasses the need to access hugepage mount point and files within it.
Otherwise, hugepage directory must be made accessible
for writing to the unprivileged user.
A good way for managing multiple applications using hugepages
is to mount the filesystem with group permissions
and add a supplementary group to each application or container.

One option is to use the script provided by this project:

```
export HUGEDIR=$HOME/huge-1G
mkdir -p $HUGEDIR
sudo dpdk-hugepages.py --mount --directory $HUGEDIR --user `id -u` --group `id -g`

```

In production environment, the OS can manage mount points
( [systemd example](https://github.com/systemd/systemd/blob/main/units/dev-hugepages.mount)).

The `hugetlb` filesystem has additional options to guarantee or limit
the amount of memory that is possible to allocate using the mount point.
Refer to the [documentation](https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt).

Note

Using `vfio-pci` kernel driver, if applicable, can eliminate the need
for physical addresses and therefore eliminate the permission requirements
described below.

If the driver requires using physical addresses (PA),
the executable file must be granted additional capabilities:

- `DAC_READ_SEARCH` and `SYS_ADMIN` to read `/proc/self/pagemaps`

- `IPC_LOCK` to lock hugepages in memory


```
setcap cap_dac_read_search,cap_ipc_lock,cap_sys_admin+ep <executable>

```

If physical addresses are not accessible,
the following message will appear during EAL initialization:

```
EAL: rte_mem_virt2phy(): cannot open /proc/self/pagemap: Permission denied

```

It is harmless in case PA are not needed.

### 10.1.2. Resource Limits

When running as non-root user, there may be some additional resource limits
that are imposed by the system. Specifically, the following resource limits may
need to be adjusted in order to ensure normal DPDK operation:

- RLIMIT\_LOCKS (number of file locks that can be held by a process)

- RLIMIT\_NOFILE (number of open file descriptors that can be held open by a process)

- RLIMIT\_MEMLOCK (amount of pinned pages the process is allowed to have)


The above limits can usually be adjusted by editing
`/etc/security/limits.conf` file, and rebooting.

See [Hugepage Mapping](https://doc.dpdk.org/guides-25.03/prog_guide/env_abstraction_layer.html#hugepage-mapping) section to learn how these limits affect EAL.

### 10.1.3. Device Control

If the HPET is to be used, `/dev/hpet` permissions must be adjusted.

For `vfio-pci` kernel driver, the following Linux file system objects’
permissions should be adjusted:

- The VFIO device file, `/dev/vfio/vfio`

- The directories under `/dev/vfio` that correspond to IOMMU group numbers of
devices intended to be used by DPDK, for example, `/dev/vfio/50`


## 10.2. Power Management and Power Saving Functionality

Enhanced Intel SpeedStep® Technology must be enabled in the platform BIOS if the power management feature of DPDK is to be used.
Otherwise, the sys file folder `/sys/devices/system/cpu/cpu0/cpufreq` will not exist, and the CPU frequency- based power management cannot be used.
Consult the relevant BIOS documentation to determine how these settings can be accessed.

For example, on some Intel reference platform BIOS variants, the path to Enhanced Intel SpeedStep® Technology is:

```
Advanced
  -> Processor Configuration
  -> Enhanced Intel SpeedStep\ |reg| Tech

```

In addition, C3 and C6 should be enabled as well for power management. The path of C3 and C6 on the same platform BIOS is:

```
Advanced
  -> Processor Configuration
  -> Processor C3 Advanced
  -> Processor Configuration
  -> Processor C6

```

## 10.3. Using Linux Core Isolation to Reduce Context Switches

While the threads used by a DPDK application are pinned to logical cores on the system,
it is possible for the Linux scheduler to run other tasks on those cores.
To help prevent additional workloads, timers, RCU processing and IRQs
from running on those cores, it is possible to use
the Linux kernel parameters `isolcpus`, `nohz_full`, `irqaffinity`
to isolate them from the general Linux scheduler tasks.

For example, if a given CPU has 0-7 cores
and DPDK applications are to run on logical cores 2, 4 and 6,
the following should be added to the kernel parameter list:

```
isolcpus=2,4,6 nohz_full=2,4,6 irqaffinity=0,1,3,5,7

```

Note

More detailed information about the above parameters can be found at
[NO\_HZ](https://www.kernel.org/doc/html/latest/timers/no_hz.html),
[IRQ](https://www.kernel.org/doc/html/latest/core-api/irq/),
and [kernel parameters](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html)

For more fine grained control over resource management and performance tuning
one can look into “Linux cgroups”,
[cpusets](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/cpusets.html),
[cpuset man pages](https://man7.org/linux/man-pages/man7/cpuset.7.html), and
[systemd CPU affinity](https://www.freedesktop.org/software/systemd/man/systemd.exec.html).

Also see
[CPU isolation example](https://www.suse.com/c/cpu-isolation-practical-example-part-5/)
and [systemd core isolation example](https://www.rcannings.com/systemd-core-isolation/).

## 10.4. High Precision Event Timer (HPET) Functionality

DPDK can support the system HPET as a timer source rather than the system default timers,
such as the core Time-Stamp Counter (TSC) on x86 systems.
To enable HPET support in DPDK:

1. Ensure that HPET is enabled in BIOS settings.

2. Enable `HPET_MMAP` support in kernel configuration.
Note that this my involve doing a kernel rebuild,
as many common linux distributions do _not_ have this setting
enabled by default in their kernel builds.

3. Enable DPDK support for HPET by using the build-time meson option `use_hpet`,
for example, `meson configure -Duse_hpet=true`


For an application to use the `rte_get_hpet_cycles()` and `rte_get_hpet_hz()` API calls,
and optionally to make the HPET the default time source for the rte\_timer library,
the `rte_eal_hpet_init()` API call should be called at application initialization.
This API call will ensure that the HPET is accessible,
returning an error to the application if it is not.

For applications that require timing APIs, but not the HPET timer specifically,
it is recommended that the `rte_get_timer_cycles()` and `rte_get_timer_hz()`
API calls be used instead of the HPET-specific APIs.
These generic APIs can work with either TSC or HPET time sources,
depending on what is requested by an application call to `rte_eal_hpet_init()`,
if any, and on what is available on the system at runtime.


================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_index.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/index.html"
title: "Getting Started Guide for Linux — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- Getting Started Guide for Linux
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/index.rst.txt)

* * *

# Getting Started Guide for Linux

- [1\. Introduction](https://doc.dpdk.org/guides-25.03/linux_gsg/intro.html)
  - [1.1. Documentation Roadmap](https://doc.dpdk.org/guides-25.03/linux_gsg/intro.html#documentation-roadmap)
- [2\. System Requirements](https://doc.dpdk.org/guides-25.03/linux_gsg/sys_reqs.html)
  - [2.1. BIOS Setting Prerequisite on x86](https://doc.dpdk.org/guides-25.03/linux_gsg/sys_reqs.html#bios-setting-prerequisite-on-x86)
  - [2.2. Compilation of the DPDK](https://doc.dpdk.org/guides-25.03/linux_gsg/sys_reqs.html#compilation-of-the-dpdk)
  - [2.3. Running DPDK Applications](https://doc.dpdk.org/guides-25.03/linux_gsg/sys_reqs.html#running-dpdk-applications)
- [3\. Compiling the DPDK Target from Source](https://doc.dpdk.org/guides-25.03/linux_gsg/build_dpdk.html)
  - [3.1. Uncompress DPDK and Browse Sources](https://doc.dpdk.org/guides-25.03/linux_gsg/build_dpdk.html#uncompress-dpdk-and-browse-sources)
  - [3.2. Compiling and Installing DPDK System-wide](https://doc.dpdk.org/guides-25.03/linux_gsg/build_dpdk.html#compiling-and-installing-dpdk-system-wide)
- [4\. Cross compiling DPDK for aarch64 and aarch32](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_arm64.html)
  - [4.1. Prerequisites](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_arm64.html#prerequisites)
  - [4.2. GNU toolchain](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_arm64.html#gnu-toolchain)
  - [4.3. LLVM/Clang toolchain](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_arm64.html#llvm-clang-toolchain)
  - [4.4. Building for an aarch64 SoC on an aarch64 build machine](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_arm64.html#building-for-an-aarch64-soc-on-an-aarch64-build-machine)
  - [4.5. Supported SoC configuration](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_arm64.html#supported-soc-configuration)
- [5\. Cross compiling DPDK for LoongArch](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_loongarch.html)
  - [5.1. Prerequisites](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_loongarch.html#prerequisites)
  - [5.2. GNU toolchain](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_loongarch.html#gnu-toolchain)
  - [5.3. Supported cross-compilation targets](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_loongarch.html#supported-cross-compilation-targets)
- [6\. Cross compiling DPDK for RISC-V](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_riscv.html)
  - [6.1. Prerequisites](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_riscv.html#prerequisites)
  - [6.2. GNU toolchain](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_riscv.html#gnu-toolchain)
  - [6.3. Supported cross-compilation targets](https://doc.dpdk.org/guides-25.03/linux_gsg/cross_build_dpdk_for_riscv.html#supported-cross-compilation-targets)
- [7\. Linux Drivers](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html)
  - [7.1. Binding and Unbinding Network Ports to/from the Kernel Modules](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#binding-and-unbinding-network-ports-to-from-the-kernel-modules)
  - [7.2. VFIO](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#vfio)
  - [7.3. VFIO Platform](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#vfio-platform)
  - [7.4. Bifurcated Driver](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#bifurcated-driver)
  - [7.5. UIO](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#uio)
- [8\. Running Sample Applications](https://doc.dpdk.org/guides-25.03/linux_gsg/build_sample_apps.html)
  - [8.1. Compiling a Sample Application](https://doc.dpdk.org/guides-25.03/linux_gsg/build_sample_apps.html#compiling-a-sample-application)
  - [8.2. Running a Sample Application](https://doc.dpdk.org/guides-25.03/linux_gsg/build_sample_apps.html#running-a-sample-application)
  - [8.3. Additional Sample Applications](https://doc.dpdk.org/guides-25.03/linux_gsg/build_sample_apps.html#additional-sample-applications)
- [9\. EAL parameters](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_eal_parameters.html)
  - [9.1. Common EAL parameters](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_eal_parameters.html#common-eal-parameters)
  - [9.2. Linux-specific EAL parameters](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_eal_parameters.html#linux-specific-eal-parameters)
- [10\. Enabling Additional Functionality](https://doc.dpdk.org/guides-25.03/linux_gsg/enable_func.html)
  - [10.1. Running DPDK Applications Without Root Privileges](https://doc.dpdk.org/guides-25.03/linux_gsg/enable_func.html#running-dpdk-applications-without-root-privileges)
  - [10.2. Power Management and Power Saving Functionality](https://doc.dpdk.org/guides-25.03/linux_gsg/enable_func.html#power-management-and-power-saving-functionality)
  - [10.3. Using Linux Core Isolation to Reduce Context Switches](https://doc.dpdk.org/guides-25.03/linux_gsg/enable_func.html#using-linux-core-isolation-to-reduce-context-switches)
  - [10.4. High Precision Event Timer (HPET) Functionality](https://doc.dpdk.org/guides-25.03/linux_gsg/enable_func.html#high-precision-event-timer-hpet-functionality)
- [11\. How to get best performance with NICs on Intel platforms](https://doc.dpdk.org/guides-25.03/linux_gsg/nic_perf_intel_platform.html)
  - [11.1. Hardware and Memory Requirements](https://doc.dpdk.org/guides-25.03/linux_gsg/nic_perf_intel_platform.html#hardware-and-memory-requirements)
  - [11.2. Configurations before running DPDK](https://doc.dpdk.org/guides-25.03/linux_gsg/nic_perf_intel_platform.html#configurations-before-running-dpdk)
- [12\. How to get best performance on AMD platform](https://doc.dpdk.org/guides-25.03/linux_gsg/amd_platform.html)
  - [12.1. Tuning Guides for AMD EPYC SoC](https://doc.dpdk.org/guides-25.03/linux_gsg/amd_platform.html#tuning-guides-for-amd-epyc-soc)
  - [12.2. General Requirements](https://doc.dpdk.org/guides-25.03/linux_gsg/amd_platform.html#general-requirements)
  - [12.3. BIOS](https://doc.dpdk.org/guides-25.03/linux_gsg/amd_platform.html#bios)
  - [12.4. Linux GRUB](https://doc.dpdk.org/guides-25.03/linux_gsg/amd_platform.html#linux-grub)
  - [12.5. NIC and Accelerator](https://doc.dpdk.org/guides-25.03/linux_gsg/amd_platform.html#nic-and-accelerator)
  - [12.6. Compiler](https://doc.dpdk.org/guides-25.03/linux_gsg/amd_platform.html#compiler)
  - [12.7. Max LCores](https://doc.dpdk.org/guides-25.03/linux_gsg/amd_platform.html#max-lcores)
  - [12.8. Power](https://doc.dpdk.org/guides-25.03/linux_gsg/amd_platform.html#power)


================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_intro.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/intro.html"
title: "1. Introduction — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html)
- 1\. Introduction
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/intro.rst.txt)

* * *

# 1\. Introduction

This document contains instructions for installing and configuring the Data Plane Development Kit (DPDK) software.
It is designed to get customers up and running quickly.
The document describes how to compile and run a DPDK application in a Linux application (linux) environment,
without going deeply into detail.

## 1.1. Documentation Roadmap

The following is a list of DPDK documents in the suggested reading order:

- [Release Notes](https://doc.dpdk.org/guides-25.03/rel_notes/index.html): Provides release-specific information, including supported
features, limitations, fixed issues, known issues and so on. Also, provides the
answers to frequently asked questions in FAQ format.

- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html) (this document): Describes how to install and
configure the DPDK; designed to get users up and running quickly with the
software.

- [Programmer’s Guide](https://doc.dpdk.org/guides-25.03/prog_guide/index.html): Describes:


  - The software architecture and how to use it (through examples),
    specifically in a Linux\* application (linux) environment

  - The content of the DPDK, the build system (including the commands
    that can be used to build the development kit and an application)
    and guidelines for porting an application

  - Optimizations used in the software and those that should be considered
    for new development


A glossary of terms is also provided.

- [API Reference](https://doc.dpdk.org/api/html/index.html): Provides detailed information about DPDK functions,
data structures and other programming constructs.

- [Sample Applications User Guides](https://doc.dpdk.org/guides-25.03/sample_app_ug/index.html): Describes a set of sample applications.
Each chapter describes a sample application that showcases specific functionality
and provides instructions on how to compile, run and use the sample application.

- **Driver Reference Guides**: Provides details on each driver inside a particular category.
Separate guides exist for each of:

  - [Baseband Device Drivers](https://doc.dpdk.org/guides-25.03/bbdevs/index.html)

  - [Compression Device Drivers](https://doc.dpdk.org/guides-25.03/compressdevs/index.html)

  - [Crypto Device Drivers](https://doc.dpdk.org/guides-25.03/cryptodevs/index.html)

  - [DMA Device Drivers](https://doc.dpdk.org/guides-25.03/dmadevs/index.html)

  - [Event Device Drivers](https://doc.dpdk.org/guides-25.03/eventdevs/index.html)

  - [General-Purpose Graphics Processing Unit Drivers](https://doc.dpdk.org/guides-25.03/gpus/index.html)

  - [Mempool Device Driver](https://doc.dpdk.org/guides-25.03/mempool/index.html)

  - [Network Interface Controller Drivers](https://doc.dpdk.org/guides-25.03/nics/index.html)

  - [Rawdev Drivers](https://doc.dpdk.org/guides-25.03/rawdevs/index.html)

  - [REGEX Device Drivers](https://doc.dpdk.org/guides-25.03/regexdevs/index.html)

  - [vDPA Device Drivers](https://doc.dpdk.org/guides-25.03/vdpadevs/index.html)


================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_linux_drivers.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html"
title: "7. Linux Drivers — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html)
- 7\. Linux Drivers
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/linux_drivers.rst.txt)

* * *

# 7\. Linux Drivers

Different PMDs may require different kernel drivers in order to work properly.
Depending on the PMD being used, a corresponding kernel driver should be loaded,
and network ports should be bound to that driver.

## 7.1. Binding and Unbinding Network Ports to/from the Kernel Modules

Note

PMDs which use the bifurcated driver should not be unbound from their kernel drivers.
This section is for PMDs which use the UIO or VFIO drivers.
See [Bifurcated Driver](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#bifurcated-driver) section for more details.

Note

It is recommended that `vfio-pci` be used as the kernel module for DPDK-bound ports in all cases.
If an IOMMU is unavailable, the `vfio-pci` can be used in [no-iommu](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#vfio-noiommu) mode.
If, for some reason, vfio is unavailable, then UIO-based modules, `igb_uio` and `uio_pci_generic` may be used.
See section [UIO](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#uio) for details.

Most devices require that the hardware to be used by DPDK be unbound from the kernel driver it uses,
and instead be bound to the `vfio-pci` kernel module before the application is run.
For such PMDs, any network ports or other hardware under Linux\* control will be ignored and cannot be used by the application.

To bind ports to the `vfio-pci` module
for DPDK use, or to return ports to Linux control,
a utility script called `dpdk-devbind.py` is provided in the `usertools` subdirectory.
This utility can be used to provide a view of the current state of the network ports on the system,
and to bind and unbind those ports from the different kernel modules,
including the VFIO and UIO modules.
The following are some examples of how the script can be used.
A full description of the script and its parameters can be obtained
by calling the script with the `--help` or `--usage` options.
Note that the UIO or VFIO kernel modules to be used,
should be loaded into the kernel before running the `dpdk-devbind.py` script.

Note

Due to the way VFIO works, there are certain limitations
to which devices can be used with VFIO.
Mainly it comes down to how IOMMU groups work.
Any Virtual Function device can usually be used with VFIO on its own,
but physical devices may require either all ports bound to VFIO,
or some of them bound to VFIO while others not being bound to anything at all.

If your device is behind a PCI-to-PCI bridge,
the bridge will then be part of the IOMMU group in which your device is in.
Therefore, the bridge driver should also be unbound from the bridge PCI device
for VFIO to work with devices behind the bridge.

Note

While any user can run the `dpdk-devbind.py` script
to view the status of the network ports,
binding or unbinding network ports requires root privileges.

To see the status of all network ports on the system:

```
./usertools/dpdk-devbind.py --status

Network devices using DPDK-compatible driver
============================================
0000:82:00.0 '82599EB 10-GbE NIC' drv=vfio-pci unused=ixgbe
0000:82:00.1 '82599EB 10-GbE NIC' drv=vfio-pci unused=ixgbe

Network devices using kernel driver
===================================
0000:04:00.0 'I350 1-GbE NIC' if=em0  drv=igb unused=vfio-pci *Active*
0000:04:00.1 'I350 1-GbE NIC' if=eth1 drv=igb unused=vfio-pci
0000:04:00.2 'I350 1-GbE NIC' if=eth2 drv=igb unused=vfio-pci
0000:04:00.3 'I350 1-GbE NIC' if=eth3 drv=igb unused=vfio-pci

Other network devices
=====================
<none>

```

To bind device `eth1`,\`\`04:00.1\`\`, to the `vfio-pci` driver:

```
./usertools/dpdk-devbind.py --bind=vfio-pci 04:00.1

```

or, alternatively,

```
./usertools/dpdk-devbind.py --bind=vfio-pci eth1

```

When specifying device ids, wildcards can be used for the final part of the address.
To restore device `82:00.0` and `82:00.1` to their original kernel binding:

```
./usertools/dpdk-devbind.py --bind=ixgbe 82:00.*

```

## 7.2. VFIO

VFIO is a robust and secure driver that relies on IOMMU protection.
To make use of VFIO, the `vfio-pci` module must be loaded:

```
sudo modprobe vfio-pci

```

VFIO kernel is usually present by default in all distributions,
however please consult your distributions documentation to make sure that is the case.

To make use of full VFIO functionality,
both kernel and BIOS must support and be configured
to use IO virtualization (such as Intel® VT-d).

Note

In most cases, specifying “iommu=on” as kernel parameter should be enough to
configure the Linux kernel to use IOMMU.

For proper operation of VFIO when running DPDK applications as a non-privileged user, correct permissions should also be set up.
For more information, please refer to [Running DPDK Applications Without Root Privileges](https://doc.dpdk.org/guides-25.03/linux_gsg/enable_func.html#running-without-root-privileges).

### 7.2.1. VFIO no-IOMMU mode

If there is no IOMMU available on the system, VFIO can still be used,
but it has to be loaded with an additional module parameter:

```
modprobe vfio enable_unsafe_noiommu_mode=1

```

Alternatively, one can also enable this option in an already loaded kernel module:

```
echo 1 > /sys/module/vfio/parameters/enable_unsafe_noiommu_mode

```

After that, VFIO can be used with hardware devices as usual.

Note

It may be required to unload all VFIO related-modules before probing
the module again with `enable_unsafe_noiommu_mode=1` parameter.

Warning

Since no-IOMMU mode forgoes IOMMU protection, it is inherently unsafe.
That said, it does make it possible for the user
to keep the degree of device access and programming that VFIO has,
in situations where IOMMU is not available.

### 7.2.2. VFIO Memory Mapping Limits

For DMA mapping of either external memory or hugepages, VFIO interface is used.
VFIO does not support partial unmap of once mapped memory. Hence DPDK’s memory is
mapped in hugepage granularity or system page granularity. Number of DMA
mappings is limited by kernel with user locked memory limit of a process (rlimit)
for system/hugepage memory. Another per-container overall limit applicable both
for external memory and system memory was added in kernel 5.1 defined by
VFIO module parameter `dma_entry_limit` with a default value of 64K.
When application is out of DMA entries, these limits need to be adjusted to
increase the allowed limit.

When `--no-huge` option is used,
the page size used is of smaller size of `4K` or `64K`
and we shall need to increase `dma_entry_limit`.

To update the `dma_entry_limit`,
`vfio_iommu_type1` has to be loaded with additional module parameter:

```
modprobe vfio_iommu_type1 dma_entry_limit=512000

```

Alternatively, one can also change this value in an already loaded kernel module:

```
echo 512000 > /sys/module/vfio_iommu_type1/parameters/dma_entry_limit

```

### 7.2.3. Creating Virtual Functions using vfio-pci

Since Linux version 5.7,
the `vfio-pci` module supports the creation of virtual functions.
After the PF is bound to `vfio-pci` module,
the user can create the VFs using the `sysfs` interface,
and these VFs will be bound to `vfio-pci` module automatically.

When the PF is bound to `vfio-pci`,
by default it will have a randomly generated VF token.
For security reasons, this token is write only,
so the user cannot read it from the kernel directly.
To access the VFs, the user needs to create a new token,
and use it to initialize both VF and PF devices.
The tokens are in UUID format,
so any UUID generation tool can be used to create a new token.

This VF token can be passed to DPDK by using EAL parameter `--vfio-vf-token`.
The token will be used for all PF and VF ports within the application.

1. Generate the VF token by uuid command





```
14d63f20-8445-11ea-8900-1f9ce7d5650d

```

2. Load the `vfio-pci` module with `enable_sriov` parameter set





```
sudo modprobe vfio-pci enable_sriov=1

```





Alternatively, pass the `enable_sriov` parameter through the `sysfs` if the module is already loaded or is built-in:





```
echo 1 | sudo tee /sys/module/vfio_pci/parameters/enable_sriov

```

3. Bind the PCI devices to `vfio-pci` driver





```
./usertools/dpdk-devbind.py -b vfio-pci 0000:86:00.0

```

4. Create the desired number of VF devices





```
echo 2 > /sys/bus/pci/devices/0000:86:00.0/sriov_numvfs

```

5. Start the DPDK application that will manage the PF device





```
<build_dir>/app/dpdk-testpmd -l 22-25 -n 4 -a 86:00.0 \
   --vfio-vf-token=14d63f20-8445-11ea-8900-1f9ce7d5650d --file-prefix=pf -- -i

```

6. Start the DPDK application that will manage the VF device





```
<build_dir>/app/dpdk-testpmd -l 26-29 -n 4 -a 86:02.0 \
   --vfio-vf-token=14d63f20-8445-11ea-8900-1f9ce7d5650d --file-prefix=vf0 -- -i

```


Note

Linux versions earlier than version 5.7 do not support the creation of
virtual functions within the VFIO framework.

### 7.2.4. Troubleshooting VFIO

In certain situations, using `dpdk-devbind.py` script
to bind a device to VFIO driver may fail.
The first place to check is the kernel messages:

```
dmesg | tail
...
[ 1297.875090] vfio-pci: probe of 0000:31:00.0 failed with error -22
...

```

In most cases, the `error -22` indicates that the VFIO subsystem
could not be enabled because there is no IOMMU support.

To check whether the kernel has been booted with correct parameters,
one can check the kernel command-line:

```
cat /proc/cmdline

```

Please refer to earlier sections on how to configure kernel parameters
correctly for your system.

If the kernel is configured correctly, one also has to make sure that
the BIOS configuration has virtualization features (such as Intel® VT-d).
There is no standard way to check if the platform is configured correctly,
so please check with your platform documentation to see if it has such features,
and how to enable them.

In certain distributions, default kernel configuration is such that
the no-IOMMU mode is disabled altogether at compile time.
This can be checked in the boot configuration of your system:

```
cat /boot/config-$(uname -r) | grep NOIOMMU
# CONFIG_VFIO_NOIOMMU is not set

```

If `CONFIG_VFIO_NOIOMMU` is not enabled in the kernel configuration,
VFIO driver will not support the no-IOMMU mode,
and other alternatives (such as UIO drivers) will have to be used.

## 7.3. VFIO Platform

VFIO Platform is a kernel driver that extends capabilities of VFIO
by adding support for platform devices that reside behind an IOMMU.
Linux usually learns about platform devices directly from device tree
during boot-up phase,
unlike for example, PCI devices which have necessary information built-in.

To make use of VFIO platform, the `vfio-platform` module must be loaded first:

```
sudo modprobe vfio-platform

```

Note

By default `vfio-platform` assumes that platform device has dedicated reset driver.
If such driver is missing or device does not require one,
this option can be turned off by setting `reset_required=0` module parameter.

Afterwards platform device needs to be bound to `vfio-platform`.
This is standard procedure requiring two steps.
First `driver_override`, which is available inside platform device directory,
needs to be set to `vfio-platform`:

```
sudo echo vfio-platform > /sys/bus/platform/devices/DEV/driver_override

```

Next `DEV` device must be bound to `vfio-platform` driver:

```
sudo echo DEV > /sys/bus/platform/drivers/vfio-platform/bind

```

On application startup, DPDK platform bus driver scans `/sys/bus/platform/devices`
searching for devices that have `driver` symbolic link
pointing to `vfio-platform` driver.
Finally, scanned devices are matched against available PMDs.
Matching is successful if either PMD name or PMD alias matches kernel driver name
or PMD name matches platform device name, all in that order.

VFIO Platform depends on ARM/ARM64 and is usually enabled on distributions
running on these systems.
Consult your distributions documentation to make sure that is the case.

## 7.4. Bifurcated Driver

PMDs which use the bifurcated driver co-exists with the device kernel driver.
On such model the NIC is controlled by the kernel, while the data
path is performed by the PMD directly on top of the device.

Such model has the following benefits:

> - It is secure and robust, as the memory management and isolation
> is done by the kernel.
>
> - It enables the user to use legacy linux tools such as `ethtool` or
> `ifconfig` while running DPDK application on the same network ports.
>
> - It enables the DPDK application to filter only part of the traffic,
> while the rest will be directed and handled by the kernel driver.
> The flow bifurcation is performed by the NIC hardware.
> As an example, using [Flow isolated mode](https://doc.dpdk.org/guides-25.03/prog_guide/ethdev/flow_offload.html#flow-isolated-mode) allows to choose
> strictly what is received in DPDK.

More about the bifurcated driver can be found in
NVIDIA [bifurcated PMD](https://www.dpdk.org/wp-content/uploads/sites/35/2016/10/Day02-Session04-RonyEfraim-Userspace2016.pdf) presentation.

## 7.5. UIO

Warning

Using UIO drivers is inherently unsafe due to this method lacking IOMMU protection,
and can only be done by root user.

In situations where using VFIO is not an option, there are alternative drivers one can use.
In many cases, the standard `uio_pci_generic` module included in the Linux kernel
can be used as a substitute for VFIO. This module can be loaded using the command:

```
sudo modprobe uio_pci_generic

```

Note

`uio_pci_generic` module doesn’t support the creation of virtual functions.

As an alternative to the `uio_pci_generic`, there is the `igb_uio` module
which can be found in the repository [dpdk-kmods](http://git.dpdk.org/dpdk-kmods).
It can be loaded as shown below:

```
sudo modprobe uio
sudo insmod igb_uio.ko

```

Note

For some devices which lack support for legacy interrupts, e.g. virtual function
(VF) devices, the `igb_uio` module may be needed in place of `uio_pci_generic`.

Note

If UEFI secure boot is enabled,
the Linux kernel may disallow the use of UIO on the system.
Therefore, devices for use by DPDK should be bound to the `vfio-pci` kernel module
rather than any UIO-based module.
For more details see [Binding and Unbinding Network Ports to/from the Kernel Modules](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#linux-gsg-binding-kernel) below.

Note

If the devices used for DPDK are bound to a UIO-based kernel module,
please make sure that the IOMMU is disabled or is in passthrough mode.
One can add `intel_iommu=off` or `amd_iommu=off` or `intel_iommu=on iommu=pt`
in GRUB command line on x86\_64 systems,
or add `iommu.passthrough=1` on aarch64 systems.


================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_linux_eal_parameters.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/linux_eal_parameters.html"
title: "9. EAL parameters — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html)
- 9\. EAL parameters
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/linux_eal_parameters.rst.txt)

* * *

# 9\. EAL parameters

This document contains a list of all EAL parameters. These parameters can be
used by any DPDK application running on Linux.

## 9.1. Common EAL parameters

The following EAL parameters are common to all platforms supported by DPDK.

### 9.1.1. Lcore-related options

- `-c <core mask>`

Set the hexadecimal bitmask of the cores to run on.

- `-l <core list>`

List of cores to run on

The argument format is `<c1>[-c2][,c3[-c4],...]`
where `c1`, `c2`, etc are core indexes between 0 and 128.

- `--lcores <core map>`

Map lcore set to physical cpu set

The argument format is:





```
<lcores[@cpus]>[<,lcores[@cpus]>...]

```





Lcore and CPU lists are grouped by `(` and `)` Within the group.
The `-` character is used as a range separator and `,` is used as a
single number separator.
The grouping `()` can be omitted for single element group.
The `@` can be omitted if cpus and lcores have the same value.


Note

At a given instance only one core option `--lcores`, `-l` or `-c` can
be used.

- `--main-lcore <core ID>`

Core ID that is used as main.

- `-s <service core mask>`

Hexadecimal bitmask of cores to be used as service cores.


### 9.1.2. Device-related options

- `-b, --block <[domain:]bus:devid.func>`

Skip probing a PCI device to prevent EAL from using it.
Multiple -b options are allowed.


Note

Block list cannot be used with the allow list `-a` option.

- `-a, --allow <[domain:]bus:devid.func>`

Add a PCI device in to the list of devices to probe.


Note

Allow list cannot be used with the block list `-b` option.

- `--vdev <device arguments>`

Add a virtual device using the format:





```
<driver><id>[,key=val, ...]

```





For example:





```
  --vdev 'net_pcap0,rx_pcap=input.pcap,tx_pcap=output.pcap'

```

- `-d <path to shared object or directory>`

Load external drivers. An argument can be a single shared object file, or a
directory containing multiple driver shared objects. Multiple -d options are
allowed.

- `--no-pci`

Disable PCI bus.


### 9.1.3. Multiprocessing-related options

- `--proc-type <primary|secondary|auto>`

Set the type of the current process.

- `--base-virtaddr <address>`

Attempt to use a different starting address for all memory maps of the
primary DPDK process. This can be helpful if secondary processes cannot
start due to conflicts in address map.


### 9.1.4. Memory-related options

- `-n <number of channels>`

Set the number of memory channels to use.

- `-r <number of ranks>`

Set the number of memory ranks (auto-detected by default).

- `-m <megabytes>`

Amount of memory to preallocate at startup.

- `--in-memory`

Do not create any shared data structures and run entirely in memory. Implies
`--no-shconf` and (if applicable) `--huge-unlink`.

- `--iova-mode <pa|va>`

Force IOVA mode to a specific value.

- `--huge-worker-stack[=size]`

Allocate worker stack memory from hugepage memory. Stack size defaults
to system pthread stack size unless the optional size (in kbytes) is
specified.


### 9.1.5. Debugging options

- `--no-shconf`

No shared files created (implies no secondary process support).

- `--no-huge`

Use anonymous memory instead of hugepages (implies no secondary process
support).

- `--log-level <type:val>`

Specify log level for a specific component. For example:





```
  --log-level lib.eal:debug

```





Can be specified multiple times.

- `--trace=<regex-match>`

Enable trace based on regular expression trace name. By default, the trace is
disabled. User must specify this option to enable trace.
For example:

Global trace configuration for EAL only:





```
  --trace=eal

```





Global trace configuration for ALL the components:





```
  --trace=.*

```





Can be specified multiple times up to 32 times.

- `--trace-dir=<directory path>`

Specify trace directory for trace output. For example:

Configuring `/tmp/` as a trace output directory:





```
  --trace-dir=/tmp

```





By default, trace output will created at `home` directory and parameter
must be specified once only.

- `--trace-bufsz=<val>`

Specify maximum size of allocated memory for trace output for each thread.
Valid unit can be either `B` or `K` or `M` for `Bytes`, `KBytes`
and `MBytes` respectively. For example:

Configuring `2MB` as a maximum size for trace output file:





```
  --trace-bufsz=2M

```





By default, size of trace output file is `1MB` and parameter
must be specified once only.

- `--trace-mode=<o[verwrite] | d[iscard] >`

Specify the mode of update of trace output file. Either update on a file
can be wrapped or discarded when file size reaches its maximum limit.
For example:

To `discard` update on trace output file:





```
  --trace-mode=d or --trace-mode=discard

```





Default mode is `overwrite` and parameter must be specified once only.


### 9.1.6. Other options

- `-h`, `--help`

Display help message listing all EAL parameters.

- `-v`

Display the version information on startup.

- `--mbuf-pool-ops-name`:

Pool ops name for mbuf to use.

- `--telemetry`:


> Enable telemetry (enabled by default).

- `--no-telemetry`:


> Disable telemetry.

- `--force-max-simd-bitwidth=<val>`:


> Specify the maximum SIMD bitwidth size to handle. This limits which vector paths,
> if any, are taken, as any paths taken must use a bitwidth below the max bitwidth limit.
> For example, to allow all SIMD bitwidths up to and including AVX-512:
>
> ```
> --force-max-simd-bitwidth=512
>
> ```
>
> The following example shows limiting the bitwidth to 64-bits to disable all vector code:
>
> ```
> --force-max-simd-bitwidth=64
>
> ```
>
> To disable use of max SIMD bitwidth limit:
>
> ```
> --force-max-simd-bitwidth=0
>
> ```

## 9.2. Linux-specific EAL parameters

In addition to common EAL parameters, there are also Linux-specific EAL
parameters.

### 9.2.1. Device-related options

- `--create-uio-dev`

Create `/dev/uioX` files for devices bound to igb\_uio kernel driver
(usually done by the igb\_uio driver itself).

- `--vmware-tsc-map`

Use VMware TSC map instead of native RDTSC.

- `--no-hpet`

Do not use the HPET timer.

- `--vfio-intr <legacy|msi|msix>`

Use specified interrupt mode for devices bound to VFIO kernel driver.

- `--vfio-vf-token <uuid>`

Use specified VF token for devices bound to VFIO kernel driver.


### 9.2.2. Multiprocessing-related options

- `--file-prefix <prefix name>`

Use a different shared data file prefix for a DPDK process. This option
allows running multiple independent DPDK primary/secondary processes under
different prefixes.


### 9.2.3. Memory-related options

- `--legacy-mem`

Use legacy DPDK memory allocation mode.

- `--socket-mem <amounts of memory per socket>`

Preallocate specified amounts of memory per socket. The parameter is a
comma-separated list of values. For example:





```
  --socket-mem 1024,2048

```





This will allocate 1 gigabyte of memory on socket 0, and 2048 megabytes of
memory on socket 1.

- `--socket-limit <amounts of memory per socket>`

Place a per-socket upper limit on memory use (non-legacy memory mode only).
0 will disable the limit for a particular socket.

- `--single-file-segments`

Create fewer files in hugetlbfs (non-legacy mode only).

- `--huge-dir <path to hugetlbfs directory>`

Use specified hugetlbfs directory instead of autodetected ones. This can be
a sub-directory within a hugetlbfs mountpoint.

- `--huge-unlink[=existing|always|never]`

No `--huge-unlink` option or `--huge-unlink=existing` is the default:
existing hugepage files are removed and re-created
to ensure the kernel clears the memory and prevents any data leaks.

With `--huge-unlink` (no value) or `--huge-unlink=always`,
hugepage files are also removed before mapping them,
so that the application leaves no files in hugetlbfs.
This mode implies no multi-process support.

When `--huge-unlink=never` is specified, existing hugepage files
are never removed, but are remapped instead, allowing hugepage reuse.
This makes restart faster by saving time to clear memory at initialization,
but it may slow down zeroed allocations later.
Reused hugepages can contain data from previous processes that used them,
which may be a security concern.
Hugepage files created in this mode are also not removed
when all the hugepages mapped from them are freed,
which allows to reuse these files after a restart.

- `--match-allocations`

Free hugepages back to system exactly as they were originally allocated.


### 9.2.4. Other options

- `--syslog <syslog facility>`

Set syslog facility. Valid syslog facilities are:





```
auth
cron
daemon
ftp
kern
lpr
mail
news
syslog
user
uucp
local0
local1
local2
local3
local4
local5
local6
local7

```


================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_nic_perf_intel_platform.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/nic_perf_intel_platform.html"
title: "11. How to get best performance with NICs on Intel platforms — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html)
- 11\. How to get best performance with NICs on Intel platforms
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/nic_perf_intel_platform.rst.txt)

* * *

# 11\. How to get best performance with NICs on Intel platforms

This document is a step-by-step guide for getting high performance from DPDK applications on Intel platforms.

## 11.1. Hardware and Memory Requirements

For best performance use an Intel Xeon class server system such as Ivy Bridge, Haswell or newer.

Ensure that each memory channel has at least one memory DIMM inserted, and that the memory size for each is at least 4GB.
**Note**: this has one of the most direct effects on performance.

You can check the memory configuration using `dmidecode` as follows:

```
dmidecode -t memory | grep Locator

Locator: DIMM_A1
Bank Locator: NODE 1
Locator: DIMM_A2
Bank Locator: NODE 1
Locator: DIMM_B1
Bank Locator: NODE 1
Locator: DIMM_B2
Bank Locator: NODE 1
...
Locator: DIMM_G1
Bank Locator: NODE 2
Locator: DIMM_G2
Bank Locator: NODE 2
Locator: DIMM_H1
Bank Locator: NODE 2
Locator: DIMM_H2
Bank Locator: NODE 2

```

The sample output above shows a total of 8 channels, from `A` to `H`, where each channel has 2 DIMMs.

You can also use `dmidecode` to determine the memory frequency:

```
dmidecode -t memory | grep Speed

Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
Configured Clock Speed: Unknown
Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
...
Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
Configured Clock Speed: Unknown
Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
Configured Clock Speed: Unknown

```

The output shows a speed of 2133 MHz (DDR4) and Unknown (not existing).
This aligns with the previous output which showed that each channel has one memory bar.

### 11.1.1. Network Interface Card Requirements

Use a [DPDK supported](https://core.dpdk.org/supported/) high end NIC such as the Intel XL710 40GbE.

Make sure each NIC has been flashed the latest version of NVM/firmware.

Use PCIe Gen3 slots, such as Gen3 `x8` or Gen3 `x16` because PCIe Gen2 slots don’t provide enough bandwidth
for 2 x 10GbE and above.
You can use `lspci` to check the speed of a PCI slot using something like the following:

```
lspci -s 03:00.1 -vv | grep LnkSta

LnkSta: Speed 8GT/s, Width x8, TrErr- Train- SlotClk+ DLActive- ...
LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete+ ...

```

When inserting NICs into PCI slots always check the caption, such as CPU0 or CPU1 to indicate which socket it is connected to.

Care should be take with NUMA.
If you are using 2 or more ports from different NICs, it is best to ensure that these NICs are on the same CPU socket.
An example of how to determine this is shown further below.

### 11.1.2. BIOS Settings

The following are some recommendations on BIOS settings. Different platforms will have different BIOS naming
so the following is mainly for reference:

1. Establish the steady state for the system, consider reviewing BIOS settings desired for best performance characteristic e.g. optimize for performance or energy efficiency.

2. Match the BIOS settings to the needs of the application you are testing.

3. Typically, **Performance** as the CPU Power and Performance policy is a reasonable starting point.

4. Consider using Turbo Boost to increase the frequency on cores.

5. Disable all virtualization options when you test the physical function of the NIC, and turn on VT-d if you wants to use VFIO.


### 11.1.3. Linux boot command line

The following are some recommendations on GRUB boot settings:

1. Use the default grub file as a starting point.

2. Reserve 1G huge pages via grub configurations. For example to reserve 8 huge pages of 1G size:





```
default_hugepagesz=1G hugepagesz=1G hugepages=8

```

3. Isolate CPU cores which will be used for DPDK. For example:





```
isolcpus=2,3,4,5,6,7,8

```

4. If it wants to use VFIO, use the following additional grub parameters:





```
iommu=pt intel_iommu=on

```


## 11.2. Configurations before running DPDK

1. Reserve huge pages.
See the earlier section on [Use of Hugepages in the Linux Environment](https://doc.dpdk.org/guides-25.03/linux_gsg/sys_reqs.html#linux-gsg-hugepages) for more details.





```
# Get the hugepage size.
awk '/Hugepagesize/ {print $2}' /proc/meminfo

# Get the total huge page numbers.
awk '/HugePages_Total/ {print $2} ' /proc/meminfo

# Unmount the hugepages.
umount `awk '/hugetlbfs/ {print $2}' /proc/mounts`

# Create the hugepage mount folder.
mkdir -p /mnt/huge

# Mount to the specific folder.
mount -t hugetlbfs nodev /mnt/huge

```

2. Check the CPU layout using the DPDK `cpu_layout` utility:





```
cd dpdk_folder

usertools/cpu_layout.py

```





Or run `lscpu` to check the cores on each socket.

3. Check your NIC id and related socket id:





```
# List all the NICs with PCI address and device IDs.
lspci -nn | grep Eth

```





For example suppose your output was as follows:





```
82:00.0 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
82:00.1 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
85:00.0 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
85:00.1 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]

```





Check the PCI device related numa node id:





```
cat /sys/bus/pci/devices/0000\:xx\:00.x/numa_node

```





Usually `0x:00.x` is on socket 0 and `8x:00.x` is on socket 1.
**Note**: To get the best performance, ensure that the core and NICs are in the same socket.
In the example above `85:00.0` is on socket 1 and should be used by cores on socket 1 for the best performance.

4. Check which kernel drivers needs to be loaded and whether there is a need to unbind the network ports from their kernel drivers.
More details about DPDK setup and Linux kernel requirements see [Compiling the DPDK Target from Source](https://doc.dpdk.org/guides-25.03/linux_gsg/build_dpdk.html#linux-gsg-compiling-dpdk) and [Linux Drivers](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_drivers.html#linux-gsg-linux-drivers).


================================================================================
FILE: doc.dpdk.org_guides-25.03_linux_gsg_sys_reqs.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/linux_gsg/sys_reqs.html"
title: "2. System Requirements — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Getting Started Guide for Linux](https://doc.dpdk.org/guides-25.03/linux_gsg/index.html)
- 2\. System Requirements
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/linux_gsg/sys_reqs.rst.txt)

* * *

# 2\. System Requirements

This chapter describes the packages required to compile the DPDK.

## 2.1. BIOS Setting Prerequisite on x86

For the majority of platforms, no special BIOS settings are needed to use basic DPDK functionality.
However, for additional HPET timer and power management functionality,
and high performance of small packets, BIOS setting changes may be needed.
Consult the section on [Enabling Additional Functionality](https://doc.dpdk.org/guides-25.03/linux_gsg/enable_func.html#enabling-additional-functionality)
for more information on the required changes.

## 2.2. Compilation of the DPDK

**Required Tools and Libraries:**

Note

The setup commands and installed packages needed on various systems may be different.
For details on Linux distributions and the versions tested, please consult the DPDK Release Notes.

- General development tools including a C compiler supporting the C11 standard,
including standard atomics, for example: GCC (version 5.0+) or Clang (version 3.6+),
and `pkg-config` or `pkgconf` to be used when building end-user binaries against DPDK.

  - For RHEL/Fedora systems these can be installed using `dnf groupinstall "Development Tools"`

  - For Ubuntu/Debian systems these can be installed using `apt install build-essential`

  - For Alpine Linux, `apk add alpine-sdk bsd-compat-headers`

Note

pkg-config 0.27, supplied with RHEL-7,
does not process the Libs.private section correctly,
resulting in statically linked applications not being linked properly.
Use an updated version of `pkg-config` or `pkgconf` instead when building applications

- Python 3.6 or later.

- Meson (version 0.57+) and ninja

  - `meson` & `ninja-build` packages in most Linux distributions

  - If the packaged version is below the minimum version, the latest versions
    can be installed from Python’s “pip” repository: `pip3 install meson ninja`
- `pyelftools` (version 0.22+)

  - For Fedora systems it can be installed using `dnf install python-pyelftools`

  - For RHEL/CentOS systems it can be installed using `pip3 install pyelftools`

  - For Ubuntu/Debian it can be installed using `apt install python3-pyelftools`

  - For Alpine Linux, `apk add py3-elftools`
- Library for handling NUMA (Non Uniform Memory Access).

  - `numactl-devel` in RHEL/Fedora;

  - `libnuma-dev` in Debian/Ubuntu;

  - `numactl-dev` in Alpine Linux

Note

Please ensure that the latest patches are applied to third party libraries
and software to avoid any known vulnerabilities.

**Optional Tools:**

- Intel® oneAPI DPC++/C++ Compiler.

- IBM® Advance ToolChain for Powerlinux. This is a set of open source development tools and runtime libraries
which allows users to take leading edge advantage of IBM’s latest POWER hardware features on Linux. To install
it, see the IBM official installation document.


**Additional Libraries**

A number of DPDK components, such as libraries and poll-mode drivers (PMDs) have additional dependencies.
For DPDK builds, the presence or absence of these dependencies will be automatically detected
enabling or disabling the relevant components appropriately.

In each case, the relevant library development package ( `-devel` or `-dev`) is needed to build the DPDK components.

For libraries the additional dependencies include:

- libarchive: for some unit tests using tar to get their resources.

- libelf: to compile and use the bpf library.


For poll-mode drivers, the additional dependencies for each driver can be
found in that driver’s documentation in the relevant DPDK guide document,
e.g. [Network Interface Controller Drivers](https://doc.dpdk.org/guides-25.03/nics/index.html)

## 2.3. Running DPDK Applications

To run a DPDK application, some customization may be required on the target machine.

### 2.3.1. System Software

**Required:**

- Kernel version >= 4.19

The kernel version required is based on the oldest long term stable kernel available
at kernel.org when the DPDK version is in development.
Compatibility for recent distribution kernels will be kept, notably RHEL/CentOS 7.

The kernel version in use can be checked using the command:





```
uname -r

```

- glibc >= 2.7 (for features related to cpuset)

The version can be checked using the `ldd --version` command.

- Kernel configuration

In the Fedora OS and other common distributions, such as Ubuntu, or Red Hat Enterprise Linux,
the vendor supplied kernel configurations can be used to run most DPDK applications.

For other kernel builds, options which should be enabled for DPDK include:

  - HUGETLBFS

  - PROC\_PAGE\_MONITOR support

  - HPET and HPET\_MMAP configuration options should also be enabled if HPET support is required.
    See the section on [High Precision Event Timer (HPET) Functionality](https://doc.dpdk.org/guides-25.03/linux_gsg/enable_func.html#high-precision-event-timer) for more details.

### 2.3.2. Use of Hugepages in the Linux Environment

Hugepage support is required for the large memory pool allocation used for packet buffers
(the HUGETLBFS option must be enabled in the running kernel as indicated the previous section).
By using hugepage allocations, performance is increased since fewer pages are needed,
and therefore less Translation Lookaside Buffers (TLBs, high speed translation caches),
which reduce the time it takes to translate a virtual page address to a physical page address.
Without hugepages, high TLB miss rates would occur with the standard 4k page size, slowing performance.

#### 2.3.2.1. Reserving Hugepages for DPDK Use

The reservation of hugepages can be performed at run time.
This is done by echoing the number of hugepages required
to a `nr_hugepages` file in the `/sys/kernel/` directory
corresponding to a specific page size (in Kilobytes).
For a single-node system, the command to use is as follows
(assuming that 1024 of 2MB pages are required):

```
echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages

```

On a NUMA machine, the above command will usually divide the number of hugepages
equally across all NUMA nodes (assuming there is enough memory on all NUMA nodes).
However, pages can also be reserved explicitly on individual NUMA nodes
using a `nr_hugepages` file in the `/sys/devices/` directory:

```
echo 1024 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages
echo 1024 > /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages

```

The tool `dpdk-hugepages.py` can be used to manage hugepages.

Note

Some kernel versions may not allow reserving 1 GB hugepages at run time,
so reserving them at boot time may be the only option.
Please see below for instructions.

**Alternative:**

In the general case, reserving hugepages at run time is perfectly fine,
but in use cases where having lots of physically contiguous memory is required,
it is preferable to reserve hugepages at boot time,
as that will help in preventing physical memory from becoming heavily fragmented.

To reserve hugepages at boot time, a parameter is passed to the Linux kernel on the kernel command line.

For 2 MB pages, just pass the hugepages option to the kernel. For example, to reserve 1024 pages of 2 MB, use:

```
hugepages=1024

```

For other hugepage sizes, for example 1G pages, the size must be specified explicitly and
can also be optionally set as the default hugepage size for the system.
For example, to reserve 4G of hugepage memory in the form of four 1G pages, the following options should be passed to the kernel:

```
default_hugepagesz=1G hugepagesz=1G hugepages=4

```

Note

The hugepage sizes that a CPU supports can be determined from the CPU flags on Intel architecture.
If pse exists, 2M hugepages are supported; if pdpe1gb exists, 1G hugepages are supported.
On IBM Power architecture, the supported hugepage sizes are 16MB and 16GB.

Note

For 64-bit applications, it is recommended to use 1 GB hugepages if the platform supports them.

In the case of a dual-socket NUMA system,
the number of hugepages reserved at boot time is generally divided equally between the two sockets
(on the assumption that sufficient memory is present on both sockets).

See the Documentation/admin-guide/kernel-parameters.txt file in your Linux source tree for further details of these and other kernel options.

#### 2.3.2.2. Using Hugepages with the DPDK

If secondary process support is not required, DPDK is able to use hugepages
without any configuration by using “in-memory” mode.
Please see [EAL parameters](https://doc.dpdk.org/guides-25.03/linux_gsg/linux_eal_parameters.html) for more details.

If secondary process support is required,
mount points for hugepages need to be created.
On modern Linux distributions, a default mount point for hugepages
is provided by the system and is located at `/dev/hugepages`.
This mount point will use the default hugepage size
set by the kernel parameters as described above.

However, in order to use hugepage sizes other than the default, it is necessary
to manually create mount points for those hugepage sizes (e.g. 1GB pages).

To make the hugepages of size 1GB available for DPDK use,
following steps must be performed:

```
mkdir /mnt/huge
mount -t hugetlbfs pagesize=1GB /mnt/huge

```

The mount point can be made permanent across reboots, by adding the following line to the `/etc/fstab` file:

```
nodev /mnt/huge hugetlbfs pagesize=1GB 0 0

```


================================================================================
FILE: doc.dpdk.org_guides-25.03_platform_index.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/platform/index.html"
title: "Platform Specific Guides — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- Platform Specific Guides
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/platform/index.rst.txt)

* * *

# Platform Specific Guides

The following are platform specific guides and setup information.

- [1\. NVIDIA BlueField Board Support Package](https://doc.dpdk.org/guides-25.03/platform/bluefield.html)
  - [1.1. Supported BlueField Platforms](https://doc.dpdk.org/guides-25.03/platform/bluefield.html#supported-bluefield-platforms)
  - [1.2. Common Offload HW Drivers](https://doc.dpdk.org/guides-25.03/platform/bluefield.html#common-offload-hw-drivers)
  - [1.3. Steps To Setup Platform](https://doc.dpdk.org/guides-25.03/platform/bluefield.html#steps-to-setup-platform)
  - [1.4. Compile DPDK](https://doc.dpdk.org/guides-25.03/platform/bluefield.html#compile-dpdk)
- [2\. Marvell cnxk platform guide](https://doc.dpdk.org/guides-25.03/platform/cnxk.html)
  - [2.1. Supported OCTEON cnxk SoCs](https://doc.dpdk.org/guides-25.03/platform/cnxk.html#supported-octeon-cnxk-socs)
  - [2.2. Resource Virtualization Unit architecture](https://doc.dpdk.org/guides-25.03/platform/cnxk.html#resource-virtualization-unit-architecture)
  - [2.3. LBK HW Access](https://doc.dpdk.org/guides-25.03/platform/cnxk.html#lbk-hw-access)
  - [2.4. SDP interface](https://doc.dpdk.org/guides-25.03/platform/cnxk.html#sdp-interface)
  - [2.5. cnxk packet flow](https://doc.dpdk.org/guides-25.03/platform/cnxk.html#cnxk-packet-flow)
  - [2.6. HW Offload Drivers](https://doc.dpdk.org/guides-25.03/platform/cnxk.html#hw-offload-drivers)
  - [2.7. Procedure to Setup Platform](https://doc.dpdk.org/guides-25.03/platform/cnxk.html#procedure-to-setup-platform)
  - [2.8. Debugging Options](https://doc.dpdk.org/guides-25.03/platform/cnxk.html#debugging-options)
  - [2.9. Compile DPDK](https://doc.dpdk.org/guides-25.03/platform/cnxk.html#compile-dpdk)
- [3\. NXP QorIQ DPAA Board Support Package](https://doc.dpdk.org/guides-25.03/platform/dpaa.html)
  - [3.1. Supported DPAA SoCs](https://doc.dpdk.org/guides-25.03/platform/dpaa.html#supported-dpaa-socs)
  - [3.2. Common Offload HW Block Drivers](https://doc.dpdk.org/guides-25.03/platform/dpaa.html#common-offload-hw-block-drivers)
  - [3.3. Steps To Setup Platform](https://doc.dpdk.org/guides-25.03/platform/dpaa.html#steps-to-setup-platform)
- [4\. NXP QorIQ DPAA2 Board Support Package](https://doc.dpdk.org/guides-25.03/platform/dpaa2.html)
  - [4.1. Supported DPAA2 SoCs](https://doc.dpdk.org/guides-25.03/platform/dpaa2.html#supported-dpaa2-socs)
  - [4.2. Common Offload HW Block Drivers](https://doc.dpdk.org/guides-25.03/platform/dpaa2.html#common-offload-hw-block-drivers)
  - [4.3. Steps To Setup Platform](https://doc.dpdk.org/guides-25.03/platform/dpaa2.html#steps-to-setup-platform)
- [5\. NVIDIA MLX5 Common Driver](https://doc.dpdk.org/guides-25.03/platform/mlx5.html)
  - [5.1. Design](https://doc.dpdk.org/guides-25.03/platform/mlx5.html#design)
  - [5.2. Classes](https://doc.dpdk.org/guides-25.03/platform/mlx5.html#classes)
  - [5.3. Compilation Prerequisites](https://doc.dpdk.org/guides-25.03/platform/mlx5.html#compilation-prerequisites)
  - [5.4. Compilation Options](https://doc.dpdk.org/guides-25.03/platform/mlx5.html#compilation-options)
  - [5.5. Environment Configuration](https://doc.dpdk.org/guides-25.03/platform/mlx5.html#environment-configuration)
  - [5.6. Device Arguments](https://doc.dpdk.org/guides-25.03/platform/mlx5.html#device-arguments)
- [6\. OCTEON TX Board Support Package](https://doc.dpdk.org/guides-25.03/platform/octeontx.html)
  - [6.1. Common Offload HW Block Drivers](https://doc.dpdk.org/guides-25.03/platform/octeontx.html#common-offload-hw-block-drivers)
  - [6.2. Steps To Setup Platform](https://doc.dpdk.org/guides-25.03/platform/octeontx.html#steps-to-setup-platform)
  - [6.3. Setup Platform Using OCTEON TX SDK](https://doc.dpdk.org/guides-25.03/platform/octeontx.html#setup-platform-using-octeon-tx-sdk)


================================================================================
FILE: doc.dpdk.org_guides-25.03_prog_guide_cryptodev_lib.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/prog_guide/cryptodev_lib.html"
title: "4. Cryptography Device Library — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Programmer’s Guide](https://doc.dpdk.org/guides-25.03/prog_guide/index.html)
- 4\. Cryptography Device Library
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/prog_guide/cryptodev_lib.rst.txt)

* * *

# 4\. Cryptography Device Library

The cryptodev library provides a Crypto device framework for management and
provisioning of hardware and software Crypto poll mode drivers, defining generic
APIs which support a number of different Crypto operations. The framework
currently only supports cipher, authentication, chained cipher/authentication
and AEAD symmetric and asymmetric Crypto operations.

The usages in security protocols are discussed in the [Security Support](https://doc.dpdk.org/guides-25.03/howto/security.html) guide.

## 4.1. Design Principles

The cryptodev library follows the same basic principles as those used in DPDK’s
Ethernet Device framework. The Crypto framework provides a generic Crypto device
framework which supports both physical (hardware) and virtual (software) Crypto
devices as well as a generic Crypto API which allows Crypto devices to be
managed and configured and supports Crypto operations to be provisioned on
Crypto poll mode driver.

## 4.2. Device Management

### 4.2.1. Device Creation

Physical Crypto devices are discovered during the PCI probe/enumeration of the
EAL function which is executed at DPDK initialization, based on
their PCI device identifier, each unique PCI BDF (bus/bridge, device,
function). Specific physical Crypto devices, like other physical devices in DPDK
can be listed using the EAL command line options.

Virtual devices can be created by two mechanisms, either using the EAL command
line options or from within the application using an EAL API directly.

From the command line using the –vdev EAL option

```
--vdev  'crypto_aesni_mb0,max_nb_queue_pairs=2,socket_id=0'

```

Note

- If DPDK application requires multiple software crypto PMD devices then required
number of `--vdev` with appropriate libraries are to be added.

- An Application with crypto PMD instances sharing the same library requires unique ID.


Example: `--vdev  'crypto_aesni_mb0' --vdev  'crypto_aesni_mb1'`

Or using the rte\_vdev\_init API within the application code.

```
rte_vdev_init("crypto_aesni_mb",
                  "max_nb_queue_pairs=2,socket_id=0")

```

All virtual Crypto devices support the following initialization parameters:

- `max_nb_queue_pairs` \- maximum number of queue pairs supported by the device.

- `socket_id` \- socket on which to allocate the device resources on.


### 4.2.2. Device Identification

Each device, whether virtual or physical is uniquely designated by two
identifiers:

- A unique device index used to designate the Crypto device in all functions
exported by the cryptodev API.

- A device name used to designate the Crypto device in console messages, for
administration or debugging purposes. For ease of use, the port name includes
the port index.


### 4.2.3. Device Configuration

The configuration of each Crypto device includes the following operations:

- Allocation of resources, including hardware resources if a physical device.

- Resetting the device into a well-known default state.

- Initialization of statistics counters.


The rte\_cryptodev\_configure API is used to configure a Crypto device.

```
int rte_cryptodev_configure(uint8_t dev_id,
                            struct rte_cryptodev_config *config)

```

The `rte_cryptodev_config` structure is used to pass the configuration
parameters for socket selection and number of queue pairs.

```
struct rte_cryptodev_config {
	int socket_id;			/**< Socket to allocate resources on */
	uint16_t nb_queue_pairs;
	/**< Number of queue pairs to configure on device */
	uint64_t ff_disable;
	/**< Feature flags to be disabled. Only the following features are
	 * allowed to be disabled,
	 *  - RTE_CRYPTODEV_FF_SYMMETRIC_CRYPTO
	 *  - RTE_CRYPTODEV_FF_ASYMMETRIC_CRYPTO
	 *  - RTE_CRYTPODEV_FF_SECURITY
	 */
};

```

### 4.2.4. Configuration of Queue Pairs

Each Crypto devices queue pair is individually configured through the
`rte_cryptodev_queue_pair_setup` API.
Each queue pairs resources may be allocated on a specified socket.

```
int rte_cryptodev_queue_pair_setup(uint8_t dev_id, uint16_t queue_pair_id,
            const struct rte_cryptodev_qp_conf *qp_conf,
            int socket_id)

```

```
struct rte_cryptodev_qp_conf {
	uint32_t nb_descriptors; /**< Number of descriptors per queue pair */
	struct rte_mempool *mp_session;
	/**< The mempool for creating session in sessionless mode */
	uint8_t priority;
	/**< Priority for this queue pair relative to other queue pairs.
	 *
	 * The requested priority should in the range of
	 * [@ref RTE_CRYPTODEV_QP_PRIORITY_HIGHEST, @ref RTE_CRYPTODEV_QP_PRIORITY_LOWEST].
	 * The implementation may normalize the requested priority to
	 * device supported priority value.
	 */
};

```

The field `mp_session` is used for creating temporary session to process
the crypto operations in the session-less mode.
They can be the same other different mempools. Please note not all Cryptodev
PMDs supports session-less mode.

### 4.2.5. Logical Cores, Memory and Queues Pair Relationships

The Crypto device Library as the Poll Mode Driver library support NUMA for when
a processor’s logical cores and interfaces utilize its local memory. Therefore
Crypto operations, and in the case of symmetric Crypto operations, the session
and the mbuf being operated on, should be allocated from memory pools created
in the local memory. The buffers should, if possible, remain on the local
processor to obtain the best performance results and buffer descriptors should
be populated with mbufs allocated from a mempool allocated from local memory.

The run-to-completion model also performs better, especially in the case of
virtual Crypto devices, if the Crypto operation and session and data buffer is
in local memory instead of a remote processor’s memory. This is also true for
the pipe-line model provided all logical cores used are located on the same
processor.

Multiple logical cores should never share the same queue pair for enqueuing
operations or dequeuing operations on the same Crypto device since this would
require global locks and hinder performance. It is however possible to use a
different logical core to dequeue an operation on a queue pair from the logical
core which it was enqueued on. This means that a crypto burst enqueue/dequeue
APIs are a logical place to transition from one logical core to another in a
packet processing pipeline.

## 4.3. Device Features and Capabilities

Crypto devices define their functionality through two mechanisms, global device
features and algorithm capabilities. Global devices features identify device
wide level features which are applicable to the whole device such as
the device having hardware acceleration or supporting symmetric and/or asymmetric
Crypto operations.

The capabilities mechanism defines the individual algorithms/functions which
the device supports, such as a specific symmetric Crypto cipher,
authentication operation or Authenticated Encryption with Associated Data
(AEAD) operation.

### 4.3.1. Device Features

Currently the following Crypto device features are defined:

- Symmetric Crypto operations

- Asymmetric Crypto operations

- Chaining of symmetric Crypto operations

- SSE accelerated SIMD vector operations

- AVX accelerated SIMD vector operations

- AVX2 accelerated SIMD vector operations

- AESNI accelerated instructions

- Hardware off-load processing


### 4.3.2. Device Operation Capabilities

Crypto capabilities which identify particular algorithm which the Crypto PMD
supports are defined by the operation type, the operation transform, the
transform identifier and then the particulars of the transform. For the full
scope of the Crypto capability see the definition of the structure in the
_DPDK API Reference_.

```
struct rte_cryptodev_capabilities;

```

Each Crypto poll mode driver defines its own private array of capabilities
for the operations it supports. Below is an example of the capabilities for a
PMD which supports the authentication algorithm SHA1\_HMAC and the cipher
algorithm AES\_CBC.

```
static const struct rte_cryptodev_capabilities pmd_capabilities[] = {
    {    /* SHA1 HMAC */
        .op = RTE_CRYPTO_OP_TYPE_SYMMETRIC,
        .sym = {
            .xform_type = RTE_CRYPTO_SYM_XFORM_AUTH,
            .auth = {
                .algo = RTE_CRYPTO_AUTH_SHA1_HMAC,
                .block_size = 64,
                .key_size = {
                    .min = 64,
                    .max = 64,
                    .increment = 0
                },
                .digest_size = {
                    .min = 12,
                    .max = 12,
                    .increment = 0
                },
                .aad_size = { 0 },
                .iv_size = { 0 }
            }
        }
    },
    {    /* AES CBC */
        .op = RTE_CRYPTO_OP_TYPE_SYMMETRIC,
        .sym = {
            .xform_type = RTE_CRYPTO_SYM_XFORM_CIPHER,
            .cipher = {
                .algo = RTE_CRYPTO_CIPHER_AES_CBC,
                .block_size = 16,
                .key_size = {
                    .min = 16,
                    .max = 32,
                    .increment = 8
                },
                .iv_size = {
                    .min = 16,
                    .max = 16,
                    .increment = 0
                }
            }
        }
    }
}

```

### 4.3.3. Capabilities Discovery

Discovering the features and capabilities of a Crypto device poll mode driver
is achieved through the `rte_cryptodev_info_get` function.

```
void rte_cryptodev_info_get(uint8_t dev_id,
                            struct rte_cryptodev_info *dev_info);

```

This allows the user to query a specific Crypto PMD and get all the device
features and capabilities. The `rte_cryptodev_info` structure contains all the
relevant information for the device.

```
struct rte_cryptodev_info {
	const char *driver_name;	/**< Driver name. */
	uint8_t driver_id;		/**< Driver identifier */
	struct rte_device *device;	/**< Generic device information. */

	uint64_t feature_flags;
	/**< Feature flags exposes HW/SW features for the given device */

	const struct rte_cryptodev_capabilities *capabilities;
	/**< Array of devices supported capabilities */

	unsigned max_nb_queue_pairs;
	/**< Maximum number of queues pairs supported by device. */

	uint16_t min_mbuf_headroom_req;
	/**< Minimum mbuf headroom required by device */

	uint16_t min_mbuf_tailroom_req;
	/**< Minimum mbuf tailroom required by device */

	struct {
		unsigned max_nb_sessions;
		/**< Maximum number of sessions supported by device.
		 * If 0, the device does not have any limitation in
		 * number of sessions that can be used.
		 */
	} sym;
};

```

## 4.4. Operation Processing

Scheduling of Crypto operations on DPDK’s application data path is
performed using a burst oriented asynchronous API set. A queue pair on a Crypto
device accepts a burst of Crypto operations using enqueue burst API. On physical
Crypto devices the enqueue burst API will place the operations to be processed
on the devices hardware input queue, for virtual devices the processing of the
Crypto operations is usually completed during the enqueue call to the Crypto
device. The dequeue burst API will retrieve any processed operations available
from the queue pair on the Crypto device, from physical devices this is usually
directly from the devices processed queue, and for virtual device’s from a
`rte_ring` where processed operations are placed after being processed on the
enqueue call.

### 4.4.1. Private data

For session-based operations, the set and get API provides a mechanism for an
application to store and retrieve the private user data information stored along
with the crypto session.

For example, suppose an application is submitting a crypto operation with a session
associated and wants to indicate private user data information which is required to be
used after completion of the crypto operation. In this case, the application can use
the set API to set the user data and retrieve it using get API.

```
int rte_cryptodev_sym_session_set_user_data(
        struct rte_cryptodev_sym_session *sess, void *data, uint16_t size);

void * rte_cryptodev_sym_session_get_user_data(
        struct rte_cryptodev_sym_session *sess);

```

Please note the `size` passed to set API cannot be bigger than the predefined
`user_data_sz` when creating the session header mempool, otherwise the
function will return error. Also when `user_data_sz` was defined as `0` when
creating the session header mempool, the get API will always return `NULL`.

For session-less mode, the private user data information can be placed along with the
`struct rte_crypto_op`. The `rte_crypto_op::private_data_offset` indicates the
start of private data information. The offset is counted from the start of the
rte\_crypto\_op including other crypto information such as the IVs (since there can
be an IV also for authentication).

### 4.4.2. User callback APIs

The add APIs configures a user callback function to be called for each burst of crypto
ops received/sent on a given crypto device queue pair. The return value is a pointer
that can be used later to remove the callback using remove API. Application is expected
to register a callback function of type `rte_cryptodev_callback_fn`. Multiple callback
functions can be added for a given queue pair. API does not restrict on maximum number of
callbacks.

Callbacks registered by application would not survive `rte_cryptodev_configure` as it
reinitializes the callback list. It is user responsibility to remove all installed
callbacks before calling `rte_cryptodev_configure` to avoid possible memory leakage.

So, the application is expected to add user callback after `rte_cryptodev_configure`.
The callbacks can also be added at the runtime. These callbacks get executed when
`rte_cryptodev_enqueue_burst`/ `rte_cryptodev_dequeue_burst` is called.

```
struct rte_cryptodev_cb *
        rte_cryptodev_add_enq_callback(uint8_t dev_id, uint16_t qp_id,
                                       rte_cryptodev_callback_fn cb_fn,
                                       void *cb_arg);

struct rte_cryptodev_cb *
        rte_cryptodev_add_deq_callback(uint8_t dev_id, uint16_t qp_id,
                                       rte_cryptodev_callback_fn cb_fn,
                                       void *cb_arg);

uint16_t (* rte_cryptodev_callback_fn)(uint16_t dev_id, uint16_t qp_id,
                                       struct rte_crypto_op **ops,
                                       uint16_t nb_ops, void *user_param);

```

The remove API removes a callback function added by
`rte_cryptodev_add_enq_callback`/ `rte_cryptodev_add_deq_callback`.

```
int rte_cryptodev_remove_enq_callback(uint8_t dev_id, uint16_t qp_id,
                                      struct rte_cryptodev_cb *cb);

int rte_cryptodev_remove_deq_callback(uint8_t dev_id, uint16_t qp_id,
                                      struct rte_cryptodev_cb *cb);

```

### 4.4.3. Enqueue / Dequeue Burst APIs

The burst enqueue API uses a Crypto device identifier and a queue pair
identifier to specify the Crypto device queue pair to schedule the processing on.
The `nb_ops` parameter is the number of operations to process which are
supplied in the `ops` array of `rte_crypto_op` structures.
The enqueue function returns the number of operations it actually enqueued for
processing, a return value equal to `nb_ops` means that all packets have been
enqueued.

```
uint16_t rte_cryptodev_enqueue_burst(uint8_t dev_id, uint16_t qp_id,
                                     struct rte_crypto_op **ops, uint16_t nb_ops)

```

The dequeue API uses the same format as the enqueue API of processed but
the `nb_ops` and `ops` parameters are now used to specify the max processed
operations the user wishes to retrieve and the location in which to store them.
The API call returns the actual number of processed operations returned, this
can never be larger than `nb_ops`.

```
uint16_t rte_cryptodev_dequeue_burst(uint8_t dev_id, uint16_t qp_id,
                                     struct rte_crypto_op **ops, uint16_t nb_ops)

```

### 4.4.4. Operation Representation

An Crypto operation is represented by an rte\_crypto\_op structure, which is a
generic metadata container for all necessary information required for the
Crypto operation to be processed on a particular Crypto device poll mode driver.

![../_images/crypto_op.svg](https://doc.dpdk.org/guides-25.03/_images/crypto_op.svg)

The operation structure includes the operation type, the operation status
and the session type (session-based/less), a reference to the operation
specific data, which can vary in size and content depending on the operation
being provisioned. It also contains the source mempool for the operation,
if it allocated from a mempool.

If Crypto operations are allocated from a Crypto operation mempool, see next
section, there is also the ability to allocate private memory with the
operation for applications purposes.

Application software is responsible for specifying all the operation specific
fields in the `rte_crypto_op` structure which are then used by the Crypto PMD
to process the requested operation.

### 4.4.5. Operation Management and Allocation

The cryptodev library provides an API set for managing Crypto operations which
utilize the Mempool Library to allocate operation buffers. Therefore, it ensures
that the crypto operation is interleaved optimally across the channels and
ranks for optimal processing.
A `rte_crypto_op` contains a field indicating the pool that it originated from.
When calling `rte_crypto_op_free(op)`, the operation returns to its original pool.

```
extern struct rte_mempool *
rte_crypto_op_pool_create(const char *name, enum rte_crypto_op_type type,
                          unsigned nb_elts, unsigned cache_size, uint16_t priv_size,
                          int socket_id);

```

During pool creation `rte_crypto_op_init()` is called as a constructor to
initialize each Crypto operation which subsequently calls
`__rte_crypto_op_reset()` to configure any operation type specific fields based
on the type parameter.

`rte_crypto_op_alloc()` and `rte_crypto_op_bulk_alloc()` are used to allocate
Crypto operations of a specific type from a given Crypto operation mempool.
`__rte_crypto_op_reset()` is called on each operation before being returned to
allocate to a user so the operation is always in a good known state before use
by the application.

```
struct rte_crypto_op *rte_crypto_op_alloc(struct rte_mempool *mempool,
                                          enum rte_crypto_op_type type)

unsigned rte_crypto_op_bulk_alloc(struct rte_mempool *mempool,
                                  enum rte_crypto_op_type type,
                                  struct rte_crypto_op **ops, uint16_t nb_ops)

```

`rte_crypto_op_free()` is called by the application to return an operation to
its allocating pool.

```
void rte_crypto_op_free(struct rte_crypto_op *op)

```

## 4.5. Symmetric Cryptography Support

The cryptodev library currently provides support for the following symmetric
Crypto operations; cipher, authentication, including chaining of these
operations, as well as also supporting AEAD operations.

### 4.5.1. Session and Session Management

Sessions are used in symmetric cryptographic processing to store the immutable
data defined in a cryptographic transform which is used in the operation
processing of a packet flow. Sessions are used to manage information such as
expand cipher keys and HMAC IPADs and OPADs, which need to be calculated for a
particular Crypto operation, but are immutable on a packet to packet basis for
a flow. Crypto sessions cache this immutable data in a optimal way for the
underlying PMD and this allows further acceleration of the offload of
Crypto workloads.

The Crypto device framework provides APIs to create session mempool and allocate
and initialize sessions for crypto devices, where sessions are mempool objects.
The application has to use `rte_cryptodev_sym_session_pool_create()` to
create the session mempool header and the private data with the size specified
by the user through the `elt_size` parameter in the function.
The session private data is for the driver to initialize and access
during crypto operations, hence the `elt_size` should be big enough
for all drivers that will share this mempool.
To obtain the proper session private data size of a crypto device,
the user can call `rte_cryptodev_sym_get_private_session_size()` function.
In case of heterogeneous crypto devices which will share the same session mempool,
the maximum session private data size of them should be passed.

Once the session mempools have been created, `rte_cryptodev_sym_session_create()`
is used to allocate and initialize the session from the given mempool.
The created session can ONLY be used by the crypto devices sharing the same driver ID
as the device ID passed into the function as the parameter.
In addition, a symmetric transform chain is used to specify the operation and its parameters.
See the section below for details on transforms.

When a session is no longer used, user must call `rte_cryptodev_sym_session_free()`
to uninitialize the session data and return the session
back to the mempool it belongs.

### 4.5.2. Transforms and Transform Chaining

Symmetric Crypto transforms ( `rte_crypto_sym_xform`) are the mechanism used
to specify the details of the Crypto operation. For chaining of symmetric
operations such as cipher encrypt and authentication generate, the next pointer
allows transform to be chained together. Crypto devices which support chaining
must publish the chaining of symmetric Crypto operations feature flag. Allocation of the
xform structure is in the application domain. To allow future API extensions in a
backwardly compatible manner, e.g. addition of a new parameter, the application should
zero the full xform struct before populating it.

Currently there are three transforms types cipher, authentication and AEAD.
Also it is important to note that the order in which the
transforms are passed indicates the order of the chaining.

```
struct rte_crypto_sym_xform {
	struct rte_crypto_sym_xform *next;
	/**< next xform in chain */
	enum rte_crypto_sym_xform_type type
	; /**< xform type */
	union {
		struct rte_crypto_auth_xform auth;
		/**< Authentication / hash xform */
		struct rte_crypto_cipher_xform cipher;
		/**< Cipher xform */
		struct rte_crypto_aead_xform aead;
		/**< AEAD xform */
	};
};

```

The API does not place a limit on the number of transforms that can be chained
together but this will be limited by the underlying Crypto device poll mode
driver which is processing the operation.

![../_images/crypto_xform_chain.svg](https://doc.dpdk.org/guides-25.03/_images/crypto_xform_chain.svg)

### 4.5.3. Symmetric Operations

The symmetric Crypto operation structure contains all the mutable data relating
to performing symmetric cryptographic processing on a referenced mbuf data
buffer. It is used for either cipher, authentication, AEAD and chained
operations.

As a minimum the symmetric operation must have a source data buffer ( `m_src`),
a valid session (or transform chain if in session-less mode) and the minimum
authentication/ cipher/ AEAD parameters required depending on the type of operation
specified in the session or the transform
chain.

```
struct rte_crypto_sym_op {
	struct rte_mbuf *m_src;	/**< source mbuf */
	struct rte_mbuf *m_dst;	/**< destination mbuf */

	union {
		void *session;
		/**< Handle for the initialised crypto/security session context */
		struct rte_crypto_sym_xform *xform;
		/**< Session-less API crypto operation parameters */
	};

	union {
		struct {
			struct {
				uint32_t offset;
				 /**< Starting point for AEAD processing, specified as
				  * number of bytes from start of packet in source
				  * buffer.
				  */
				uint32_t length;
				 /**< The message length, in bytes, of the source buffer
				  * on which the cryptographic operation will be
				  * computed.
				  */
			} data; /**< Data offsets and length for AEAD */
			struct {
				uint8_t *data;
				/**< This points to the location where the digest result
				 * should be inserted (in the case of digest generation)
				 * or where the purported digest exists (in the case of
				 * digest verification).
				 *
				 * At session creation time, the client specified the
				 * digest result length with the digest_length member
				 * of the @ref rte_crypto_auth_xform structure. For
				 * physical crypto devices the caller must allocate at
				 * least digest_length of physically contiguous memory
				 * at this location.
				 *
				 * For digest generation, the digest result will
				 * overwrite any data at this location.
				 *
				 * @note
				 * For GCM (@ref RTE_CRYPTO_AEAD_AES_GCM), for
				 * "digest result" read "authentication tag T".
				 */
				rte_iova_t phys_addr;
				/**< Physical address of digest */
			} digest; /**< Digest parameters */
			struct {
				uint8_t *data;
				/**< Pointer to Additional Authenticated Data (AAD)
				 * needed for authenticated cipher mechanisms (CCM and
				 * GCM)
				 *
				 * Specifically for CCM (@ref RTE_CRYPTO_AEAD_AES_CCM),
				 * the caller should setup this field as follows:
				 *
				 * - the additional authentication data itself should
				 * be written starting at an offset of 18 bytes into
				 * the array, leaving room for the first block (16 bytes)
				 * and the length encoding in the first two bytes of the
				 * second block.
				 *
				 * - Note that PMDs may modify the memory reserved
				 * (first 18 bytes and the final padding).
				 *
				 * Finally, for GCM (@ref RTE_CRYPTO_AEAD_AES_GCM), the
				 * caller should setup this field as follows:
				 *
				 */
				rte_iova_t phys_addr;	/**< physical address */
			} aad;
			/**< Additional authentication parameters */
		} aead;

		struct {
			struct {
				struct {
					uint32_t offset;
					 /**< Starting point for cipher processing,
					  * specified as number of bytes from start
					  * of data in the source buffer.
					  * The result of the cipher operation will be
					  * written back into the output buffer
					  * starting at this location.
					  *
					  * @note
					  * For SNOW 3G @ RTE_CRYPTO_CIPHER_SNOW3G_UEA2,
					  * KASUMI @ RTE_CRYPTO_CIPHER_KASUMI_F8
					  * and ZUC @ RTE_CRYPTO_CIPHER_ZUC_EEA3,
					  * this field should be in bits. For
					  * digest-encrypted cases this must be
					  * an 8-bit multiple.
					  */
					uint32_t length;
					 /**< The message length, in bytes, of the
					  * source buffer on which the cryptographic
					  * operation will be computed.
					  * This is also the same as the result length.
					  * For block ciphers, this must be a
					  * multiple of the block size,
					  * or for the AES-XTS a multiple of the data-unit length
					  * as described in xform.
					  *
					  * @note
					  * For SNOW 3G @ RTE_CRYPTO_AUTH_SNOW3G_UEA2,
					  * KASUMI @ RTE_CRYPTO_CIPHER_KASUMI_F8
					  * and ZUC @ RTE_CRYPTO_CIPHER_ZUC_EEA3,
					  * this field should be in bits. For
					  * digest-encrypted cases this must be
					  * an 8-bit multiple.
					  */
				} data; /**< Data offsets and length for ciphering */
			} cipher;

			struct {
				struct {
					uint32_t offset;
					 /**< Starting point for hash processing,
					  * specified as number of bytes from start of
					  * packet in source buffer.
					  *
					  * @note
					  * For SNOW 3G @ RTE_CRYPTO_AUTH_SNOW3G_UIA2,
					  * KASUMI @ RTE_CRYPTO_AUTH_KASUMI_F9
					  * and ZUC @ RTE_CRYPTO_AUTH_ZUC_EIA3,
					  * this field should be in bits. For
					  * digest-encrypted cases this must be
					  * an 8-bit multiple.
					  *
					  * @note
					  * For KASUMI @ RTE_CRYPTO_AUTH_KASUMI_F9,
					  * this offset should be such that
					  * data to authenticate starts at COUNT.
					  *
					  * @note
					  * For DOCSIS security protocol, this
					  * offset is the DOCSIS header length
					  * and, therefore, also the CRC offset
					  * i.e. the number of bytes into the
					  * packet at which CRC calculation
					  * should begin.
					  */
					uint32_t length;
					 /**< The message length, in bytes, of the source
					  * buffer that the hash will be computed on.
					  *
					  * @note
					  * For SNOW 3G @ RTE_CRYPTO_AUTH_SNOW3G_UIA2,
					  * KASUMI @ RTE_CRYPTO_AUTH_KASUMI_F9
					  * and ZUC @ RTE_CRYPTO_AUTH_ZUC_EIA3,
					  * this field should be in bits. For
					  * digest-encrypted cases this must be
					  * an 8-bit multiple.
					  *
					  * @note
					  * For KASUMI @ RTE_CRYPTO_AUTH_KASUMI_F9,
					  * the length should include the COUNT,
					  * FRESH, message, direction bit and padding
					  * (to be multiple of 8 bits).
					  *
					  * @note
					  * For DOCSIS security protocol, this
					  * is the CRC length i.e. the number of
					  * bytes in the packet over which the
					  * CRC should be calculated
					  */
				} data;
				/**< Data offsets and length for authentication */

				struct {
					uint8_t *data;
					/**< This points to the location where
					 * the digest result should be inserted
					 * (in the case of digest generation)
					 * or where the purported digest exists
					 * (in the case of digest verification).
					 *
					 * At session creation time, the client
					 * specified the digest result length with
					 * the digest_length member of the
					 * @ref rte_crypto_auth_xform structure.
					 * For physical crypto devices the caller
					 * must allocate at least digest_length of
					 * physically contiguous memory at this
					 * location.
					 *
					 * For digest generation, the digest result
					 * will overwrite any data at this location.
					 *
					 * @note
					 * Digest-encrypted case.
					 * Digest can be generated, appended to
					 * the end of raw data and encrypted
					 * together using chained digest
					 * generation
					 * (@ref RTE_CRYPTO_AUTH_OP_GENERATE)
					 * and encryption
					 * (@ref RTE_CRYPTO_CIPHER_OP_ENCRYPT)
					 * xforms. Similarly, authentication
					 * of the raw data against appended,
					 * decrypted digest, can be performed
					 * using decryption
					 * (@ref RTE_CRYPTO_CIPHER_OP_DECRYPT)
					 * and digest verification
					 * (@ref RTE_CRYPTO_AUTH_OP_VERIFY)
					 * chained xforms.
					 * To perform those operations, a few
					 * additional conditions must be met:
					 * - caller must allocate at least
					 * digest_length of memory at the end of
					 * source and (in case of out-of-place
					 * operations) destination buffer; those
					 * buffers can be linear or split using
					 * scatter-gather lists,
					 * - digest data pointer must point to
					 * the end of source or (in case of
					 * out-of-place operations) destination
					 * data, which is pointer to the
					 * data buffer + auth.data.offset +
					 * auth.data.length,
					 * - cipher.data.offset +
					 * cipher.data.length must be greater
					 * than auth.data.offset +
					 * auth.data.length and is typically
					 * equal to auth.data.offset +
					 * auth.data.length + digest_length.
					 * - for wireless algorithms, i.e.
					 * SNOW 3G, KASUMI and ZUC, as the
					 * cipher.data.length,
					 * cipher.data.offset,
					 * auth.data.length and
					 * auth.data.offset are in bits, they
					 * must be 8-bit multiples.
					 *
					 * Note, that for security reasons, it
					 * is PMDs' responsibility to not
					 * leave an unencrypted digest in any
					 * buffer after performing auth-cipher
					 * operations.
					 *
					 */
					rte_iova_t phys_addr;
					/**< Physical address of digest */
				} digest; /**< Digest parameters */
			} auth;
		};
	};
};

```

## 4.6. Synchronous mode

Some cryptodevs support synchronous mode alongside with a standard asynchronous
mode. In that case operations are performed directly when calling
`rte_cryptodev_sym_cpu_crypto_process` method instead of enqueuing and
dequeuing an operation before. This mode of operation allows cryptodevs which
utilize CPU cryptographic acceleration to have significant performance boost
comparing to standard asynchronous approach. Cryptodevs supporting synchronous
mode have `RTE_CRYPTODEV_FF_SYM_CPU_CRYPTO` feature flag set.

To perform a synchronous operation a call to
`rte_cryptodev_sym_cpu_crypto_process` has to be made with vectorized
operation descriptor ( `struct rte_crypto_sym_vec`) containing:

- `num` \- number of operations to perform,

- pointer to an array of size `num` containing a scatter-gather list
descriptors of performed operations ( `struct rte_crypto_sgl`). Each instance
of `struct rte_crypto_sgl` consists of a number of segments and a pointer to
an array of segment descriptors `struct rte_crypto_vec`;

- pointers to arrays of size `num` containing IV, AAD and digest information
in the `cpu_crypto` sub-structure,

- pointer to an array of size `num` where status information will be stored
for each operation.


Function returns a number of successfully completed operations and sets
appropriate status number for each operation in the status array provided as
a call argument. Status different than zero must be treated as error.

For more details, e.g. how to convert an mbuf to an SGL, please refer to an
example usage in the IPsec library implementation.

### 4.6.1. Cryptodev Raw Data-path APIs

The Crypto Raw data-path APIs are a set of APIs designed to enable external
libraries/applications to leverage the cryptographic processing provided by
DPDK crypto PMDs through the cryptodev API but in a manner that is not
dependent on native DPDK data structures (eg. rte\_mbuf, rte\_crypto\_op, … etc)
in their data-path implementation.

The raw data-path APIs have the following advantages:

- External data structure friendly design. The new APIs uses the operation
descriptor `struct rte_crypto_sym_vec` that supports raw data pointer and
IOVA addresses as input. Moreover, the APIs does not require the user to
allocate the descriptor from mempool, nor requiring mbufs to describe input
data’s virtual and IOVA addresses. All these features made the translation
from user’s own data structure into the descriptor easier and more efficient.

- Flexible enqueue and dequeue operation. The raw data-path APIs gives the
user more control to the enqueue and dequeue operations, including the
capability of precious enqueue/dequeue count, abandoning enqueue or dequeue
at any time, and operation status translation and set on the fly.


Cryptodev PMDs which support the raw data-path APIs will have
`RTE_CRYPTODEV_FF_SYM_RAW_DP` feature flag presented. To use this feature,
the user shall create a local `struct rte_crypto_raw_dp_ctx` buffer and
extend to at least the length returned by `rte_cryptodev_get_raw_dp_ctx_size`
function call. The created buffer is then initialized using
`rte_cryptodev_configure_raw_dp_ctx` function with the `is_update`
parameter as 0. The library and the crypto device driver will then set the
buffer and attach either the cryptodev sym session, the rte\_security session,
or the cryptodev xform for session-less operation into the ctx buffer, and
set the corresponding enqueue and dequeue function handlers based on the
algorithm information stored in the session or xform. When the `is_update`
parameter passed into `rte_cryptodev_configure_raw_dp_ctx` is 1, the driver
will not initialize the buffer but only update the session or xform and
the function handlers accordingly.

After the `struct rte_crypto_raw_dp_ctx` buffer is initialized, it is now
ready for enqueue and dequeue operation. There are two different enqueue
functions: `rte_cryptodev_raw_enqueue` to enqueue single raw data
operation, and `rte_cryptodev_raw_enqueue_burst` to enqueue a descriptor
with multiple operations. In case of the application uses similar approach to
`struct rte_crypto_sym_vec` to manage its data burst but with different
data structure, using the `rte_cryptodev_raw_enqueue_burst` function may be
less efficient as this is a situation where the application has to loop over
all crypto operations to assemble the `struct rte_crypto_sym_vec` descriptor
from its own data structure, and then the driver will loop over them again to
translate every operation in the descriptor to the driver’s specific queue data.
The `rte_cryptodev_raw_enqueue` should be used to save one loop for each data
burst instead.

The `rte_cryptodev_raw_enqueue` and `rte_cryptodev_raw_enqueue_burst`
functions will return or set the enqueue status. `rte_cryptodev_raw_enqueue`
will return the status directly, `rte_cryptodev_raw_enqueue_burst` will
return the number of operations enqueued or stored (explained as follows) and
set the `enqueue_status` buffer provided by the user. The possible
enqueue status values are:

- `1`: the operation(s) is/are enqueued successfully.

- `0`: the operation(s) is/are cached successfully in the crypto device queue
but is not actually enqueued. The user shall call
`rte_cryptodev_raw_enqueue_done` function after the expected operations
are stored. The crypto device will then start enqueuing all of them at
once.

- The negative integer: error occurred during enqueue.


Calling `rte_cryptodev_configure_raw_dp_ctx` with the parameter `is_update`
set as 0 twice without the enqueue function returning or setting enqueue status
to 1 or `rte_cryptodev_raw_enqueue_done` function being called in between will
invalidate any operation stored in the device queue but not enqueued. This
feature is useful when the user wants to abandon partially enqueued operations
for a failed enqueue burst operation and try enqueuing in a whole later.

Similar as enqueue, there are two dequeue functions:
`rte_cryptodev_raw_dequeue` for dequeuing single operation, and
`rte_cryptodev_raw_dequeue_burst` for dequeuing a burst of operations (e.g.
all operations in a `struct rte_crypto_sym_vec` descriptor). The
`rte_cryptodev_raw_dequeue_burst` function allows the user to provide callback
functions to retrieve dequeue count from the enqueued user data and write the
expected status value to the user data on the fly. The dequeue functions also
set the dequeue status:

- `1`: the operation(s) is/are dequeued successfully.

- `0`: the operation(s) is/are completed but is not actually dequeued (hence
still kept in the device queue). The user shall call the
`rte_cryptodev_raw_dequeue_done` function after the expected number of
operations (e.g. all operations in a descriptor) are dequeued. The crypto
device driver will then free them from the queue at once.

- The negative integer: error occurred during dequeue.


Calling `rte_cryptodev_configure_raw_dp_ctx` with the parameter `is_update`
set as 0 twice without the dequeue functions execution changed dequeue\_status
to 1 or `rte_cryptodev_raw_dequeue_done` function being called in between will
revert the crypto device queue’s dequeue effort to the moment when the
`struct rte_crypto_raw_dp_ctx` buffer is initialized. This feature is useful
when the user wants to abandon partially dequeued data and try dequeuing again
later in a whole.

There are a few limitations to the raw data path APIs:

- Only support in-place operations.

- APIs are NOT thread-safe.

- CANNOT mix the raw data-path API’s enqueue with rte\_cryptodev\_enqueue\_burst,
or vice versa.


See _DPDK API Reference_ for details on each API definitions.

## 4.7. Sample code

There are various sample applications that show how to use the cryptodev library,
such as the L2fwd with Crypto sample application (L2fwd-crypto) and
the IPsec Security Gateway application (ipsec-secgw).

While these applications demonstrate how an application can be created to perform
generic crypto operation, the required complexity hides the basic steps of
how to use the cryptodev APIs.

The following sample code shows the basic steps to encrypt several buffers
with AES-CBC (although performing other crypto operations is similar),
using one of the crypto PMDs available in DPDK.

```
/*
 * Simple example to encrypt several buffers with AES-CBC using
 * the Cryptodev APIs.
 */

#define MAX_SESSIONS         1024
#define NUM_MBUFS            1024
#define POOL_CACHE_SIZE      128
#define BURST_SIZE           32
#define BUFFER_SIZE          1024
#define AES_CBC_IV_LENGTH    16
#define AES_CBC_KEY_LENGTH   16
#define IV_OFFSET            (sizeof(struct rte_crypto_op) + \
                             sizeof(struct rte_crypto_sym_op))

struct rte_mempool *mbuf_pool, *crypto_op_pool;
struct rte_mempool *session_pool, *session_priv_pool;
unsigned int session_size;
int ret;

/* Initialize EAL. */
ret = rte_eal_init(argc, argv);
if (ret < 0)
    rte_exit(EXIT_FAILURE, "Invalid EAL arguments\n");

uint8_t socket_id = rte_socket_id();

/* Create the mbuf pool. */
mbuf_pool = rte_pktmbuf_pool_create("mbuf_pool",
                                NUM_MBUFS,
                                POOL_CACHE_SIZE,
                                0,
                                RTE_MBUF_DEFAULT_BUF_SIZE,
                                socket_id);
if (mbuf_pool == NULL)
    rte_exit(EXIT_FAILURE, "Cannot create mbuf pool\n");

/*
 * The IV is always placed after the crypto operation,
 * so some private data is required to be reserved.
 */
unsigned int crypto_op_private_data = AES_CBC_IV_LENGTH;

/* Create crypto operation pool. */
crypto_op_pool = rte_crypto_op_pool_create("crypto_op_pool",
                                        RTE_CRYPTO_OP_TYPE_SYMMETRIC,
                                        NUM_MBUFS,
                                        POOL_CACHE_SIZE,
                                        crypto_op_private_data,
                                        socket_id);
if (crypto_op_pool == NULL)
    rte_exit(EXIT_FAILURE, "Cannot create crypto op pool\n");

/* Create the virtual crypto device. */
char args[128];
const char *crypto_name = "crypto_aesni_mb0";
snprintf(args, sizeof(args), "socket_id=%d", socket_id);
ret = rte_vdev_init(crypto_name, args);
if (ret != 0)
    rte_exit(EXIT_FAILURE, "Cannot create virtual device");

uint8_t cdev_id = rte_cryptodev_get_dev_id(crypto_name);

/* Get private session data size. */
session_size = rte_cryptodev_sym_get_private_session_size(cdev_id);

#ifdef USE_TWO_MEMPOOLS
/* Create session mempool for the session header. */
session_pool = rte_cryptodev_sym_session_pool_create("session_pool",
                                MAX_SESSIONS,
                                0,
                                POOL_CACHE_SIZE,
                                0,
                                socket_id);

/*
 * Create session private data mempool for the
 * private session data for the crypto device.
 */
session_priv_pool = rte_mempool_create("session_pool",
                                MAX_SESSIONS,
                                session_size,
                                POOL_CACHE_SIZE,
                                0, NULL, NULL, NULL,
                                NULL, socket_id,
                                0);

#else
/* Use of the same mempool for session header and private data */
    session_pool = rte_cryptodev_sym_session_pool_create("session_pool",
                                MAX_SESSIONS * 2,
                                session_size,
                                POOL_CACHE_SIZE,
                                0,
                                socket_id);

    session_priv_pool = session_pool;

#endif

/* Configure the crypto device. */
struct rte_cryptodev_config conf = {
    .nb_queue_pairs = 1,
    .socket_id = socket_id
};

struct rte_cryptodev_qp_conf qp_conf = {
    .nb_descriptors = 2048,
    .mp_session = session_pool,
    .mp_session_private = session_priv_pool
};

if (rte_cryptodev_configure(cdev_id, &conf) < 0)
    rte_exit(EXIT_FAILURE, "Failed to configure cryptodev %u", cdev_id);

if (rte_cryptodev_queue_pair_setup(cdev_id, 0, &qp_conf, socket_id) < 0)
    rte_exit(EXIT_FAILURE, "Failed to setup queue pair\n");

if (rte_cryptodev_start(cdev_id) < 0)
    rte_exit(EXIT_FAILURE, "Failed to start device\n");

/* Create the crypto transform. */
uint8_t cipher_key[16] = {0};
struct rte_crypto_sym_xform cipher_xform = {
    .next = NULL,
    .type = RTE_CRYPTO_SYM_XFORM_CIPHER,
    .cipher = {
        .op = RTE_CRYPTO_CIPHER_OP_ENCRYPT,
        .algo = RTE_CRYPTO_CIPHER_AES_CBC,
        .key = {
            .data = cipher_key,
            .length = AES_CBC_KEY_LENGTH
        },
        .iv = {
            .offset = IV_OFFSET,
            .length = AES_CBC_IV_LENGTH
        }
    }
};

/* Create crypto session and initialize it for the crypto device. */
struct rte_cryptodev_sym_session *session;
session = rte_cryptodev_sym_session_create(cdev_id, &cipher_xform,
                session_pool);
if (session == NULL)
    rte_exit(EXIT_FAILURE, "Session could not be created\n");

/* Get a burst of crypto operations. */
struct rte_crypto_op *crypto_ops[BURST_SIZE];
if (rte_crypto_op_bulk_alloc(crypto_op_pool,
                        RTE_CRYPTO_OP_TYPE_SYMMETRIC,
                        crypto_ops, BURST_SIZE) == 0)
    rte_exit(EXIT_FAILURE, "Not enough crypto operations available\n");

/* Get a burst of mbufs. */
struct rte_mbuf *mbufs[BURST_SIZE];
if (rte_pktmbuf_alloc_bulk(mbuf_pool, mbufs, BURST_SIZE) < 0)
    rte_exit(EXIT_FAILURE, "Not enough mbufs available");

/* Initialize the mbufs and append them to the crypto operations. */
unsigned int i;
for (i = 0; i < BURST_SIZE; i++) {
    if (rte_pktmbuf_append(mbufs[i], BUFFER_SIZE) == NULL)
        rte_exit(EXIT_FAILURE, "Not enough room in the mbuf\n");
    crypto_ops[i]->sym->m_src = mbufs[i];
}

/* Set up the crypto operations. */
for (i = 0; i < BURST_SIZE; i++) {
    struct rte_crypto_op *op = crypto_ops[i];
    /* Modify bytes of the IV at the end of the crypto operation */
    uint8_t *iv_ptr = rte_crypto_op_ctod_offset(op, uint8_t *,
                                            IV_OFFSET);

    generate_random_bytes(iv_ptr, AES_CBC_IV_LENGTH);

    op->sym->cipher.data.offset = 0;
    op->sym->cipher.data.length = BUFFER_SIZE;

    /* Attach the crypto session to the operation */
    rte_crypto_op_attach_sym_session(op, session);
}

/* Enqueue the crypto operations in the crypto device. */
uint16_t num_enqueued_ops = rte_cryptodev_enqueue_burst(cdev_id, 0,
                                        crypto_ops, BURST_SIZE);

/*
 * Dequeue the crypto operations until all the operations
 * are processed in the crypto device.
 */
uint16_t num_dequeued_ops, total_num_dequeued_ops = 0;
do {
    struct rte_crypto_op *dequeued_ops[BURST_SIZE];
    num_dequeued_ops = rte_cryptodev_dequeue_burst(cdev_id, 0,
                                    dequeued_ops, BURST_SIZE);
    total_num_dequeued_ops += num_dequeued_ops;

    /* Check if operation was processed successfully */
    for (i = 0; i < num_dequeued_ops; i++) {
        if (dequeued_ops[i]->status != RTE_CRYPTO_OP_STATUS_SUCCESS)
            rte_exit(EXIT_FAILURE,
                    "Some operations were not processed correctly");
    }

    rte_mempool_put_bulk(crypto_op_pool, (void **)dequeued_ops,
                                        num_dequeued_ops);
} while (total_num_dequeued_ops < num_enqueued_ops);

```

## 4.8. Asymmetric Cryptography

The cryptodev library currently provides support for the following asymmetric
Crypto operations; RSA, Modular exponentiation and inversion, Diffie-Hellman and
Elliptic Curve Diffie-Hellman public and/or private key generation and shared
secret compute, DSA and EdDSA signature generation and verification.

### 4.8.1. Session and Session Management

Sessions are used in asymmetric cryptographic processing to store the immutable
data defined in asymmetric cryptographic transform which is further used in the
operation processing. Sessions typically stores information, such as, public
and private key information or domain params or prime modulus data i.e. immutable
across data sets. Crypto sessions cache this immutable data in a optimal way for the
underlying PMD and this allows further acceleration of the offload of Crypto workloads.

Like symmetric, the Crypto device framework provides APIs to allocate and initialize
asymmetric sessions for crypto devices, where sessions are mempool objects.
It is the application’s responsibility to create and manage the session mempools.
Application using both symmetric and asymmetric sessions should allocate and maintain
different sessions pools for each type.

An application can use `rte_cryptodev_asym_session_pool_create()` to create a mempool
with a specified number of elements. The element size will allow for the session header,
and the max private session size.
The max private session size is chosen based on available crypto devices,
the biggest private session size is used. This means any of those devices can be used,
and the mempool element will have available space for its private session data.

Once the session mempools have been created, `rte_cryptodev_asym_session_create()`
is used to allocate and initialize an asymmetric session from the given mempool.
An asymmetric transform chain is used to specify the operation and its parameters.
See the section below for details on transforms.

When a session is no longer used, user must call `rte_cryptodev_asym_session_clear()`
for each of the crypto devices that are using the session, to free all driver
private asymmetric session data. Once this is done, session should be freed using
`rte_cryptodev_asym_session_free()` which returns them to their mempool.

### 4.8.2. Asymmetric Sessionless Support

Asymmetric crypto framework supports session-less operations as well.

Fields that should be set by user are:

Member xform of struct rte\_crypto\_asym\_op should point to the user created rte\_crypto\_asym\_xform.
Note that rte\_crypto\_asym\_xform should be immutable for the lifetime of associated crypto\_op.

Member sess\_type of rte\_crypto\_op should also be set to RTE\_CRYPTO\_OP\_SESSIONLESS.

### 4.8.3. Transforms and Transform Chaining

Asymmetric Crypto transforms ( `rte_crypto_asym_xform`) are the mechanism used
to specify the details of the asymmetric Crypto operation. Next pointer within
xform allows transform to be chained together. Also it is important to note that
the order in which the transforms are passed indicates the order of the chaining. Allocation
of the xform structure is in the application domain. To allow future API extensions in a
backwardly compatible manner, e.g. addition of a new parameter, the application should
zero the full xform struct before populating it.

Not all asymmetric crypto xforms are supported for chaining. Currently supported
asymmetric crypto chaining is Diffie-Hellman private key generation followed by
public generation. Also, currently API does not support chaining of symmetric and
asymmetric crypto xforms.

Each xform defines specific asymmetric crypto algo. Currently supported are:
\\* RSA
\\* Modular operations (Exponentiation and Inverse)
\\* Diffie-Hellman
\\* DSA
\\* Elliptic Curve Diffie-Hellman
\\* None - special case where PMD may support a passthrough mode. More for diagnostic purpose

See _DPDK API Reference_ for details on each rte\_crypto\_xxx\_xform struct

### 4.8.4. Asymmetric Operations

The asymmetric Crypto operation structure contains all the mutable data relating
to asymmetric cryptographic processing on an input data buffer. It uses either
RSA, Modular, Diffie-Hellman or DSA operations depending upon session it is attached
to.

Every operation must carry a valid session handle which further carries information
on xform or xform-chain to be performed on op. Every xform type defines its own set
of operational params in their respective rte\_crypto\_xxx\_op\_param struct. Depending
on xform information within session, PMD picks up and process respective op\_param
struct.
Unlike symmetric, asymmetric operations do not use mbufs for input/output.
They operate on data buffer of type `rte_crypto_param`.

See _DPDK API Reference_ for details on each rte\_crypto\_xxx\_op\_param struct

### 4.8.5. Private user data

Similar to symmetric above, asymmetric also has a set and get API that provides a
mechanism for an application to store and retrieve the private user data information
stored along with the crypto session.

```
int rte_cryptodev_asym_session_set_user_data(void *sess,
        void *data, uint16_t size);

void * rte_cryptodev_asym_session_get_user_data(void *sess);

```

Please note the `size` passed to set API cannot be bigger than the predefined
`user_data_sz` when creating the session mempool, otherwise the function will
return an error. Also when `user_data_sz` was defined as `0` when
creating the session mempool, the get API will always return `NULL`.

## 4.9. Asymmetric crypto Sample code

There’s a unit test application test\_cryptodev\_asym.c inside unit test framework that
show how to setup and process asymmetric operations using cryptodev library.

The following code samples are taken from the test application mentioned above,
and show basic steps to compute modular exponentiation using an openssl PMD
available in DPDK (performing other crypto operations is similar except change
to respective op and xform setup).

Note

The following code snippets are taken from multiple functions, so variable
names may differ slightly between sections.

Configure the virtual device, queue pairs, crypto op pool and session mempool.

```
ts_params->op_mpool = rte_crypto_op_pool_create(
		"CRYPTO_ASYM_OP_POOL",
		RTE_CRYPTO_OP_TYPE_ASYMMETRIC,
		TEST_NUM_BUFS, 0,
		0,
		rte_socket_id());
if (ts_params->op_mpool == NULL) {
	RTE_LOG(ERR, USER1, "Can't create ASYM_CRYPTO_OP_POOL\n");
	return TEST_FAILED;
}

/* Create an OPENSSL device if required */
if (gbl_driver_id == rte_cryptodev_driver_id_get(
		RTE_STR(CRYPTODEV_NAME_OPENSSL_PMD))) {
	nb_devs = rte_cryptodev_device_count_by_driver(
			rte_cryptodev_driver_id_get(
			RTE_STR(CRYPTODEV_NAME_OPENSSL_PMD)));
	if (nb_devs < 1) {
		ret = rte_vdev_init(
			RTE_STR(CRYPTODEV_NAME_OPENSSL_PMD),
			NULL);

		TEST_ASSERT(ret == 0, "Failed to create "
			"instance of pmd : %s",
			RTE_STR(CRYPTODEV_NAME_OPENSSL_PMD));
	}
}

/* Get list of valid crypto devs */
nb_devs = rte_cryptodev_devices_get(
			rte_cryptodev_driver_name_get(gbl_driver_id),
			valid_devs, RTE_CRYPTO_MAX_DEVS);
if (nb_devs < 1) {
	RTE_LOG(ERR, USER1, "No crypto devices found?\n");
	return TEST_SKIPPED;
}

/*
 * Get first valid asymmetric device found in test suite param and
 * break
 */
for (i = 0; i < nb_devs ; i++) {
	rte_cryptodev_info_get(valid_devs[i], &info);
	if (info.feature_flags & RTE_CRYPTODEV_FF_ASYMMETRIC_CRYPTO) {
		dev_id = ts_params->valid_devs[0] = valid_devs[i];
		break;
	}
}

if (dev_id == -1) {
	RTE_LOG(ERR, USER1, "Device doesn't support asymmetric. "
		"Test skipped.\n");
	return TEST_FAILED;
}

/* Set valid device count */
ts_params->valid_dev_count = nb_devs;

/* configure device with num qp */
ts_params->conf.nb_queue_pairs = info.max_nb_queue_pairs;
ts_params->conf.socket_id = SOCKET_ID_ANY;
ts_params->conf.ff_disable = RTE_CRYPTODEV_FF_SECURITY |
		RTE_CRYPTODEV_FF_SYMMETRIC_CRYPTO;
TEST_ASSERT_SUCCESS(rte_cryptodev_configure(dev_id,
		&ts_params->conf),
		"Failed to configure cryptodev %u with %u qps",
		dev_id, ts_params->conf.nb_queue_pairs);

/* configure qp */
ts_params->qp_conf.nb_descriptors = DEFAULT_NUM_OPS_INFLIGHT;
ts_params->qp_conf.mp_session = ts_params->session_mpool;
for (qp_id = 0; qp_id < info.max_nb_queue_pairs; qp_id++) {
	TEST_ASSERT_SUCCESS(rte_cryptodev_queue_pair_setup(
		dev_id, qp_id, &ts_params->qp_conf,
		rte_cryptodev_socket_id(dev_id)),
		"Failed to setup queue pair %u on cryptodev %u ASYM",
		qp_id, dev_id);
}

ts_params->session_mpool = rte_cryptodev_asym_session_pool_create(
		"test_asym_sess_mp", TEST_NUM_SESSIONS, 0, 0,
		SOCKET_ID_ANY);

TEST_ASSERT_NOT_NULL(ts_params->session_mpool,
		"session mempool allocation failed");

```

Create MODEX data vectors.

```
uint8_t mod_p[] = {
	0x00, 0xb3, 0xa1, 0xaf, 0xb7, 0x13, 0x08, 0x00,
	0x0a, 0x35, 0xdc, 0x2b, 0x20, 0x8d, 0xa1, 0xb5,
	0xce, 0x47, 0x8a, 0xc3, 0x80, 0xf4, 0x7d, 0x4a,
	0xa2, 0x62, 0xfd, 0x61, 0x7f, 0xb5, 0xa8, 0xde,
	0x0a, 0x17, 0x97, 0xa0, 0xbf, 0xdf, 0x56, 0x5a,
	0x3d, 0x51, 0x56, 0x4f, 0x70, 0x70, 0x3f, 0x63,
	0x6a, 0x44, 0x5b, 0xad, 0x84, 0x0d, 0x3f, 0x27,
	0x6e, 0x3b, 0x34, 0x91, 0x60, 0x14, 0xb9, 0xaa,
	0x72, 0xfd, 0xa3, 0x64, 0xd2, 0x03, 0xa7, 0x53,
	0x87, 0x9e, 0x88, 0x0b, 0xc1, 0x14, 0x93, 0x1a,
	0x62, 0xff, 0xb1, 0x5d, 0x74, 0xcd, 0x59, 0x63,
	0x18, 0x11, 0x3d, 0x4f, 0xba, 0x75, 0xd4, 0x33,
	0x4e, 0x23, 0x6b, 0x7b, 0x57, 0x44, 0xe1, 0xd3,
	0x03, 0x13, 0xa6, 0xf0, 0x8b, 0x60, 0xb0, 0x9e,
	0xee, 0x75, 0x08, 0x9d, 0x71, 0x63, 0x13, 0xcb,
	0xa6, 0x81, 0x92, 0x14, 0x03, 0x22, 0x2d, 0xde,
	0x55
};

uint8_t mod_e[] = {0x01, 0x00, 0x01};

```

Setup crypto xform to do modular exponentiation using data vectors.

```
struct rte_crypto_asym_xform modex_xform = {
	.next = NULL,
	.xform_type = RTE_CRYPTO_ASYM_XFORM_MODEX,
	.modex = {
		.modulus = {
			.data = mod_p,
			.length = sizeof(mod_p)
		},
		.exponent = {
			.data = mod_e,
			.length = sizeof(mod_e)
		}
	}
};

```

Generate crypto op, create and attach a session, then process packets.

```
op = rte_crypto_op_alloc(op_mpool, RTE_CRYPTO_OP_TYPE_ASYMMETRIC);
if (!op) {
	RTE_LOG(ERR, USER1,
		"line %u FAILED: %s",
		__LINE__, "Failed to allocate asymmetric crypto "
		"operation struct");
	status = TEST_FAILED;
	goto error_exit;
}

ret = rte_cryptodev_asym_session_create(dev_id, &modex_xform, sess_mpool, &sess);
if (ret < 0) {
	RTE_LOG(ERR, USER1,
			 "line %u "
			"FAILED: %s", __LINE__,
			"Session creation failed");
	status = (ret == -ENOTSUP) ? TEST_SKIPPED : TEST_FAILED;
	goto error_exit;
}

asym_op = op->asym;
memcpy(input, base, sizeof(base));
asym_op->modex.base.data = input;
asym_op->modex.base.length = sizeof(base);
asym_op->modex.result.data = result;
asym_op->modex.result.length = sizeof(result);
/* attach asymmetric crypto session to crypto operations */
rte_crypto_op_attach_asym_session(op, sess);

RTE_LOG(DEBUG, USER1, "Process ASYM operation");
/* Process crypto operation */
if (rte_cryptodev_enqueue_burst(dev_id, 0, &op, 1) != 1) {
	RTE_LOG(ERR, USER1,
			"line %u FAILED: %s",
			__LINE__, "Error sending packet for operation");
	status = TEST_FAILED;
	goto error_exit;
}

while (rte_cryptodev_dequeue_burst(dev_id, 0, &result_op, 1) == 0)
	rte_pause();

if (result_op == NULL) {
	RTE_LOG(ERR, USER1,
			"line %u FAILED: %s",
			__LINE__, "Failed to process asym crypto op");
	status = TEST_FAILED;
	goto error_exit;
}

```

Note

The `rte_cryptodev_asym_session` struct is hidden from the application.
The `sess` pointer used above is a void pointer.

### 4.9.1. Asymmetric Crypto Device API

The cryptodev Library API is described in the
[DPDK API Reference](https://doc.dpdk.org/api/)

## 4.10. Device Statistics

The Cryptodev library has support for displaying Crypto device information
through the Telemetry interface. Telemetry commands that can be used
are shown below.

1. Get the list of available Crypto devices by ID:





```
   --> /cryptodev/list
{"/cryptodev/list": [0, 1, 2, 3]}

```

2. Get general information from a Crypto device:





```
   --> /cryptodev/info,0
{"/cryptodev/info": {"device_name": "0000:1c:01.0_qat_sym",
"max_nb_queue_pairs": 2}}

```

3. Get the statistics for a particular Crypto device:





```
   --> /cryptodev/stats,0
{"/cryptodev/stats": {"enqueued_count": 0, "dequeued_count": 0,
"enqueue_err_count": 0, "dequeue_err_count": 0}}

```

4. Get the capabilities of a particular Crypto device:





```
   --> /cryptodev/caps,0
{"/cryptodev/caps": {"crypto_caps": [<array of serialized bytes of\
capabilities>], "crypto_caps_n": <number of capabilities>}}

```


For more information on how to use the Telemetry interface, see
the [DPDK Telemetry User Guide](https://doc.dpdk.org/guides-25.03/howto/telemetry.html).


================================================================================
FILE: doc.dpdk.org_guides-25.03_prog_guide_env_abstraction_layer.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/prog_guide/env_abstraction_layer.html"
title: "1. Environment Abstraction Layer (EAL) Library — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Programmer’s Guide](https://doc.dpdk.org/guides-25.03/prog_guide/index.html)
- 1\. Environment Abstraction Layer (EAL) Library
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/prog_guide/env_abstraction_layer.rst.txt)

* * *

# 1\. Environment Abstraction Layer (EAL) Library

The Environment Abstraction Layer (EAL) is responsible for gaining access to low-level resources such as hardware and memory space.
It provides a generic interface that hides the environment specifics from the applications and libraries.
It is the responsibility of the initialization routine to decide how to allocate these resources
(that is, memory space, devices, timers, consoles, and so on).

Typical services expected from the EAL are:

- DPDK Loading and Launching:
The DPDK and its application are linked as a single application and must be loaded by some means.

- Core Affinity/Assignment Procedures:
The EAL provides mechanisms for assigning execution units to specific cores as well as creating execution instances.

- System Memory Reservation:
The EAL facilitates the reservation of different memory zones, for example, physical memory areas for device interactions.

- Trace and Debug Functions: Logs, dump\_stack, panic and so on.

- Utility Functions: Spinlocks and atomic counters that are not provided in libc.

- CPU Feature Identification: Determine at runtime if a particular feature, for example, Intel® AVX is supported.
Determine if the current CPU supports the feature set that the binary was compiled for.

- Interrupt Handling: Interfaces to register/unregister callbacks to specific interrupt sources.

- Alarm Functions: Interfaces to set/remove callbacks to be run at a specific time.


## 1.1. EAL in a Linux-userland Execution Environment

In a Linux user space environment, the DPDK application runs as a user-space application using the pthread library.

The EAL performs physical memory allocation using mmap() in hugetlbfs (using huge page sizes to increase performance).
This memory is exposed to DPDK service layers such as the [Memory Pool Library](https://doc.dpdk.org/guides-25.03/prog_guide/mempool_lib.html).

At this point, the DPDK services layer will be initialized, then through pthread setaffinity calls,
each execution unit will be assigned to a specific logical core to run as a user-level thread.

The time reference is provided by the CPU Time-Stamp Counter (TSC) or by the HPET kernel API through a mmap() call.

### 1.1.1. Initialization and Core Launching

Part of the initialization is done by the start function of glibc.
A check is also performed at initialization time to ensure that the micro architecture type chosen in the config file is supported by the CPU.
Then, the main() function is called. The core initialization and launch is done in rte\_eal\_init() (see the API documentation).
It consist of calls to the pthread library (more specifically, pthread\_self(), pthread\_create(), and pthread\_setaffinity\_np()).

![../_images/linuxapp_launch.svg](https://doc.dpdk.org/guides-25.03/_images/linuxapp_launch.svg)

Fig. 1.1 EAL Initialization in a Linux Application Environment

Note

Initialization of objects, such as memory zones, rings, memory pools, lpm tables and hash tables,
should be done as part of the overall application initialization on the main lcore.
The creation and initialization functions for these objects are not multi-thread safe.
However, once initialized, the objects themselves can safely be used in multiple threads simultaneously.

### 1.1.2. Shutdown and Cleanup

During the initialization of EAL resources such as hugepage backed memory can be
allocated by core components. The memory allocated during `rte_eal_init()`
can be released by calling the `rte_eal_cleanup()` function. Refer to the
API documentation for details.

### 1.1.3. Multi-process Support

The Linux EAL allows a multi-process as well as a multi-threaded (pthread) deployment model.
See chapter [Multi-process Support](https://doc.dpdk.org/guides-25.03/prog_guide/multi_proc_support.html) for more details.

### 1.1.4. Memory Mapping Discovery and Memory Reservation

The allocation of large contiguous physical memory is done using hugepages.
The EAL provides an API to reserve named memory zones in this contiguous memory.
The physical address of the reserved memory for that memory zone is also returned to the user by the memory zone reservation API.

There are two modes in which DPDK memory subsystem can operate: dynamic mode,
and legacy mode. Both modes are explained below.

Note

Memory reservations done using the APIs provided by rte\_malloc
are also backed by hugepages unless `--no-huge` option is given.

#### 1.1.4.1. Dynamic Memory Mode

Currently, this mode is only supported on Linux and Windows.

In this mode, usage of hugepages by DPDK application will grow and shrink based
on application’s requests. Any memory allocation through `rte_malloc()`,
`rte_memzone_reserve()` or other methods, can potentially result in more
hugepages being reserved from the system. Similarly, any memory deallocation can
potentially result in hugepages being released back to the system.

Memory allocated in this mode is not guaranteed to be IOVA-contiguous. If large
chunks of IOVA-contiguous are required (with “large” defined as “more than one
page”), it is recommended to either use VFIO driver for all physical devices (so
that IOVA and VA addresses can be the same, thereby bypassing physical addresses
entirely), or use legacy memory mode.

For chunks of memory which must be IOVA-contiguous, it is recommended to use
`rte_memzone_reserve()` function with `RTE_MEMZONE_IOVA_CONTIG` flag
specified. This way, memory allocator will ensure that, whatever memory mode is
in use, either reserved memory will satisfy the requirements, or the allocation
will fail.

There is no need to preallocate any memory at startup using `-m` or
`--socket-mem` command-line parameters, however it is still possible to do so,
in which case preallocate memory will be “pinned” (i.e. will never be released
by the application back to the system). It will be possible to allocate more
hugepages, and deallocate those, but any preallocated pages will not be freed.
If neither `-m` nor `--socket-mem` were specified, no memory will be
preallocated, and all memory will be allocated at runtime, as needed.

Another available option to use in dynamic memory mode is
`--single-file-segments` command-line option. This option will put pages in
single files (per memseg list), as opposed to creating a file per page. This is
normally not needed, but can be useful for use cases like userspace vhost, where
there is limited number of page file descriptors that can be passed to VirtIO.

If the application (or DPDK-internal code, such as device drivers) wishes to
receive notifications about newly allocated memory, it is possible to register
for memory event callbacks via `rte_mem_event_callback_register()` function.
This will call a callback function any time DPDK’s memory map has changed.

If the application (or DPDK-internal code, such as device drivers) wishes to be
notified about memory allocations above specified threshold (and have a chance
to deny them), allocation validator callbacks are also available via
`rte_mem_alloc_validator_callback_register()` function.

A default validator callback is provided by EAL, which can be enabled with a
`--socket-limit` command-line option, for a simple way to limit maximum amount
of memory that can be used by DPDK application.

Warning

Memory subsystem uses DPDK IPC internally, so memory allocations/callbacks
and IPC must not be mixed: it is not safe to allocate/free memory inside
memory-related or IPC callbacks, and it is not safe to use IPC inside
memory-related callbacks. See chapter
[Multi-process Support](https://doc.dpdk.org/guides-25.03/prog_guide/multi_proc_support.html) for more details about DPDK IPC.

#### 1.1.4.2. Legacy Memory Mode

This mode is enabled by specifying `--legacy-mem` command-line switch to the
EAL. This switch will have no effect on FreeBSD as FreeBSD only supports
legacy mode anyway.

This mode mimics historical behavior of EAL. That is, EAL will reserve all
memory at startup, sort all memory into large IOVA-contiguous chunks, and will
not allow acquiring or releasing hugepages from the system at runtime.

If neither `-m` nor `--socket-mem` were specified, the entire available
hugepage memory will be preallocated.

#### 1.1.4.3. Hugepage Allocation Matching

This behavior is enabled by specifying the `--match-allocations` command-line
switch to the EAL. This switch is Linux-only and not supported with
`--legacy-mem` nor `--no-huge`.

Some applications using memory event callbacks may require that hugepages be
freed exactly as they were allocated. These applications may also require
that any allocation from the malloc heap not span across allocations
associated with two different memory event callbacks. Hugepage allocation
matching can be used by these types of applications to satisfy both of these
requirements. This can result in some increased memory usage which is
very dependent on the memory allocation patterns of the application.

#### 1.1.4.4. 32-bit Support

Additional restrictions are present when running in 32-bit mode. In dynamic
memory mode, by default maximum of 2 gigabytes of VA space will be preallocated,
and all of it will be on main lcore NUMA node unless `--socket-mem` flag is
used.

In legacy mode, VA space will only be preallocated for segments that were
requested (plus padding, to keep IOVA-contiguousness).

#### 1.1.4.5. Maximum Amount of Memory

All possible virtual memory space that can ever be used for hugepage mapping in
a DPDK process is preallocated at startup, thereby placing an upper limit on how
much memory a DPDK application can have. DPDK memory is stored in segment lists,
each segment is strictly one physical page. It is possible to change the amount
of virtual memory being preallocated at startup by editing the following config
variables:

- `RTE_MAX_MEMSEG_LISTS` controls how many segment lists can DPDK have

- `RTE_MAX_MEM_MB_PER_LIST` controls how much megabytes of memory each
segment list can address

- `RTE_MAX_MEMSEG_PER_LIST` controls how many segments each segment list
can have

- `RTE_MAX_MEMSEG_PER_TYPE` controls how many segments each memory type
can have (where “type” is defined as “page size + NUMA node” combination)

- `RTE_MAX_MEM_MB_PER_TYPE` controls how much megabytes of memory each
memory type can address

- `RTE_MAX_MEM_MB` places a global maximum on the amount of memory
DPDK can reserve


Normally, these options do not need to be changed.

Note

Preallocated virtual memory is not to be confused with preallocated hugepage
memory! All DPDK processes preallocate virtual memory at startup. Hugepages
can later be mapped into that preallocated VA space (if dynamic memory mode
is enabled), and can optionally be mapped into it at startup.

#### 1.1.4.6. Hugepage Mapping

Below is an overview of methods used for each OS to obtain hugepages,
explaining why certain limitations and options exist in EAL.
See the user guide for a specific OS for configuration details.

FreeBSD uses `contigmem` kernel module
to reserve a fixed number of hugepages at system start,
which are mapped by EAL at initialization using a specific `sysctl()`.

Windows EAL allocates hugepages from the OS as needed using Win32 API,
so available amount depends on the system load.
It uses `virt2phys` kernel module to obtain physical addresses,
unless running in IOVA-as-VA mode (e.g. forced with `--iova-mode=va`).

Linux allows to select any combination of the following:

- use files in hugetlbfs (the default)
or anonymous mappings ( `--in-memory`);

- map each hugepage from its own file (the default)
or map multiple hugepages from one big file ( `--single-file-segments`).


Mapping hugepages from files in hugetlbfs is essential for multi-process,
because secondary processes need to map the same hugepages.
EAL creates files like `rtemap_0`
in directories specified with `--huge-dir` option
(or in the mount point for a specific hugepage size).
The `rte` prefix can be changed using `--file-prefix`.
This may be needed for running multiple primary processes
that share a hugetlbfs mount point.
Each backing file by default corresponds to one hugepage,
it is opened and locked for the entire time the hugepage is used.
This may exhaust the number of open files limit ( `NOFILE`).
See [Segment File Descriptors](https://doc.dpdk.org/guides-25.03/prog_guide/env_abstraction_layer.html#segment-file-descriptors) section
on how the number of open backing file descriptors can be reduced.

In dynamic memory mode, EAL removes a backing hugepage file
when all pages mapped from it are freed back to the system.
However, backing files may persist after the application terminates
in case of a crash or a leak of DPDK memory (e.g. `rte_free()` is missing).
This reduces the number of hugepages available to other processes
as reported by `/sys/kernel/mm/hugepages/hugepages-*/free_hugepages`.
EAL can remove the backing files after opening them for mapping
if `--huge-unlink` is given to avoid polluting hugetlbfs.
However, since it disables multi-process anyway,
using anonymous mapping ( `--in-memory`) is recommended instead.

[EAL memory allocator](https://doc.dpdk.org/guides-25.03/prog_guide/env_abstraction_layer.html#malloc) relies on hugepages being zero-filled.
Hugepages are cleared by the kernel when a file in hugetlbfs or its part
is mapped for the first time system-wide
to prevent data leaks from previous users of the same hugepage.
EAL ensures this behavior by removing existing backing files at startup
and by recreating them before opening for mapping (as a precaution).

One exception is `--huge-unlink=never` mode.
It is used to speed up EAL initialization, usually on application restart.
Clearing memory constitutes more than 95% of hugepage mapping time.
EAL can save it by remapping existing backing files
with all the data left in the mapped hugepages (“dirty” memory).
Such segments are marked with `RTE_MEMSEG_FLAG_DIRTY`.
Memory allocator detects dirty segments and handles them accordingly,
in particular, it clears memory requested with `rte_zmalloc*()`.
In this mode EAL also does not remove a backing file
when all pages mapped from it are freed,
because they are intended to be reusable at restart.

Anonymous mapping does not allow multi-process architecture.
This mode does not use hugetlbfs
and thus does not require root permissions for memory management
(the limit of locked memory amount, `MEMLOCK`, still applies).
It is free of filename conflict and leftover file issues.
If `memfd_create(2)` is supported both at build and run time,
DPDK memory manager can provide file descriptors for memory segments,
which are required for VirtIO with vhost-user backend.
This can exhaust the number of open files limit ( `NOFILE`)
despite not creating any visible files.
See [Segment File Descriptors](https://doc.dpdk.org/guides-25.03/prog_guide/env_abstraction_layer.html#segment-file-descriptors) section
on how the number of open file descriptors used by EAL can be reduced.

#### 1.1.4.7. Segment File Descriptors

On Linux, in most cases, EAL will store segment file descriptors in EAL. This
can become a problem when using smaller page sizes due to underlying limitations
of `glibc` library. For example, Linux API calls such as `select()` may not
work correctly because `glibc` does not support more than certain number of
file descriptors.

There are two possible solutions for this problem. The recommended solution is
to use `--single-file-segments` mode, as that mode will not use a file
descriptor per each page, and it will keep compatibility with Virtio with
vhost-user backend. This option is not available when using `--legacy-mem`
mode.

Another option is to use bigger page sizes. Since fewer pages are required to
cover the same memory area, fewer file descriptors will be stored internally
by EAL.

#### 1.1.4.8. Hugepage Worker Stacks

When the `--huge-worker-stack[=size]` EAL option is specified, worker
thread stacks are allocated from hugepage memory local to the NUMA node
of the thread. Worker stack size defaults to system pthread stack size
if the optional size parameter is not specified.

Warning

Stacks allocated from hugepage memory are not protected by guard
pages. Worker stacks must be sufficiently sized to prevent stack
overflow when this option is used.

As with normal thread stacks, hugepage worker thread stack size is
fixed and is not dynamically resized. Therefore, an application that
is free of stack page faults under a given load should be safe with
hugepage worker thread stacks given the same thread stack size and
loading conditions.

### 1.1.5. Support for Externally Allocated Memory

It is possible to use externally allocated memory in DPDK. There are two ways in
which using externally allocated memory can work: the malloc heap API’s, and
manual memory management.

- Using heap API’s for externally allocated memory


Using a set of malloc heap API’s is the recommended way to use externally
allocated memory in DPDK. In this way, support for externally allocated memory
is implemented through overloading the socket ID - externally allocated heaps
will have socket ID’s that would be considered invalid under normal
circumstances. Requesting an allocation to take place from a specified
externally allocated memory is a matter of supplying the correct socket ID to
DPDK allocator, either directly (e.g. through a call to `rte_malloc`) or
indirectly (through data structure-specific allocation API’s such as
`rte_ring_create`). Using these API’s also ensures that mapping of externally
allocated memory for DMA is also performed on any memory segment that is added
to a DPDK malloc heap.

Since there is no way DPDK can verify whether memory is available or valid, this
responsibility falls on the shoulders of the user. All multiprocess
synchronization is also user’s responsibility, as well as ensuring that all
calls to add/attach/detach/remove memory are done in the correct order. It is
not required to attach to a memory area in all processes - only attach to memory
areas as needed.

The expected workflow is as follows:

- Get a pointer to memory area

- Create a named heap

- Add memory area(s) to the heap

- If IOVA table is not specified, IOVA addresses will be assumed to be
unavailable, and DMA mappings will not be performed

- Other processes must attach to the memory area before they can use it


- Get socket ID used for the heap

- Use normal DPDK allocation procedures, using supplied socket ID

- If memory area is no longer needed, it can be removed from the heap

- Other processes must detach from this memory area before it can be removed


- If heap is no longer needed, remove it

- Socket ID will become invalid and will not be reused


For more information, please refer to `rte_malloc` API documentation,
specifically the `rte_malloc_heap_*` family of function calls.

- Using externally allocated memory without DPDK API’s


While using heap API’s is the recommended method of using externally allocated
memory in DPDK, there are certain use cases where the overhead of DPDK heap API
is undesirable - for example, when manual memory management is performed on an
externally allocated area. To support use cases where externally allocated
memory will not be used as part of normal DPDK workflow, there is also another
set of API’s under the `rte_extmem_*` namespace.

These API’s are (as their name implies) intended to allow registering or
unregistering externally allocated memory to/from DPDK’s internal page table, to
allow API’s like `rte_mem_virt2memseg` etc. to work with externally allocated
memory. Memory added this way will not be available for any regular DPDK
allocators; DPDK will leave this memory for the user application to manage.

The expected workflow is as follows:

- Get a pointer to memory area

- Register memory within DPDK

- If IOVA table is not specified, IOVA addresses will be assumed to be
unavailable

- Other processes must attach to the memory area before they can use it


- Perform DMA mapping with `rte_dev_dma_map` if needed

- Use the memory area in your application

- If memory area is no longer needed, it can be unregistered

- If the area was mapped for DMA, unmapping must be performed before
unregistering memory

- Other processes must detach from the memory area before it can be
unregistered


Since these externally allocated memory areas will not be managed by DPDK, it is
therefore up to the user application to decide how to use them and what to do
with them once they’re registered.

### 1.1.6. Per-lcore and Shared Variables

By default, static variables, memory blocks allocated on the DPDK heap,
and other types of memory are shared by all DPDK threads.

An application, a DPDK library, or a PMD may opt to keep per-thread state.

Per-thread data can be maintained using either [lcore variables](https://doc.dpdk.org/guides-25.03/prog_guide/lcore_var.html),
_thread-local storage (TLS)_ (see `rte_per_lcore.h`),
or a static array of `RTE_MAX_LCORE` elements, indexed by `rte_lcore_id()`.
These methods allow per-lcore data to be largely internal to the module
and not directly exposed in its API.
Another approach is to explicitly handle per-thread aspects in the API
(e.g., the ports in the eventdev API).

Lcore variables are suitable for small objects that are statically allocated
at the time of module or application initialization.
An lcore variable takes on one value for each lcore ID-equipped thread
(i.e., for both EAL threads and registered non-EAL threads,
in total `RTE_MAX_LCORE` instances).
The lifetime of lcore variables is independent of the owning threads
and can, therefore, be initialized before the threads are created.

Variables with thread-local storage are allocated when the thread is created
and exist until the thread terminates.
These are applicable for every thread in the process.
Only very small objects should be allocated in TLS,
as large TLS objects can significantly slow down thread creation
and may unnecessarily increase the memory footprint of applications
that extensively use unregistered threads.

A common but now largely obsolete DPDK pattern is to use a static array
sized according to the maximum number of lcore ID-equipped threads
(i.e., with `RTE_MAX_LCORE` elements).
To avoid _false sharing_, each element must be both cache-aligned
and include an `RTE_CACHE_GUARD`.
This extensive use of padding causes internal fragmentation (i.e., unused space)
and reduces cache hit rates.

For more discussions on per-lcore state,
refer to the [lcore variables documentation](https://doc.dpdk.org/guides-25.03/prog_guide/lcore_var.html).

### 1.1.7. Logs

While originally part of EAL, DPDK logging functionality is now provided by the [Log Library](https://doc.dpdk.org/guides-25.03/prog_guide/log_lib.html).

#### 1.1.7.1. Trace and Debug Functions

There are some debug functions to dump the stack in glibc.
The rte\_panic() function can voluntarily provoke a SIG\_ABORT,
which can trigger the generation of a core file, readable by gdb.

### 1.1.8. CPU Feature Identification

The EAL can query the CPU at runtime (using the rte\_cpu\_get\_features() function) to determine which CPU features are available.

### 1.1.9. User Space Interrupt Event

- User Space Interrupt and Alarm Handling in Host Thread


The EAL creates a host thread to poll the UIO device file descriptors to detect the interrupts.
Callbacks can be registered or unregistered by the EAL functions for a specific interrupt event
and are called in the host thread asynchronously.
The EAL also allows timed callbacks to be used in the same way as for NIC interrupts.

Note

In DPDK PMD, the only interrupts handled by the dedicated host thread are those for link status change
(link up and link down notification) and for sudden device removal.

- RX Interrupt Event


The receive and transmit routines provided by each PMD don’t limit themselves to execute in polling thread mode.
To ease the idle polling with tiny throughput, it’s useful to pause the polling and wait until the wake-up event happens.
The RX interrupt is the first choice to be such kind of wake-up event, but probably won’t be the only one.

EAL provides the event APIs for this event-driven thread mode.
Taking Linux as an example, the implementation relies on epoll. Each thread can monitor an epoll instance
in which all the wake-up events’ file descriptors are added. The event file descriptors are created and mapped to
the interrupt vectors according to the UIO/VFIO spec.
From FreeBSD’s perspective, kqueue is the alternative way, but not implemented yet.

EAL initializes the mapping between event file descriptors and interrupt vectors, while each device initializes the mapping
between interrupt vectors and queues. In this way, EAL actually is unaware of the interrupt cause on the specific vector.
The eth\_dev driver takes responsibility to program the latter mapping.

Note

Per queue RX interrupt event is only allowed in VFIO which supports multiple MSI-X vector. In UIO, the RX interrupt
together with other interrupt causes shares the same vector. In this case, when RX interrupt and LSC(link status change)
interrupt are both enabled(intr\_conf.lsc == 1 && intr\_conf.rxq == 1), only the former is capable.

The RX interrupt are controlled/enabled/disabled by ethdev APIs - ‘rte\_eth\_dev\_rx\_intr\_\*’. They return failure if the PMD
hasn’t support them yet. The intr\_conf.rxq flag is used to turn on the capability of RX interrupt per device.

- Device Removal Event


This event is triggered by a device being removed at a bus level. Its
underlying resources may have been made unavailable (i.e. PCI mappings
unmapped). The PMD must make sure that on such occurrence, the application can
still safely use its callbacks.

This event can be subscribed to in the same way one would subscribe to a link
status change event. The execution context is thus the same, i.e. it is the
dedicated interrupt host thread.

Considering this, it is likely that an application would want to close a
device having emitted a Device Removal Event. In such case, calling
`rte_eth_dev_close()` can trigger it to unregister its own Device Removal Event
callback. Care must be taken not to close the device from the interrupt handler
context. It is necessary to reschedule such closing operation.

### 1.1.10. Block list

The EAL PCI device block list functionality can be used to mark certain NIC ports as unavailable,
so they are ignored by the DPDK.
The ports to be blocked are identified using the PCIe\* description (Domain:Bus:Device.Function).

### 1.1.11. Misc Functions

Locks and atomic operations are per-architecture (i686 and x86\_64).

### 1.1.12. Lock annotations

R/W locks, seq locks and spinlocks have been instrumented to help developers in
catching issues in DPDK.

This instrumentation relies on
[clang Thread Safety checks](https://clang.llvm.org/docs/ThreadSafetyAnalysis.html).
All attributes are prefixed with \_\_rte and are fully described in the clang
documentation.

Some general comments:

- it is important that lock requirements are expressed at the function
declaration level in headers so that other code units can be inspected,

- when some global lock is necessary to some user-exposed API, it is preferred
to expose it via an internal helper rather than expose the global variable,

- there are a list of known limitations with clang instrumentation, but before
waiving checks with `__rte_no_thread_safety_analysis` in your code, please
discuss it on the mailing list,


The checks are enabled by default for libraries and drivers.
They can be disabled by setting `annotate_locks` to `false` in
the concerned library/driver `meson.build`.

### 1.1.13. IOVA Mode Detection

IOVA Mode is selected by considering what the current usable Devices on the
system require and/or support.

On FreeBSD, RTE\_IOVA\_PA is always the default. On Linux, the IOVA mode is
detected based on a 2-step heuristic detailed below.

For the first step, EAL asks each bus its requirement in terms of IOVA mode
and decides on a preferred IOVA mode.

- if all buses report RTE\_IOVA\_PA, then the preferred IOVA mode is RTE\_IOVA\_PA,

- if all buses report RTE\_IOVA\_VA, then the preferred IOVA mode is RTE\_IOVA\_VA,

- if all buses report RTE\_IOVA\_DC, no bus expressed a preference, then the
preferred mode is RTE\_IOVA\_DC,

- if the buses disagree (at least one wants RTE\_IOVA\_PA and at least one wants
RTE\_IOVA\_VA), then the preferred IOVA mode is RTE\_IOVA\_DC (see below with the
check on Physical Addresses availability),


If the buses have expressed no preference on which IOVA mode to pick, then a
default is selected using the following logic:

- if physical addresses are not available, RTE\_IOVA\_VA mode is used

- if /sys/kernel/iommu\_groups is not empty, RTE\_IOVA\_VA mode is used

- otherwise, RTE\_IOVA\_PA mode is used


In the case when the buses had disagreed on their preferred IOVA mode, part of
the buses won’t work because of this decision.

The second step checks if the preferred mode complies with the Physical
Addresses availability since those are only available to root user in recent
kernels. Namely, if the preferred mode is RTE\_IOVA\_PA but there is no access to
Physical Addresses, then EAL init fails early, since later probing of the
devices would fail anyway.

Note

The RTE\_IOVA\_VA mode is preferred as the default in most cases for the
following reasons:

- All drivers are expected to work in RTE\_IOVA\_VA mode, irrespective of
physical address availability.

- By default, the mempool, first asks for IOVA-contiguous memory using
`RTE_MEMZONE_IOVA_CONTIG`. This is slow in RTE\_IOVA\_PA mode and it may
affect the application boot time.

- It is easy to enable large amount of IOVA-contiguous memory use cases
with IOVA in VA mode.


It is expected that all PCI drivers work in both RTE\_IOVA\_PA and
RTE\_IOVA\_VA modes.

If a PCI driver does not support RTE\_IOVA\_PA mode, the
`RTE_PCI_DRV_NEED_IOVA_AS_VA` flag is used to dictate that this PCI
driver can only work in RTE\_IOVA\_VA mode.

### 1.1.14. IOVA Mode Configuration

Auto detection of the IOVA mode, based on probing the bus and IOMMU configuration, may not report
the desired addressing mode when virtual devices that are not directly attached to the bus are present.
To facilitate forcing the IOVA mode to a specific value the EAL command line option `--iova-mode` can
be used to select either physical addressing(‘pa’) or virtual addressing(‘va’).

### 1.1.15. Max SIMD bitwidth

The EAL provides a single setting to limit the max SIMD bitwidth used by DPDK,
which is used in determining the vector path, if any, chosen by a component.
The value can be set at runtime by an application using the
‘rte\_vect\_set\_max\_simd\_bitwidth(uint16\_t bitwidth)’ function,
which should only be called once at initialization, before EAL init.
The value can be overridden by the user using the EAL command-line option ‘–force-max-simd-bitwidth’.

When choosing a vector path, along with checking the CPU feature support,
the value of the max SIMD bitwidth must also be checked, and can be retrieved using the
‘rte\_vect\_get\_max\_simd\_bitwidth()’ function.
The value should be compared against the enum values for accepted max SIMD bitwidths:

```
enum rte_vect_max_simd {
    RTE_VECT_SIMD_DISABLED = 64,
    RTE_VECT_SIMD_128 = 128,
    RTE_VECT_SIMD_256 = 256,
    RTE_VECT_SIMD_512 = 512,
    RTE_VECT_SIMD_MAX = INT16_MAX + 1,
};

 if (rte_vect_get_max_simd_bitwidth() >= RTE_VECT_SIMD_512)
     /* Take AVX-512 vector path */
 else if (rte_vect_get_max_simd_bitwidth() >= RTE_VECT_SIMD_256)
     /* Take AVX2 vector path */

```

## 1.2. Memory Segments and Memory Zones (memzone)

The mapping of physical memory is provided by this feature in the EAL.
As physical memory can have gaps, the memory is described in a table of descriptors,
and each descriptor (called rte\_memseg ) describes a physical page.

On top of this, the memzone allocator’s role is to reserve contiguous portions of physical memory.
These zones are identified by a unique name when the memory is reserved.

The rte\_memzone descriptors are also located in the configuration structure.
This structure is accessed using rte\_eal\_get\_configuration().
The lookup (by name) of a memory zone returns a descriptor containing the physical address of the memory zone.

Memory zones can be reserved with specific start address alignment by supplying the align parameter
(by default, they are aligned to cache line size).
The alignment value should be a power of two and not less than the cache line size (64 bytes).
Memory zones can also be reserved from either 2 MB or 1 GB hugepages, provided that both are available on the system.

Both memsegs and memzones are stored using `rte_fbarray` structures. Please
refer to _DPDK API Reference_ for more information.

## 1.3. Multiple pthread

DPDK usually pins one pthread per core to avoid the overhead of task switching.
This allows for significant performance gains, but lacks flexibility and is not always efficient.

Power management helps to improve the CPU efficiency by limiting the CPU runtime frequency.
However, alternately it is possible to utilize the idle cycles available to take advantage of
the full capability of the CPU.

By taking advantage of cgroup, the CPU utilization quota can be simply assigned.
This gives another way to improve the CPU efficiency, however, there is a prerequisite;
DPDK must handle the context switching between multiple pthreads per core.

For further flexibility, it is useful to set pthread affinity not only to a CPU but to a CPU set.

### 1.3.1. EAL pthread and lcore Affinity

The term “lcore” refers to an EAL thread, which is really a Linux/FreeBSD pthread.
“EAL pthreads” are created and managed by EAL and execute the tasks issued by _remote\_launch_.
In each EAL pthread, there is a TLS (Thread Local Storage) called _\_lcore\_id_ for unique identification.
As EAL pthreads usually bind 1:1 to the physical CPU, the _\_lcore\_id_ is typically equal to the CPU ID.

When using multiple pthreads, however, the binding is no longer always 1:1 between an EAL pthread and a specified physical CPU.
The EAL pthread may have affinity to a CPU set, and as such the _\_lcore\_id_ will not be the same as the CPU ID.
For this reason, there is an EAL long option ‘–lcores’ defined to assign the CPU affinity of lcores.
For a specified lcore ID or ID group, the option allows setting the CPU set for that EAL pthread.

The format pattern:

–lcores=’<lcore\_set>\[@cpu\_set\]\[,<lcore\_set>\[@cpu\_set\],…\]’

‘lcore\_set’ and ‘cpu\_set’ can be a single number, range or a group.

A number is a “digit(\[0-9\]+)”; a range is “<number>-<number>”; a group is “(<number\|range>\[,<number\|range>,…\])”.

If a ‘@cpu\_set’ value is not supplied, the value of ‘cpu\_set’ will default to the value of ‘lcore\_set’.

> ```
> For example, "--lcores='1,2@(5-7),(3-5)@(0,2),(0,6),7-8'" which means start 9 EAL thread;
>     lcore 0 runs on cpuset 0x41 (cpu 0,6);
>     lcore 1 runs on cpuset 0x2 (cpu 1);
>     lcore 2 runs on cpuset 0xe0 (cpu 5,6,7);
>     lcore 3,4,5 runs on cpuset 0x5 (cpu 0,2);
>     lcore 6 runs on cpuset 0x41 (cpu 0,6);
>     lcore 7 runs on cpuset 0x80 (cpu 7);
>     lcore 8 runs on cpuset 0x100 (cpu 8).
>
> ```

Using this option, for each given lcore ID, the associated CPUs can be assigned.
It’s also compatible with the pattern of corelist(‘-l’) option.

### 1.3.2. non-EAL pthread support

It is possible to use the DPDK execution context with any user pthread (aka. non-EAL pthreads).
There are two kinds of non-EAL pthreads:

- a registered non-EAL pthread with a valid _\_lcore\_id_ that was successfully assigned by calling `rte_thread_register()`,

- a non registered non-EAL pthread with a LCORE\_ID\_ANY,


For non registered non-EAL pthread (with a LCORE\_ID\_ANY _\_lcore\_id_), some libraries will use an alternative unique ID (e.g. TID), some will not be impacted at all, and some will work but with limitations (e.g. timer and mempool libraries).

All these impacts are mentioned in [Known Issues](https://doc.dpdk.org/guides-25.03/prog_guide/env_abstraction_layer.html#known-issue-label) section.

### 1.3.3. Public Thread API

There are two public APIs `rte_thread_set_affinity()` and `rte_thread_get_affinity()` introduced for threads.
When they’re used in any pthread context, the Thread Local Storage(TLS) will be set/get.

Those TLS include _\_cpuset_ and _\_socket\_id_:

- _\_cpuset_ stores the CPUs bitmap to which the pthread is affinitized.

- _\_socket\_id_ stores the NUMA node of the CPU set. If the CPUs in CPU set belong to different NUMA node, the _\_socket\_id_ will be set to SOCKET\_ID\_ANY.


### 1.3.4. Control Thread API

It is possible to create Control Threads using the public API
`rte_thread_create_control()`.
Those threads can be used for management/infrastructure tasks and are used
internally by DPDK for multi process support and interrupt handling.

Those threads will be scheduled on CPUs part of the original process CPU
affinity from which the dataplane and service lcores are excluded.

For example, on a 8 CPUs system, starting a dpdk application with -l 2,3
(dataplane cores), then depending on the affinity configuration which can be
controlled with tools like taskset (Linux) or cpuset (FreeBSD),

- with no affinity configuration, the Control Threads will end up on
0-1,4-7 CPUs.

- with affinity restricted to 2-4, the Control Threads will end up on
CPU 4.

- with affinity restricted to 2-3, the Control Threads will end up on
CPU 2 (main lcore, which is the default when no CPU is available).


### 1.3.5. Known Issues

- rte\_mempool

The rte\_mempool uses a per-lcore cache inside the mempool.
For unregistered non-EAL pthreads, `rte_lcore_id()` will not return a valid number.
So for now, when rte\_mempool is used with unregistered non-EAL pthreads, the put/get operations will bypass the default mempool cache and there is a performance penalty because of this bypass.
Only user-owned external caches can be used in an unregistered non-EAL context in conjunction with `rte_mempool_generic_put()` and `rte_mempool_generic_get()` that accept an explicit cache parameter.

- rte\_ring

rte\_ring supports multi-producer enqueue and multi-consumer dequeue.
However, it is non-preemptive, this has a knock on effect of making rte\_mempool non-preemptible.



Note



The “non-preemptive” constraint means:



- a pthread doing multi-producers enqueues on a given ring must not
be preempted by another pthread doing a multi-producer enqueue on
the same ring.

- a pthread doing multi-consumers dequeues on a given ring must not
be preempted by another pthread doing a multi-consumer dequeue on
the same ring.


Bypassing this constraint may cause the 2nd pthread to spin until the 1st one is scheduled again.
Moreover, if the 1st pthread is preempted by a context that has an higher priority, it may even cause a dead lock.

This means, use cases involving preemptible pthreads should consider using rte\_ring carefully.

1. It CAN be used for preemptible single-producer and single-consumer use case.

2. It CAN be used for non-preemptible multi-producer and preemptible single-consumer use case.

3. It CAN be used for preemptible single-producer and non-preemptible multi-consumer use case.

4. It MAY be used by preemptible multi-producer and/or preemptible multi-consumer pthreads whose scheduling policy are all SCHED\_OTHER(cfs), SCHED\_IDLE or SCHED\_BATCH. User SHOULD be aware of the performance penalty before using it.

5. It MUST not be used by multi-producer/consumer pthreads, whose scheduling policies are SCHED\_FIFO or SCHED\_RR.


Alternatively, applications can use the lock-free stack mempool handler. When
considering this handler, note that:

  - It is currently limited to the aarch64 and x86\_64 platforms, because it uses
    an instruction (16-byte compare-and-swap) that is not yet available on other
    platforms.

  - It has worse average-case performance than the non-preemptive rte\_ring, but
    software caching (e.g. the mempool cache) can mitigate this by reducing the
    number of stack accesses.
- rte\_timer

Running `rte_timer_manage()` on an unregistered non-EAL pthread is not allowed. However, resetting/stopping the timer from a non-EAL pthread is allowed.

- rte\_log

In unregistered non-EAL pthreads, there is no per thread loglevel and logtype, global loglevels are used.

- misc

The debug statistics of rte\_ring, rte\_mempool and rte\_timer are not supported in an unregistered non-EAL pthread.


### 1.3.6. Signal Safety

> The Posix API defines an async-signal-safe function as one that can be safely
> called from with a signal handler. Many DPDK functions are non-reentrant and
> therefore are unsafe to call from a signal handler.
>
> The kinds of issues that make DPDK functions unsafe can be understood when
> one considers that much of the code in DPDK uses locks and other shared
> resources. For example, calling `rte_mempool_lookup()` from a signal
> would deadlock if the signal happened during previous call `rte_mempool`
> routines.
>
> Other functions are not signal safe because they use one or more
> library routines that are not themselves signal safe.
> For example, calling `rte_panic()` is not safe in a signal handler
> because it uses `rte_log()` and `rte_log()` may call `vfprintf()` or
> `syslog()` library functions which are not in the list of
> signal safe functions
> [Signal-Safety manual page](https://man7.org/linux/man-pages/man7/signal-safety.7.html).
>
> The set of functions that are expected to be async-signal-safe in DPDK
> is shown in the following table. The functions not otherwise noted
> are not async-signal-safe.

| Function |
| --- |
| rte\_dump\_stack |
| rte\_eal\_get\_lcore\_state |
| rte\_eal\_get\_runtime\_dir |
| rte\_eal\_has\_hugepages |
| rte\_eal\_has\_pci |
| rte\_eal\_lcore\_role |
| rte\_eal\_process\_type |
| rte\_eal\_using\_phys\_addrs |
| rte\_get\_hpet\_cycles |
| rte\_get\_hpet\_hz |
| rte\_get\_main\_lcore |
| rte\_get\_next\_lcore |
| rte\_get\_tsc\_hz |
| rte\_hypervisor\_get |
| rte\_hypervisor\_get\_name |
| rte\_lcore\_count |
| rte\_lcore\_cpuset |
| rte\_lcore\_has\_role |
| rte\_lcore\_index |
| rte\_lcore\_is\_enabled |
| rte\_lcore\_to\_cpu\_id |
| rte\_lcore\_to\_socket\_id |
| rte\_log\_get\_global\_level |
| rte\_log\_get\_level |
| rte\_memory\_get\_nchannel |
| rte\_memory\_get\_nrank |
| rte\_reciprocal\_value |
| rte\_reciprocal\_value\_u64 |
| rte\_socket\_count |
| rte\_socket\_id |
| rte\_socket\_id\_by\_idx |
| rte\_strerror |
| rte\_strscpy |
| rte\_strsplit |
| rte\_sys\_gettid |
| rte\_uuid\_compare |
| rte\_uuid\_is\_null |
| rte\_uuid\_parse |
| rte\_uuid\_unparse |

Table 1.1 **Signal Safe Functions**

### 1.3.7. cgroup control

The following is a simple example of cgroup control usage, there are two pthreads(t0 and t1) doing packet I/O on the same core ($CPU).
We expect only 50% of CPU spend on packet IO.

> ```
> mkdir /sys/fs/cgroup/cpu/pkt_io
> mkdir /sys/fs/cgroup/cpuset/pkt_io
>
> echo $cpu > /sys/fs/cgroup/cpuset/cpuset.cpus
>
> echo $t0 > /sys/fs/cgroup/cpu/pkt_io/tasks
> echo $t0 > /sys/fs/cgroup/cpuset/pkt_io/tasks
>
> echo $t1 > /sys/fs/cgroup/cpu/pkt_io/tasks
> echo $t1 > /sys/fs/cgroup/cpuset/pkt_io/tasks
>
> cd /sys/fs/cgroup/cpu/pkt_io
> echo 100000 > pkt_io/cpu.cfs_period_us
> echo  50000 > pkt_io/cpu.cfs_quota_us
>
> ```

## 1.4. Malloc

The EAL provides a malloc API to allocate any-sized memory.

The objective of this API is to provide malloc-like functions to allow
allocation from hugepage memory and to facilitate application porting.
The _DPDK API Reference_ manual describes the available functions.

Typically, these kinds of allocations should not be done in data plane
processing because they are slower than pool-based allocation and make
use of locks within the allocation and free paths.
However, they can be used in configuration code.

Refer to the rte\_malloc() function description in the _DPDK API Reference_
manual for more information.

### 1.4.1. Alignment and NUMA Constraints

The rte\_malloc() takes an align argument that can be used to request a memory
area that is aligned on a multiple of this value (which must be a power of two).

On systems with NUMA support, a call to the rte\_malloc() function will return
memory that has been allocated on the NUMA socket of the core which made the call.
A set of APIs is also provided, to allow memory to be explicitly allocated on a
NUMA socket directly, or by allocated on the NUMA socket where another core is
located, in the case where the memory is to be used by a logical core other than
on the one doing the memory allocation.

### 1.4.2. Use Cases

This API is meant to be used by an application that requires malloc-like
functions at initialization time.

For allocating/freeing data at runtime, in the fast-path of an application,
the memory pool library should be used instead.

### 1.4.3. Internal Implementation

#### 1.4.3.1. Data Structures

There are two data structure types used internally in the malloc library:

- struct malloc\_heap - used to track free space on a per-socket basis

- struct malloc\_elem - the basic element of allocation and free-space
tracking inside the library.


##### 1.4.3.1.1. Structure: malloc\_heap

The malloc\_heap structure is used to manage free space on a per-socket basis.
Internally, there is one heap structure per NUMA node, which allows us to
allocate memory to a thread based on the NUMA node on which this thread runs.
While this does not guarantee that the memory will be used on that NUMA node,
it is no worse than a scheme where the memory is always allocated on a fixed
or random node.

The key fields of the heap structure and their function are described below
(see also diagram above):

- lock - the lock field is needed to synchronize access to the heap.
Given that the free space in the heap is tracked using a linked list,
we need a lock to prevent two threads manipulating the list at the same time.

- free\_head - this points to the first element in the list of free nodes for
this malloc heap.

- first - this points to the first element in the heap.

- last - this points to the last element in the heap.


![../_images/malloc_heap.svg](https://doc.dpdk.org/guides-25.03/_images/malloc_heap.svg)

Fig. 1.2 Example of a malloc heap and malloc elements within the malloc library

##### 1.4.3.1.2. Structure: malloc\_elem

The malloc\_elem structure is used as a generic header structure for various
blocks of memory.
It is used in two different ways - all shown in the diagram above:

1. As a header on a block of free or allocated memory - normal case

2. As a padding header inside a block of memory


The most important fields in the structure and how they are used are described below.

Malloc heap is a doubly-linked list, where each element keeps track of its
previous and next elements. Due to the fact that hugepage memory can come and
go, neighboring malloc elements may not necessarily be adjacent in memory.
Also, since a malloc element may span multiple pages, its contents may not
necessarily be IOVA-contiguous either - each malloc element is only guaranteed
to be virtually contiguous.

Note

If the usage of a particular field in one of the above three usages is not
described, the field can be assumed to have an undefined value in that
situation, for example, for padding headers only the “state” and “pad”
fields have valid values.

- heap - this pointer is a reference back to the heap structure from which
this block was allocated.
It is used for normal memory blocks when they are being freed, to add the
newly-freed block to the heap’s free-list.

- prev - this pointer points to previous header element/block in memory. When
freeing a block, this pointer is used to reference the previous block to
check if that block is also free. If so, and the two blocks are immediately
adjacent to each other, then the two free blocks are merged to form a single
larger block.

- next - this pointer points to next header element/block in memory. When
freeing a block, this pointer is used to reference the next block to check
if that block is also free. If so, and the two blocks are immediately
adjacent to each other, then the two free blocks are merged to form a single
larger block.

- free\_list - this is a structure pointing to previous and next elements in
this heap’s free list.
It is only used in normal memory blocks; on `malloc()` to find a suitable
free block to allocate and on `free()` to add the newly freed element to
the free-list.

- state - This field can have one of three values: `FREE`, `BUSY` or
`PAD`.
The former two are to indicate the allocation state of a normal memory block
and the latter is to indicate that the element structure is a dummy structure
at the end of the start-of-block padding, i.e. where the start of the data
within a block is not at the start of the block itself, due to alignment
constraints.
In that case, the pad header is used to locate the actual malloc element
header for the block.

- dirty - this flag is only meaningful when `state` is `FREE`.
It indicates that the content of the element is not fully zero-filled.
Memory from such blocks must be cleared when requested via `rte_zmalloc*()`.
Dirty elements only appear with `--huge-unlink=never`.

- pad - this holds the length of the padding present at the start of the block.
In the case of a normal block header, it is added to the address of the end
of the header to give the address of the start of the data area, i.e. the
value passed back to the application on a malloc.
Within a dummy header inside the padding, this same value is stored, and is
subtracted from the address of the dummy header to yield the address of the
actual block header.

- size - the size of the data block, including the header itself.


#### 1.4.3.2. Memory Allocation

On EAL initialization, all preallocated memory segments are setup as part of the
malloc heap. This setup involves placing an [element header](https://doc.dpdk.org/guides-25.03/prog_guide/env_abstraction_layer.html#malloc-elem)
with `FREE` at the start of each virtually contiguous segment of memory.
The `FREE` element is then added to the `free_list` for the malloc heap.

This setup also happens whenever memory is allocated at runtime (if supported),
in which case newly allocated pages are also added to the heap, merging with any
adjacent free segments if there are any.

When an application makes a call to a malloc-like function, the malloc function
will first index the `lcore_config` structure for the calling thread, and
determine the NUMA node of that thread.
The NUMA node is used to index the array of `malloc_heap` structures which is
passed as a parameter to the `heap_alloc()` function, along with the
requested size, type, alignment and boundary parameters.

The `heap_alloc()` function will scan the free\_list of the heap, and attempt
to find a free block suitable for storing data of the requested size, with the
requested alignment and boundary constraints.

When a suitable free element has been identified, the pointer to be returned
to the user is calculated.
The cache-line of memory immediately preceding this pointer is filled with a
struct malloc\_elem header.
Because of alignment and boundary constraints, there could be free space at
the start and/or end of the element, resulting in the following behavior:

1. Check for trailing space.
If the trailing space is big enough, i.e. > 128 bytes, then the free element
is split.
If it is not, then we just ignore it (wasted space).

2. Check for space at the start of the element.
If the space at the start is small, i.e. <=128 bytes, then a pad header is
used, and the remaining space is wasted.
If, however, the remaining space is greater, then the free element is split.


The advantage of allocating the memory from the end of the existing element is
that no adjustment of the free list needs to take place - the existing element
on the free list just has its size value adjusted, and the next/previous elements
have their “prev”/”next” pointers redirected to the newly created element.

In case when there is not enough memory in the heap to satisfy allocation
request, EAL will attempt to allocate more memory from the system (if supported)
and, following successful allocation, will retry reserving the memory again. In
a multiprocessing scenario, all primary and secondary processes will synchronize
their memory maps to ensure that any valid pointer to DPDK memory is guaranteed
to be valid at all times in all currently running processes.

Failure to synchronize memory maps in one of the processes will cause allocation
to fail, even though some of the processes may have allocated the memory
successfully. The memory is not added to the malloc heap unless primary process
has ensured that all other processes have mapped this memory successfully.

Any successful allocation event will trigger a callback, for which user
applications and other DPDK subsystems can register. Additionally, validation
callbacks will be triggered before allocation if the newly allocated memory will
exceed threshold set by the user, giving a chance to allow or deny allocation.

Note

Any allocation of new pages has to go through primary process. If the
primary process is not active, no memory will be allocated even if it was
theoretically possible to do so. This is because primary’s process map acts
as an authority on what should or should not be mapped, while each secondary
process has its own, local memory map. Secondary processes do not update the
shared memory map, they only copy its contents to their local memory map.

#### 1.4.3.3. Freeing Memory

To free an area of memory, the pointer to the start of the data area is passed
to the free function.
The size of the `malloc_elem` structure is subtracted from this pointer to get
the element header for the block.
If this header is of type `PAD` then the pad length is further subtracted from
the pointer to get the proper element header for the entire block.

From this element header, we get pointers to the heap from which the block was
allocated and to where it must be freed, as well as the pointer to the previous
and next elements. These next and previous elements are then checked to see if
they are also `FREE` and are immediately adjacent to the current one, and if
so, they are merged with the current element. This means that we can never have
two `FREE` memory blocks adjacent to one another, as they are always merged
into a single block.

If deallocating pages at runtime is supported, and the free element encloses
one or more pages, those pages can be deallocated and be removed from the heap.
If DPDK was started with command-line parameters for preallocating memory
( `-m` or `--socket-mem`), then those pages that were allocated at startup
will not be deallocated.

Any successful deallocation event will trigger a callback, for which user
applications and other DPDK subsystems can register.


================================================================================
FILE: doc.dpdk.org_guides-25.03_prog_guide_index.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/prog_guide/index.html"
title: "Programmer’s Guide — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- Programmer’s Guide
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/prog_guide/index.rst.txt)

* * *

# Programmer’s Guide

## Foundation Principles

- [1\. Introduction](https://doc.dpdk.org/guides-25.03/prog_guide/intro.html)
- [2\. Overview](https://doc.dpdk.org/guides-25.03/prog_guide/overview.html)
- [3\. Source Organization](https://doc.dpdk.org/guides-25.03/prog_guide/source_org.html)
- [4\. Glossary](https://doc.dpdk.org/guides-25.03/prog_guide/glossary.html)

## Memory Management

- [1\. Lcore Variables](https://doc.dpdk.org/guides-25.03/prog_guide/lcore_var.html)
- [2\. Memory Pool Library](https://doc.dpdk.org/guides-25.03/prog_guide/mempool_lib.html)
- [3\. Packet (Mbuf) Library](https://doc.dpdk.org/guides-25.03/prog_guide/mbuf_lib.html)
- [4\. Multi-process Support](https://doc.dpdk.org/guides-25.03/prog_guide/multi_proc_support.html)

## CPU Management

- [1\. Environment Abstraction Layer (EAL) Library](https://doc.dpdk.org/guides-25.03/prog_guide/env_abstraction_layer.html)
- [2\. Power Management](https://doc.dpdk.org/guides-25.03/prog_guide/power_man.html)
- [3\. Thread Safety](https://doc.dpdk.org/guides-25.03/prog_guide/thread_safety.html)
- [4\. Service Cores](https://doc.dpdk.org/guides-25.03/prog_guide/service_cores.html)

## CPU Packet Processing

- [1\. Toeplitz Hash Library](https://doc.dpdk.org/guides-25.03/prog_guide/toeplitz_hash_lib.html)
- [2\. Hash Library](https://doc.dpdk.org/guides-25.03/prog_guide/hash_lib.html)
- [3\. Membership Library](https://doc.dpdk.org/guides-25.03/prog_guide/member_lib.html)
- [4\. IP Fragmentation and Reassembly Library](https://doc.dpdk.org/guides-25.03/prog_guide/ip_fragment_reassembly_lib.html)
- [5\. Generic Receive Offload (GRO) Library](https://doc.dpdk.org/guides-25.03/prog_guide/generic_receive_offload_lib.html)
- [6\. Generic Segmentation Offload (GSO) Library](https://doc.dpdk.org/guides-25.03/prog_guide/generic_segmentation_offload_lib.html)
- [7\. Packet Classification and Access Control (ACL) Library](https://doc.dpdk.org/guides-25.03/prog_guide/packet_classif_access_ctrl.html)
- [8\. Packet Distributor Library](https://doc.dpdk.org/guides-25.03/prog_guide/packet_distrib_lib.html)
- [9\. Elastic Flow Distributor (EFD) Library](https://doc.dpdk.org/guides-25.03/prog_guide/efd_lib.html)
- [10\. Reorder Library](https://doc.dpdk.org/guides-25.03/prog_guide/reorder_lib.html)
- [11\. Longest Prefix Match (LPM) Library](https://doc.dpdk.org/guides-25.03/prog_guide/lpm_lib.html)
- [12\. Longest Prefix Match 6 (LPM6) Library](https://doc.dpdk.org/guides-25.03/prog_guide/lpm6_lib.html)
- [13\. Routing Information Base (RIB) Library](https://doc.dpdk.org/guides-25.03/prog_guide/rib_lib.html)
- [14\. Forwarding Information Base (FIB) Library](https://doc.dpdk.org/guides-25.03/prog_guide/fib_lib.html)

## Device Libraries

- [1\. Ethernet Device Library](https://doc.dpdk.org/guides-25.03/prog_guide/ethdev/index.html)
- [2\. Link Bonding Poll Mode Driver Library](https://doc.dpdk.org/guides-25.03/prog_guide/link_bonding_poll_mode_drv_lib.html)
- [3\. Vhost Library](https://doc.dpdk.org/guides-25.03/prog_guide/vhost_lib.html)
- [4\. Cryptography Device Library](https://doc.dpdk.org/guides-25.03/prog_guide/cryptodev_lib.html)
- [5\. Security Library](https://doc.dpdk.org/guides-25.03/prog_guide/rte_security.html)
- [6\. Compression Device Library](https://doc.dpdk.org/guides-25.03/prog_guide/compressdev.html)
- [7\. Regular Expression (RegEx) Device Library](https://doc.dpdk.org/guides-25.03/prog_guide/regexdev.html)
- [8\. Wireless Baseband Device Library](https://doc.dpdk.org/guides-25.03/prog_guide/bbdev.html)
- [9\. Machine Learning (ML) Device Library](https://doc.dpdk.org/guides-25.03/prog_guide/mldev.html)
- [10\. Direct Memory Access (DMA) Device Library](https://doc.dpdk.org/guides-25.03/prog_guide/dmadev.html)
- [11\. General-Purpose Graphics Processing Unit (GPU) Library](https://doc.dpdk.org/guides-25.03/prog_guide/gpudev.html)
- [12\. Raw Device Library](https://doc.dpdk.org/guides-25.03/prog_guide/rawdev.html)
- [13\. Event Device Library](https://doc.dpdk.org/guides-25.03/prog_guide/eventdev/index.html)

## Protocol Processing Libraries

- [1\. PDCP Protocol Processing Library](https://doc.dpdk.org/guides-25.03/prog_guide/pdcp_lib.html)
- [2\. IPsec Packet Processing Library](https://doc.dpdk.org/guides-25.03/prog_guide/ipsec_lib.html)

## High-Level Libraries

- [1\. Packet Framework Library](https://doc.dpdk.org/guides-25.03/prog_guide/packet_framework.html)
- [2\. Graph Library and Inbuilt Nodes](https://doc.dpdk.org/guides-25.03/prog_guide/graph_lib.html)

## Utility Libraries

- [1\. Argparse Library](https://doc.dpdk.org/guides-25.03/prog_guide/argparse_lib.html)
- [2\. Command-line Library](https://doc.dpdk.org/guides-25.03/prog_guide/cmdline.html)
- [3\. Pointer Compression Library](https://doc.dpdk.org/guides-25.03/prog_guide/ptr_compress_lib.html)
- [4\. Timer Library](https://doc.dpdk.org/guides-25.03/prog_guide/timer_lib.html)
- [5\. Read-Copy-Update (RCU) Library](https://doc.dpdk.org/guides-25.03/prog_guide/rcu_lib.html)
- [6\. Ring Library](https://doc.dpdk.org/guides-25.03/prog_guide/ring_lib.html)
- [7\. Stack Library](https://doc.dpdk.org/guides-25.03/prog_guide/stack_lib.html)
- [8\. Log Library](https://doc.dpdk.org/guides-25.03/prog_guide/log_lib.html)
- [9\. Metrics Library](https://doc.dpdk.org/guides-25.03/prog_guide/metrics_lib.html)
- [10\. Telemetry Library](https://doc.dpdk.org/guides-25.03/prog_guide/telemetry_lib.html)
- [11\. Packet Capture Library](https://doc.dpdk.org/guides-25.03/prog_guide/pdump_lib.html)
- [12\. Packet Capture Next Generation Library](https://doc.dpdk.org/guides-25.03/prog_guide/pcapng_lib.html)
- [13\. Berkeley Packet Filter (BPF) Library](https://doc.dpdk.org/guides-25.03/prog_guide/bpf_lib.html)
- [14\. Trace Library](https://doc.dpdk.org/guides-25.03/prog_guide/trace_lib.html)

## Howto Guides

- [1\. Installing DPDK Using the meson build system](https://doc.dpdk.org/guides-25.03/prog_guide/build-sdk-meson.html)
- [2\. Running DPDK Unit Tests with Meson](https://doc.dpdk.org/guides-25.03/prog_guide/meson_ut.html)
- [3\. Building Your Own Application](https://doc.dpdk.org/guides-25.03/prog_guide/build_app.html)

## Tips & Tricks

- [1\. Performance Optimization Guidelines](https://doc.dpdk.org/guides-25.03/prog_guide/perf_opt_guidelines.html)
- [2\. Writing Efficient Code](https://doc.dpdk.org/guides-25.03/prog_guide/writing_efficient_code.html)
- [3\. Link Time Optimization](https://doc.dpdk.org/guides-25.03/prog_guide/lto.html)
- [4\. Profile Your Application](https://doc.dpdk.org/guides-25.03/prog_guide/profile_app.html)
- [5\. Running AddressSanitizer](https://doc.dpdk.org/guides-25.03/prog_guide/asan.html)


================================================================================
FILE: doc.dpdk.org_guides-25.03_prog_guide_lcore_var.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/prog_guide/lcore_var.html"
title: "1. Lcore Variables — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Programmer’s Guide](https://doc.dpdk.org/guides-25.03/prog_guide/index.html)
- 1\. Lcore Variables
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/prog_guide/lcore_var.rst.txt)

* * *

# 1\. Lcore Variables

The `rte_lcore_var.h` API provides a mechanism to allocate and
access per-lcore id variables in a space- and cycle-efficient manner.

## 1.1. Lcore Variables API

A per-lcore id variable (or lcore variable for short)
holds a unique value for each EAL thread and registered non-EAL thread.
Thus, there is one distinct value for each past, current and future
lcore id-equipped thread, with a total of `RTE_MAX_LCORE` instances.

The value of the lcore variable for one lcore id is independent of the
values associated with other lcore ids within the same variable.

For detailed information on the lcore variables API,
please refer to the `rte_lcore_var.h` API documentation.

### 1.1.1. Lcore Variable Handle

To allocate and access an lcore variable’s values, a _handle_ is used.
The handle is represented by an opaque pointer,
only to be dereferenced using the appropriate `<rte_lcore_var.h>` macros.

The handle is a pointer to the value’s type
(e.g., for an `uint32_t` lcore variable, the handle is a `uint32_t *`).

The reason the handle is typed (i.e., it’s not a void pointer or an integer)
is to enable type checking when accessing values of the lcore variable.

A handle may be passed between modules and threads
just like any other pointer.

A valid (i.e., allocated) handle never has the value NULL.
Thus, a handle set to NULL may be used
to signify that allocation has not yet been done.

### 1.1.2. Lcore Variable Allocation

An lcore variable is created in two steps:

1. Define an lcore variable handle by using `RTE_LCORE_VAR_HANDLE`.

2. Allocate lcore variable storage and initialize the handle
by using `RTE_LCORE_VAR_ALLOC` or `RTE_LCORE_VAR_INIT`.
Allocation generally occurs at the time of module initialization,
but may be done at any time.


The lifetime of an lcore variable is not tied to the thread that created it.

Each lcore variable has `RTE_MAX_LCORE` values,
one for each possible lcore id.
All of an lcore variable’s values may be accessed
from the moment the lcore variable is created,
throughout the lifetime of the EAL (i.e., until `rte_eal_cleanup()`).

Lcore variables do not need to be freed and cannot be freed.

### 1.1.3. Access

The value of any lcore variable for any lcore id
may be accessed from any thread (including unregistered threads),
but it should only be _frequently_ read from or written to by the _owner_.
A thread is considered the owner of a particular lcore variable value instance
if it has the lcore id associated with that instance.

Non-owner accesses results in _false sharing_.
As long as non-owner accesses are rare,
they will have only a very slight effect on performance.
This property of lcore variables memory organization is intentional.
See the implementation section for more information.

Values of the same lcore variable,
associated with different lcore ids may be frequently read or written
by their respective owners without risking false sharing.

An appropriate synchronization mechanism,
such as atomic load and stores,
should be employed to prevent data races between the owning thread
and any other thread accessing the same value instance.

The value of the lcore variable for a particular lcore id
is accessed via `RTE_LCORE_VAR_LCORE`.

A common pattern is for an EAL thread or a registered non-EAL thread
to access its own lcore variable value.
For this purpose, a shorthand exists as `RTE_LCORE_VAR`.

`RTE_LCORE_VAR_FOREACH` may be used to iterate
over all values of a particular lcore variable.

The handle, defined by `RTE_LCORE_VAR_HANDLE`,
is a pointer of the same type as the value,
but it must be treated as an opaque identifier
and cannot be directly dereferenced.

Lcore variable handles and value pointers may be freely passed
between different threads.

### 1.1.4. Storage

An lcore variable’s values may be of a primitive type like `int`,
but is typically a `struct`.

The lcore variable handle introduces a per-variable
(not per-value/per-lcore id) overhead of `sizeof(void *)` bytes,
so there are some memory footprint gains to be made by organizing
all per-lcore id data for a particular module as one lcore variable
(e.g., as a struct).

An application may define an lcore variable handle
without ever allocating the lcore variable.

The size of an lcore variable’s value cannot exceed
the DPDK build-time constant `RTE_MAX_LCORE_VAR`.
An lcore variable’s size is the size of one of its value instance,
not the aggregate of all its `RTE_MAX_LCORE` instances.

Lcore variables should generally _not_ be `__rte_cache_aligned`
and need _not_ include a `RTE_CACHE_GUARD` field,
since these constructs are designed to avoid false sharing.
With lcore variables, false sharing is largely avoided by other means.
In the case of an lcore variable instance,
the thread most recently accessing nearby data structures
should almost always be the lcore variable’s owner.
Adding padding (e.g., with `RTE_CACHE_GUARD`)
will increase the effective memory working set size,
potentially reducing performance.

Lcore variable values are initialized to zero by default.

Lcore variables are not stored in huge page memory.

### 1.1.5. Example

Below is an example of the use of an lcore variable:

```
struct foo_lcore_state {
        int a;
        long b;
};

static RTE_LCORE_VAR_HANDLE(struct foo_lcore_state, lcore_states);

long foo_get_a_plus_b(void)
{
        const struct foo_lcore_state *state = RTE_LCORE_VAR(lcore_states);

        return state->a + state->b;
}

RTE_INIT(rte_foo_init)
{
        RTE_LCORE_VAR_ALLOC(lcore_states);

        unsigned int lcore_id;
        struct foo_lcore_state *state;
        RTE_LCORE_VAR_FOREACH(lcore_id, state, lcore_states) {
                /* initialize state */
        }

        /* other initialization */
}

```

## 1.2. Implementation

This section gives an overview of the implementation of lcore variables,
and some background to its design.

### 1.2.1. Lcore Variable Buffers

Lcore variable values are kept in a set of `lcore_var_buffer` structs.

```
struct lcore_var_buffer {
	char data[RTE_MAX_LCORE_VAR * RTE_MAX_LCORE];
	struct lcore_var_buffer *prev;
};

```

An lcore var buffer stores at a minimum one, but usually many, lcore variables.

The value instances for all lcore ids are stored in the same buffer.
However, each lcore id has its own slice of the `data` array.
Such a slice is `RTE_MAX_LCORE_VAR` bytes in size.

In this way, the values associated with a particular lcore id
are grouped spatially close (in memory).
No padding is required to prevent false sharing.

```
static struct lcore_var_buffer *current_buffer;

/* initialized to trigger buffer allocation on first allocation */
static size_t offset = RTE_MAX_LCORE_VAR;

```

The implementation maintains a current `lcore_var_buffer` and an `offset`,
where the latter tracks how many bytes of this current buffer has been allocated.

The `offset` is progressively incremented
(by the size of the just-allocated lcore variable),
as lcore variables are being allocated.

If the allocation of a variable would result in an `offset` larger
than `RTE_MAX_LCORE_VAR` (i.e., the slice size), the buffer is full.
In that case, new buffer is allocated off the heap, and the `offset` is reset.

The lcore var buffers are arranged in a link list,
to allow freeing them at the point of `rte_eal_cleanup()`.

The lcore variable buffers are allocated off the regular C heap.
There are a number of reasons for not using `<rte_malloc.h>`
and huge pages for lcore variables:

- The libc heap is available at any time,
including early in the DPDK initialization.

- The amount of data kept in lcore variables is projected to be small,
and thus is unlikely to induce translate lookaside buffer (TLB) misses.

- The last (and potentially only) lcore buffer in the chain
will likely only partially be in use.
Huge pages of the sort used by DPDK are always resident in memory,
and their use would result in a significant amount of memory going to waste.
An example: ~256 kB worth of lcore variables are allocated
by DPDK libraries, PMDs and the application.
`RTE_MAX_LCORE_VAR` is set to 128 kB and `RTE_MAX_LCORE` to 128.
With 4 kB OS pages, only the first ~64 pages of each of the 128 per-lcore id slices
in the (only) `lcore_var_buffer` will actually be resident (paged in).
Here, demand paging saves ~98 MB of memory.


Note

Not residing in huge pages, lcore variables cannot be accessed from secondary processes.

Heap allocation failures are treated as fatal.
The reason for this unorthodox design is that a majority of the allocations
are deemed to happen at initialization.
An early heap allocation failure for a fixed amount of data is a situation
not unlike one where there is not enough memory available for static variables
(i.e., the BSS or data sections).

Provided these assumptions hold true, it’s deemed acceptable
to leave the application out of handling memory allocation failures.

The upside of this approach is that no error handling code is required
on the API user side.

### 1.2.2. Lcore Variable Handles

Upon lcore variable allocation, the lcore variables API returns
an opaque _handle_ in the form of a pointer.
The value of the pointer is `buffer->data + offset`.

Translating a handle base pointer to a pointer to a value
associated with a particular lcore id is straightforward:

```
static inline void *
rte_lcore_var_lcore(unsigned int lcore_id, void *handle)
{
	RTE_ASSERT(handle != NULL);
	return RTE_PTR_ADD(handle, lcore_id * RTE_MAX_LCORE_VAR);
}

```

`RTE_MAX_LCORE_VAR` is a public macro to allow the compiler
to optimize the `lcore_id * RTE_MAX_LCORE_VAR` expression,
and replace the multiplication with a less expensive arithmetic operation.

To maintain type safety, the `RTE_LCORE_VAR*()` macros should be used,
instead of directly invoking `rte_lcore_var_lcore()`.
The macros return a pointer of the same type as the handle
(i.e., a pointer to the value’s type).

### 1.2.3. Memory Layout

This section describes how lcore variables are organized in memory.

As an illustration, two example modules are used,
`rte_x` and `rte_y`, both maintaining per-lcore id state
as a part of their implementation.

Two different methods will be used to maintain such state -
lcore variables and, to serve as a reference, lcore id-indexed static arrays.

Certain parameters are scaled down to make graphical depictions more practical.

For the purpose of this exercise, a `RTE_MAX_LCORE` of 2 is assumed.
In a real-world configuration, the maximum number of
EAL threads and registered threads will be much greater (e.g., 128).

The lcore variables example assumes a `RTE_MAX_LCORE_VAR` of 64.
In a real-world configuration (as controlled by `rte_config.h`),
the value of this compile-time constant will be much greater (e.g., 1048576).

The per-lcore id state is also smaller than what most real-world modules would have.

#### 1.2.3.1. Lcore Variables Example

When lcore variables are used, the parts of `rte_x` and `rte_y`
that deal with the declaration and allocation of per-lcore id data
may look something like below.

```
/* -- Lcore variables -- */

/* rte_x.c */

struct x_lcore
{
    int a;
    char b;
};

static RTE_LCORE_VAR_HANDLE(struct x_lcore, x_lcores);
RTE_LCORE_VAR_INIT(x_lcores);

/../

/* rte_y.c */

struct y_lcore
{
    long c;
    long d;
};

static RTE_LCORE_VAR_HANDLE(struct y_lcore, y_lcores);
RTE_LCORE_VAR_INIT(y_lcores);

/../

```

The resulting memory layout will look something like the following:

![../_images/lcore_var_mem_layout.svg](https://doc.dpdk.org/guides-25.03/_images/lcore_var_mem_layout.svg)

The above figure assumes that `x_lcores` is allocated prior to `y_lcores`.
`RTE_LCORE_VAR_INIT()` relies constructors, run prior to `main()` in an undefined order.

The use of lcore variables ensures that per-lcore id data is kept in close proximity,
within a designated region of memory.
This proximity enhances data locality and can improve performance.

#### 1.2.3.2. Lcore Id Index Static Array Example

Below is an example of the struct declarations,
declarations and the resulting organization in memory
in case an lcore id indexed static array of cache-line aligned,
RTE\_CACHE\_GUARDed structs are used to maintain per-lcore id state.

This is a common pattern in DPDK, which lcore variables attempts to replace.

```
/* -- Cache-aligned static arrays -- */

/* rte_x.c */

struct __rte_cache_aligned x_lcore
{
    int a;
    char b;
    RTE_CACHE_GUARD;
};

static struct x_lcore x_lcores[RTE_MAX_LCORE];

/../

/* rte_y.c */

struct __rte_cache_aligned y_lcore
{
    long c;
    long d;
    RTE_CACHE_GUARD;
};

static struct y_lcore y_lcores[RTE_MAX_LCORE];

/../

```

In this approach, accessing the state for a particular lcore id is merely
a matter retrieving the lcore id and looking up the correct struct instance.

```
struct x_lcore *my_lcore_state = &x_lcores[rte_lcore_id()];

```

The address “0” at the top of the left-most column in the figure
represent the base address for the `x_lcores` array
(in the BSS segment in memory).

The figure only includes the memory layout for the `rte_x` example module.
`rte_y` would look very similar, with `y_lcores` being located
at some other address in the BSS section.

![../_images/static_array_mem_layout.svg](https://doc.dpdk.org/guides-25.03/_images/static_array_mem_layout.svg)

The static array approach results in the per-lcore id
being organized around modules, not lcore ids.
To avoid false sharing, an extensive use of padding is employed,
causing cache fragmentation.

Because the padding is interspersed with the data,
demand paging is unlikely to reduce the actual resident DRAM memory footprint.
This is because the padding is smaller
than a typical operating system memory page (usually 4 kB).

### 1.2.4. Performance

One of the goals of lcore variables is to improve performance.
This is achieved by packing often-used data in fewer cache lines,
and thus reducing fragmentation in CPU caches
and thus somewhat improving the effective cache size and cache hit rates.

The application-level gains depends much on how much data is kept in lcore variables,
and how often it is accessed,
and how much pressure the application asserts on the CPU caches
(i.e., how much other memory it accesses).

The `lcore_var_perf_autotest` is an attempt at exploring
the performance benefits (or drawbacks) of lcore variables
compared to its alternatives.
Being a micro benchmark, it needs to be taken with a grain of salt.

Generally, one shouldn’t expect more than some very modest gains in performance
after a switch from lcore id indexed arrays to lcore variables.

An additional benefit of the use of lcore variables is that it avoids
certain tricky issues related to CPU core hardware prefetching
(e.g., next-N-lines prefetching) that may cause false sharing
even when data used by two cores do not reside on the same cache line.
Hardware prefetch behavior is generally not publicly documented
and varies across CPU vendors, CPU generations and BIOS (or similar) configurations.
For applications aiming to be portable, this may cause issues.
Often, CPU hardware prefetch-induced issues are non-existent,
except some particular circumstances, where their adverse effects may be significant.

## 1.3. Alternatives

### 1.3.1. Lcore Id Indexed Static Arrays

Lcore variables are designed to replace a pattern exemplified below:

```
struct __rte_cache_aligned foo_lcore_state {
        int a;
        long b;
        RTE_CACHE_GUARD;
};

static struct foo_lcore_state lcore_states[RTE_MAX_LCORE];

```

This scheme is simple and effective, but has one drawback:
the data is organized so that objects related to all lcores for a particular module
are kept close in memory.
At a bare minimum, this requires sizing data structures
(e.g., using `__rte_cache_aligned`) to an even number of cache lines
and ensuring that allocation of such objects
are cache line aligned to avoid false sharing.
With CPU hardware prefetching and memory loads resulting from speculative execution
(functions which seemingly are getting more eager faster
than they are getting more intelligent),
one or more “guard” cache lines may be required
to separate one lcore’s data from another’s and prevent false sharing.

Lcore variables offer the advantage of working with,
rather than against, the CPU’s assumptions.
A next-line hardware prefetcher, for example, may function as intended
(i.e., to the benefit, not detriment, of system performance).

### 1.3.2. Thread Local Storage

An alternative to `rte_lcore_var.h` is the `rte_per_lcore.h` API,
which makes use of thread-local storage
(TLS, e.g., GCC `__thread` or C11 `_Thread_local`).

There are a number of differences between using TLS
and the use of lcore variables.

The lifecycle of a thread-local variable instance is tied to that of the thread.
The data cannot be accessed before the thread has been created,
nor after it has terminated.
As a result, thread-local variables must be initialized in a “lazy” manner
(e.g., at the point of thread creation).
Lcore variables may be accessed immediately after having been allocated
(which may occur before any thread beyond the main thread is running).

A thread-local variable is duplicated across all threads in the process,
including unregistered non-EAL threads (i.e., “regular” threads).
For DPDK applications heavily relying on multi-threading
(in conjunction to DPDK’s “one thread per core” pattern),
either by having many concurrent threads or creating/destroying threads at a high rate,
an excessive use of thread-local variables may cause inefficiencies
(e.g., increased thread creation overhead due to thread-local storage initialization
or increased memory footprint).
Lcore variables _only_ exist for threads with an lcore id.

Whether data in thread-local storage can be shared between threads
(i.e., whether a pointer to a thread-local variable can be passed to
and successfully dereferenced by a non-owning thread)
depends on the specifics of the TLS implementation.
With GCC `__thread` and GCC `_Thread_local`,
data sharing between threads is supported.
In the C11 standard, accessing another thread’s `_Thread_local` object
is implementation-defined.
Lcore variable instances may be accessed reliably by any thread.

Lcore variables also relies on TLS to retrieve the thread’s lcore id.
However, the rest of the per-thread data is not kept in TLS.

From a memory layout perspective, TLS is similar to lcore variables,
and thus per-thread data structure need not be padded.

In case the above-mentioned drawbacks of the use of TLS is of no significance
to a particular application, TLS is a good alternative to lcore variables.


================================================================================
FILE: doc.dpdk.org_guides-25.03_prog_guide_link_bonding_poll_mode_drv_lib.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/prog_guide/link_bonding_poll_mode_drv_lib.html"
title: "2. Link Bonding Poll Mode Driver Library — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Programmer’s Guide](https://doc.dpdk.org/guides-25.03/prog_guide/index.html)
- 2\. Link Bonding Poll Mode Driver Library
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/prog_guide/link_bonding_poll_mode_drv_lib.rst.txt)

* * *

# 2\. Link Bonding Poll Mode Driver Library

In addition to Poll Mode Drivers (PMDs) for physical and virtual hardware,
DPDK also includes a pure-software library that
allows physical PMDs to be bonded together to create a single logical PMD.

![../_images/bond-overview.svg](https://doc.dpdk.org/guides-25.03/_images/bond-overview.svg)

Fig. 2.5 Bonding PMDs

The Link Bonding PMD library(librte\_net\_bond) supports bonding of groups of
`rte_eth_dev` ports of the same speed and duplex to provide similar
capabilities to that found in Linux bonding driver to allow the aggregation
of multiple (member) NICs into a single logical interface between a server
and a switch. The new bonding PMD will then process these interfaces based on
the mode of operation specified to provide support for features such as
redundant links, fault tolerance and/or load balancing.

The librte\_net\_bond library exports a C API which provides an API for the
creation of bonding devices as well as the configuration and management of the
bonding device and its member devices.

Note

The Link Bonding PMD Library is enabled by default in the build
configuration, the library can be disabled using the meson option
“-Ddisable\_drivers=net/bonding”.

## 2.1. Link Bonding Modes Overview

Currently the Link Bonding PMD library supports following modes of operation:

- **Round-Robin (Mode 0):**


![../_images/bond-mode-0.svg](https://doc.dpdk.org/guides-25.03/_images/bond-mode-0.svg)

Fig. 2.6 Round-Robin (Mode 0)

> This mode provides load balancing and fault tolerance by transmission of
> packets in sequential order from the first available member device through
> the last. Packets are bulk dequeued from devices then serviced in a
> round-robin manner. This mode does not guarantee in order reception of
> packets and down stream should be able to handle out of order packets.

- **Active Backup (Mode 1):**


![../_images/bond-mode-1.svg](https://doc.dpdk.org/guides-25.03/_images/bond-mode-1.svg)

Fig. 2.7 Active Backup (Mode 1)

> In this mode only one member in the bond is active at any time, a different
> member becomes active if, and only if, the primary active member fails,
> thereby providing fault tolerance to member failure. The single logical
> bonding interface’s MAC address is externally visible on only one NIC (port)
> to avoid confusing the network switch.

- **Balance XOR (Mode 2):**


![../_images/bond-mode-2.svg](https://doc.dpdk.org/guides-25.03/_images/bond-mode-2.svg)

Fig. 2.8 Balance XOR (Mode 2)

> This mode provides transmit load balancing (based on the selected
> transmission policy) and fault tolerance. The default policy (layer2) uses
> a simple calculation based on the packet flow source and destination MAC
> addresses as well as the number of active members available to the bonding
> device to classify the packet to a specific member to transmit on. Alternate
> transmission policies supported are layer 2+3, this takes the IP source and
> destination addresses into the calculation of the transmit member port and
> the final supported policy is layer 3+4, this uses IP source and
> destination addresses as well as the TCP/UDP source and destination port.

Note

The coloring differences of the packets are used to identify different flow
classification calculated by the selected transmit policy

- **Broadcast (Mode 3):**


![../_images/bond-mode-3.svg](https://doc.dpdk.org/guides-25.03/_images/bond-mode-3.svg)

Fig. 2.9 Broadcast (Mode 3)

> This mode provides fault tolerance by transmission of packets on all member
> ports.

- **Link Aggregation 802.3AD (Mode 4):**


![../_images/bond-mode-4.svg](https://doc.dpdk.org/guides-25.03/_images/bond-mode-4.svg)

Fig. 2.10 Link Aggregation 802.3AD (Mode 4)

> This mode provides dynamic link aggregation according to the 802.3ad
> specification. It negotiates and monitors aggregation groups that share the
> same speed and duplex settings using the selected balance transmit policy
> for balancing outgoing traffic.
>
> DPDK implementation of this mode provide some additional requirements of
> the application.
>
> 1. It needs to call `rte_eth_tx_burst` and `rte_eth_rx_burst` with
> intervals period of less than 100ms.
>
> 2. Calls to `rte_eth_tx_burst` must have a buffer size of at least 2xN,
> where N is the number of members. This is a space required for LACP
> frames. Additionally LACP packets are included in the statistics, but
> they are not returned to the application.

- **Transmit Load Balancing (Mode 5):**


![../_images/bond-mode-5.svg](https://doc.dpdk.org/guides-25.03/_images/bond-mode-5.svg)

Fig. 2.11 Transmit Load Balancing (Mode 5)

> This mode provides an adaptive transmit load balancing. It dynamically
> changes the transmitting member, according to the computed load. Statistics
> are collected in 100ms intervals and scheduled every 10ms.

## 2.2. Implementation Details

The librte\_net\_bond bonding device is compatible with the Ethernet device API
exported by the Ethernet PMDs described in the _DPDK API Reference_.

The Link Bonding Library supports the creation of bonding devices at application
startup time during EAL initialization using the `--vdev` option as well as
programmatically via the C API `rte_eth_bond_create` function.

Bonding devices support the dynamical addition and removal of member devices using
the `rte_eth_bond_member_add` / `rte_eth_bond_member_remove` APIs.

After a member device is added to a bonding device member is stopped using
`rte_eth_dev_stop` and then reconfigured using `rte_eth_dev_configure`
the RX and TX queues are also reconfigured using `rte_eth_tx_queue_setup` /
`rte_eth_rx_queue_setup` with the parameters use to configure the bonding
device. If RSS is enabled for bonding device, this mode is also enabled on new
member and configured as well.
Any flow which was configured to the bond device also is configured to the added
member.

Setting up multi-queue mode for bonding device to RSS, makes it fully
RSS-capable, so all members are synchronized with its configuration. This mode is
intended to provide RSS configuration on members transparent for client
application implementation.

Bonding device stores its own version of RSS settings i.e. RETA, RSS hash
function and RSS key, used to set up its members. That let to define the meaning
of RSS configuration of bonding device as desired configuration of whole bonding
(as one unit), without pointing any of member inside. It is required to ensure
consistency and made it more error-proof.

RSS hash function set for bonding device, is a maximal set of RSS hash functions
supported by all bonding members. RETA size is a GCD of all its RETA’s sizes, so
it can be easily used as a pattern providing expected behavior, even if member
RETAs’ sizes are different. If RSS Key is not set for bonding device, it’s not
changed on the members and default key for device is used.

As RSS configurations, there is flow consistency in the bonding members for the
next rte flow operations:

Validate:

- Validate flow for each member, failure at least for one member causes to
bond validation failure.


Create:

- Create the flow in all members.

- Save all the members created flows objects in bonding internal flow
structure.

- Failure in flow creation for existed member rejects the flow.

- Failure in flow creation for new members in member adding time rejects
the member.


Destroy:

- Destroy the flow in all members and release the bond internal flow
memory.


Flush:

- Destroy all the bonding PMD flows in all the members.


Note

Don’t call members flush directly, It destroys all the member flows which
may include external flows or the bond internal LACP flow.

Query:

- Summarize flow counters from all the members, relevant only for
`RTE_FLOW_ACTION_TYPE_COUNT`.


Isolate:

- Call to flow isolate for all members.

- Failure in flow isolation for existed member rejects the isolate mode.

- Failure in flow isolation for new members in member adding time rejects
the member.


All settings are managed through the bonding port API and always are propagated
in one direction (from bonding to members).

### 2.2.1. Link Status Change Interrupts / Polling

Link bonding devices support the registration of a link status change callback,
using the `rte_eth_dev_callback_register` API, this will be called when the
status of the bonding device changes. For example in the case of a bonding
device which has 3 members, the link status will change to up when one member
becomes active or change to down when all members become inactive. There is no
callback notification when a single member changes state and the previous
conditions are not met. If a user wishes to monitor individual members then they
must register callbacks with that member directly.

The link bonding library also supports devices which do not implement link
status change interrupts, this is achieved by polling the devices link status at
a defined period which is set using the `rte_eth_bond_link_monitoring_set`
API, the default polling interval is 10ms. When a device is added as a member to
a bonding device it is determined using the `RTE_PCI_DRV_INTR_LSC` flag
whether the device supports interrupts or whether the link status should be
monitored by polling it.

### 2.2.2. Requirements / Limitations

The current implementation only supports devices that support the same speed
and duplex to be added as a members to the same bonding device. The bonding device
inherits these attributes from the first active member added to the bonding
device and then all further members added to the bonding device must support
these parameters.

A bonding device must have a minimum of one member before the bonding device
itself can be started.

To use a bonding device dynamic RSS configuration feature effectively, it is
also required, that all members should be RSS-capable and support, at least one
common hash function available for each of them. Changing RSS key is only
possible, when all member devices support the same key size.

To prevent inconsistency on how members process packets, once a device is added
to a bonding device, RSS and rte flow configurations should be managed through
the bonding device API, and not directly on the member.

Like all other PMD, all functions exported by a PMD are lock-free functions
that are assumed not to be invoked in parallel on different logical cores to
work on the same target object.

It should also be noted that the PMD receive function should not be invoked
directly on a member devices after they have been to a bonding device since
packets read directly from the member device will no longer be available to the
bonding device to read.

### 2.2.3. Configuration

Link bonding devices are created using the `rte_eth_bond_create` API
which requires a unique device name, the bonding mode,
and the socket Id to allocate the bonding device’s resources on.
The other configurable parameters for a bonding device are its member devices,
its primary member, a user defined MAC address and transmission policy to use if
the device is in balance XOR mode.

#### 2.2.3.1. Member Devices

Bonding devices support up to a maximum of `RTE_MAX_ETHPORTS` member devices
of the same speed and duplex. Ethernet devices can be added as a member to a
maximum of one bonding device. Member devices are reconfigured with the
configuration of the bonding device on being added to a bonding device.

The bonding also guarantees to return the MAC address of the member device to its
original value of removal of a member from it.

#### 2.2.3.2. Primary Member

The primary member is used to define the default port to use when a bonding
device is in active backup mode. A different port will only be used if, and
only if, the current primary port goes down. If the user does not specify a
primary port it will default to being the first port added to the bonding device.

#### 2.2.3.3. MAC Address

The bonding device can be configured with a user specified MAC address, this
address will be inherited by the some/all member devices depending on the
operating mode. If the device is in active backup mode then only the primary
device will have the user specified MAC, all other members will retain their
original MAC address. In mode 0, 2, 3, 4 all members devices are configure with
the bonding devices MAC address.

If a user defined MAC address is not defined then the bonding device will
default to using the primary members MAC address.

#### 2.2.3.4. Balance XOR Transmit Policies

There are 3 supported transmission policies for bonding device running in
Balance XOR mode. Layer 2, Layer 2+3, Layer 3+4.

- **Layer 2:** Ethernet MAC address based balancing is the default
transmission policy for Balance XOR bonding mode. It uses a simple XOR
calculation on the source MAC address and destination MAC address of the
packet and then calculate the modulus of this value to calculate the member
device to transmit the packet on.

- **Layer 2 + 3:** Ethernet MAC address & IP Address based balancing uses a
combination of source/destination MAC addresses and the source/destination
IP addresses of the data packet to decide which member port the packet will
be transmitted on.

- **Layer 3 + 4:** IP Address & UDP Port based balancing uses a combination
of source/destination IP Address and the source/destination UDP ports of
the packet of the data packet to decide which member port the packet will be
transmitted on.


All these policies support 802.1Q VLAN Ethernet packets, as well as IPv4, IPv6
and UDP protocols for load balancing.

## 2.3. Using Link Bonding Devices

The librte\_net\_bond library supports two modes of device creation, the libraries
export full C API or using the EAL command line to statically configure link
bonding devices at application startup. Using the EAL option it is possible to
use link bonding functionality transparently without specific knowledge of the
libraries API, this can be used, for example, to add bonding functionality,
such as active backup, to an existing application which has no knowledge of
the link bonding C API.

### 2.3.1. Using the Poll Mode Driver from an Application

Using the librte\_net\_bond libraries API it is possible to dynamically create
and manage link bonding device from within any application. Link bonding
devices are created using the `rte_eth_bond_create` API which requires a
unique device name, the link bonding mode to initial the device in and finally
the socket Id which to allocate the devices resources onto. After successful
creation of a bonding device it must be configured using the generic Ethernet
device configure API `rte_eth_dev_configure` and then the RX and TX queues
which will be used must be setup using `rte_eth_tx_queue_setup` /
`rte_eth_rx_queue_setup`.

Member devices can be dynamically added and removed from a link bonding device
using the `rte_eth_bond_member_add` / `rte_eth_bond_member_remove`
APIs but at least one member device must be added to the link bonding device
before it can be started using `rte_eth_dev_start`.

The link status of a bonding device is dictated by that of its members, if all
member device link status are down or if all members are removed from the link
bonding device then the link status of the bonding device will go down.

It is also possible to configure / query the configuration of the control
parameters of a bonding device using the provided APIs
`rte_eth_bond_mode_set/ get`, `rte_eth_bond_primary_set/get`,
`rte_eth_bond_mac_set/reset` and `rte_eth_bond_xmit_policy_set/get`.

### 2.3.2. Using Link Bonding Devices from the EAL Command Line

Link bonding devices can be created at application startup time using the
`--vdev` EAL command line option. The device name must start with the
net\_bonding prefix followed by numbers or letters. The name must be unique for
each device. Each device can have multiple options arranged in a comma
separated list. Multiple devices definitions can be arranged by calling the
`--vdev` option multiple times.

Device names and bonding options must be separated by commas as shown below:

```
./<build_dir>/app/dpdk-testpmd -l 0-3 -n 4 --vdev 'net_bonding0,bond_opt0=..,bond opt1=..'--vdev 'net_bonding1,bond _opt0=..,bond_opt1=..'

```

#### 2.3.2.1. Link Bonding EAL Options

There are multiple ways of definitions that can be assessed and combined as
long as the following two rules are respected:

- A unique device name, in the format of net\_bondingX is provided,
where X can be any combination of numbers and/or letters,
and the name is no greater than 32 characters long.

- A least one member device is provided with for each bonding device definition.

- The operation mode of the bonding device being created is provided.


The different options are:

- mode: Integer value defining the bonding mode of the device.
Currently supports modes 0,1,2,3,4,5 (round-robin, active backup, balance,
broadcast, link aggregation, transmit load balancing).


```
mode=2

```

- member: Defines the PMD device which will be added as member to the bonding
device. This option can be selected multiple times, for each device to be
added as a member. Physical devices should be specified using their PCI
address, in the format domain:bus:devid.function


```
member=0000:0a:00.0,member=0000:0a:00.1

```

- primary: Optional parameter which defines the primary member port,
is used in active backup mode to select the primary member for data TX/RX if
it is available. The primary port also is used to select the MAC address to
use when it is not defined by the user. This defaults to the first member
added to the device if it is specified. The primary device must be a member
of the bonding device.


```
primary=0000:0a:00.0

```

- socket\_id: Optional parameter used to select which socket on a NUMA device
the bonding devices resources will be allocated on.


```
socket_id=0

```

- mac: Optional parameter to select a MAC address for link bonding device,
this overrides the value of the primary member device.


```
mac=00:1e:67:1d:fd:1d

```

- xmit\_policy: Optional parameter which defines the transmission policy when
the bonding device is in balance mode. If not user specified this defaults
to l2 (layer 2) forwarding, the other transmission policies available are
l23 (layer 2+3) and l34 (layer 3+4)


```
xmit_policy=l23

```

- lsc\_poll\_period\_ms: Optional parameter which defines the polling interval
in milli-seconds at which devices which don’t support lsc interrupts are
checked for a change in the devices link status


```
lsc_poll_period_ms=100

```

- up\_delay: Optional parameter which adds a delay in milli-seconds to the
propagation of a devices link status changing to up, by default this
parameter is zero.


```
up_delay=10

```

- down\_delay: Optional parameter which adds a delay in milli-seconds to the
propagation of a devices link status changing to down, by default this
parameter is zero.


```
down_delay=50

```

#### 2.3.2.2. Examples of Usage

Create a bonding device in round robin mode with two members specified by their PCI address:

```
./<build_dir>/app/dpdk-testpmd -l 0-3 -n 4 --vdev 'net_bonding0,mode=0,member=0000:0a:00.01,member=0000:04:00.00' -- --port-topology=chained

```

Create a bonding device in round robin mode with two members specified by their PCI address and an overriding MAC address:

```
./<build_dir>/app/dpdk-testpmd -l 0-3 -n 4 --vdev 'net_bonding0,mode=0,member=0000:0a:00.01,member=0000:04:00.00,mac=00:1e:67:1d:fd:1d' -- --port-topology=chained

```

Create a bonding device in active backup mode with two members specified, and a primary member specified by their PCI addresses:

```
./<build_dir>/app/dpdk-testpmd -l 0-3 -n 4 --vdev 'net_bonding0,mode=1,member=0000:0a:00.01,member=0000:04:00.00,primary=0000:0a:00.01' -- --port-topology=chained

```

Create a bonding device in balance mode with two members specified by their PCI addresses, and a transmission policy of layer 3 + 4 forwarding:

```
./<build_dir>/app/dpdk-testpmd -l 0-3 -n 4 --vdev 'net_bonding0,mode=2,member=0000:0a:00.01,member=0000:04:00.00,xmit_policy=l34' -- --port-topology=chained

```

## 2.4. Testpmd driver specific commands

Some bonding driver specific features are integrated in testpmd.

### 2.4.1. create bonding device

Create a new bonding device:

```
testpmd> create bonding device (mode) (socket)

```

For example, to create a bonding device in mode 1 on socket 0:

```
testpmd> create bonding device 1 0
created new bonding device (port X)

```

### 2.4.2. add bonding member

Adds Ethernet device to a Link Bonding device:

```
testpmd> add bonding member (member id) (port id)

```

For example, to add Ethernet device (port 6) to a Link Bonding device (port 10):

```
testpmd> add bonding member 6 10

```

### 2.4.3. remove bonding member

Removes an Ethernet member device from a Link Bonding device:

```
testpmd> remove bonding member (member id) (port id)

```

For example, to remove Ethernet member device (port 6) to a Link Bonding device (port 10):

```
testpmd> remove bonding member 6 10

```

### 2.4.4. set bonding mode

Set the Link Bonding mode of a Link Bonding device:

```
testpmd> set bonding mode (value) (port id)

```

For example, to set the bonding mode of a Link Bonding device (port 10) to broadcast (mode 3):

```
testpmd> set bonding mode 3 10

```

### 2.4.5. set bonding primary

Set an Ethernet member device as the primary device on a Link Bonding device:

```
testpmd> set bonding primary (member id) (port id)

```

For example, to set the Ethernet member device (port 6) as the primary port of a Link Bonding device (port 10):

```
testpmd> set bonding primary 6 10

```

### 2.4.6. set bonding mac

Set the MAC address of a Link Bonding device:

```
testpmd> set bonding mac (port id) (mac)

```

For example, to set the MAC address of a Link Bonding device (port 10) to 00:00:00:00:00:01:

```
testpmd> set bonding mac 10 00:00:00:00:00:01

```

### 2.4.7. set bonding balance\_xmit\_policy

Set the transmission policy for a Link Bonding device when it is in Balance XOR mode:

```
testpmd> set bonding balance_xmit_policy (port_id) (l2|l23|l34)

```

For example, set a Link Bonding device (port 10) to use a balance policy of layer 3+4 (IP addresses & UDP ports):

```
testpmd> set bonding balance_xmit_policy 10 l34

```

### 2.4.8. set bonding mon\_period

Set the link status monitoring polling period in milliseconds for a bonding device.

This adds support for PMD member devices which do not support link status interrupts.
When the mon\_period is set to a value greater than 0 then all PMD’s which do not support
link status ISR will be queried every polling interval to check if their link status has changed:

```
testpmd> set bonding mon_period (port_id) (value)

```

For example, to set the link status monitoring polling period of bonding device (port 5) to 150ms:

```
testpmd> set bonding mon_period 5 150

```

### 2.4.9. set bonding lacp dedicated\_queue

Enable dedicated tx/rx queues on bonding devices members to handle LACP control plane traffic
when in mode 4 (link-aggregation-802.3ad):

```
testpmd> set bonding lacp dedicated_queues (port_id) (enable|disable)

```

### 2.4.10. set bonding agg\_mode

Enable one of the specific aggregators mode when in mode 4 (link-aggregation-802.3ad):

```
testpmd> set bonding agg_mode (port_id) (bandwidth|count|stable)

```

### 2.4.11. show bonding config

Show the current configuration of a Link Bonding device,
it also shows link-aggregation-802.3ad information if the link mode is mode 4:

```
testpmd> show bonding config (port id)

```

For example,
to show the configuration a Link Bonding device (port 9) with 3 member devices (1, 3, 4)
in balance mode with a transmission policy of layer 2+3:

```
testpmd> show bonding config 9
  - Dev basic:
     Bonding mode: BALANCE(2)
     Balance Xmit Policy: BALANCE_XMIT_POLICY_LAYER23
     Members (3): [1 3 4]
     Active Members (3): [1 3 4]
     Primary: [3]

```


================================================================================
FILE: doc.dpdk.org_guides-25.03_prog_guide_mempool_lib.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/prog_guide/mempool_lib.html"
title: "2. Memory Pool Library — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Programmer’s Guide](https://doc.dpdk.org/guides-25.03/prog_guide/index.html)
- 2\. Memory Pool Library
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/prog_guide/mempool_lib.rst.txt)

* * *

# 2\. Memory Pool Library

A memory pool is an allocator of a fixed-sized object.
In the DPDK, it is identified by name and uses a mempool handler to store free objects.
The default mempool handler is ring based.
It provides some other optional services such as a per-core object cache and
an alignment helper to ensure that objects are padded to spread them equally on all DRAM or DDR3 channels.

This library is used by the [Packet (Mbuf) Library](https://doc.dpdk.org/guides-25.03/prog_guide/mbuf_lib.html).

## 2.1. Cookies

In debug mode, cookies are added at the beginning and end of allocated blocks.
The allocated objects then contain overwrite protection fields to help debugging buffer overflows.

Debug mode is disabled by default,
but can be enabled by setting `RTE_LIBRTE_MEMPOOL_DEBUG` in `config/rte_config.h`.

## 2.2. Stats

In stats mode, statistics about get from/put in the pool are stored in the mempool structure.
Statistics are per-lcore to avoid concurrent access to statistics counters.

Stats mode is disabled by default,
but can be enabled by setting `RTE_LIBRTE_MEMPOOL_STATS` in `config/rte_config.h`.

## 2.3. Memory Alignment Constraints on x86 architecture

Depending on hardware memory configuration on X86 architecture, performance can be greatly improved by adding a specific padding between objects.
The objective is to ensure that the beginning of each object starts on a different channel and rank in memory so that all channels are equally loaded.

This is particularly true for packet buffers when doing L3 forwarding or flow classification.
Only the first 64 bytes are accessed, so performance can be increased by spreading the start addresses of objects among the different channels.

The number of ranks on any DIMM is the number of independent sets of DRAMs that can be accessed for the full data bit-width of the DIMM.
The ranks cannot be accessed simultaneously since they share the same data path.
The physical layout of the DRAM chips on the DIMM itself does not necessarily relate to the number of ranks.

When running an application, the EAL command line options provide the ability to add the number of memory channels and ranks.

Note

The command line must always have the number of memory channels specified for the processor.

Examples of alignment for different DIMM architectures are shown in
[Fig. 2.2](https://doc.dpdk.org/guides-25.03/prog_guide/mempool_lib.html#figure-memory-management) and [Fig. 2.3](https://doc.dpdk.org/guides-25.03/prog_guide/mempool_lib.html#figure-memory-management2).

![../_images/memory-management.svg](https://doc.dpdk.org/guides-25.03/_images/memory-management.svg)

Fig. 2.2 Two Channels and Quad-ranked DIMM Example

In this case, the assumption is that a packet is 16 blocks of 64 bytes, which is not true.

The Intel® 5520 chipset has three channels, so in most cases,
no padding is required between objects (except for objects whose size are n x 3 x 64 bytes blocks).

![../_images/memory-management2.svg](https://doc.dpdk.org/guides-25.03/_images/memory-management2.svg)

Fig. 2.3 Three Channels and Two Dual-ranked DIMM Example

When creating a new pool, the user can specify to use this feature or not.

Note

This feature is not present for Arm systems.
Modern Arm Interconnects choose the SN-F (memory channel)
using a hash of memory address bits.
As a result, the load is distributed evenly in all cases,
including the above described, rendering this feature unnecessary.

## 2.4. Local Cache

In terms of CPU usage, the cost of multiple cores accessing a memory pool’s ring of free buffers may be high
since each access requires a compare-and-set (CAS) operation.
To avoid having too many access requests to the memory pool’s ring,
the memory pool allocator can maintain a per-core cache and do bulk requests to the memory pool’s ring,
via the cache with many fewer locks on the actual memory pool structure.
In this way, each core has full access to its own cache (with locks) of free objects and
only when the cache fills does the core need to shuffle some of the free objects back to the pools ring or
obtain more objects when the cache is empty.

While this may mean a number of buffers may sit idle on some core’s cache,
the speed at which a core can access its own cache for a specific memory pool without locks provides performance gains.

The cache is composed of a small, per-core table of pointers and its length (used as a stack).
This internal cache can be enabled or disabled at creation of the pool.

The maximum size of the cache is static and is defined at compilation time (RTE\_MEMPOOL\_CACHE\_MAX\_SIZE).

[Fig. 2.4](https://doc.dpdk.org/guides-25.03/prog_guide/mempool_lib.html#figure-mempool) shows a cache in operation.

![../_images/mempool.svg](https://doc.dpdk.org/guides-25.03/_images/mempool.svg)

Fig. 2.4 A mempool in Memory with its Associated Ring

Alternatively to the internal default per-lcore local cache, an application can create and manage external caches through the `rte_mempool_cache_create()`, `rte_mempool_cache_free()` and `rte_mempool_cache_flush()` calls.
These user-owned caches can be explicitly passed to `rte_mempool_generic_put()` and `rte_mempool_generic_get()`.
The `rte_mempool_default_cache()` call returns the default internal cache if any.
In contrast to the default caches, user-owned caches can be used by unregistered non-EAL threads too.

## 2.5. Mempool Handlers

This allows external memory subsystems, such as external hardware memory
management systems and software based memory allocators, to be used with DPDK.

There are two aspects to a mempool handler.

- Adding the code for your new mempool operations (ops). This is achieved by
adding a new mempool ops code, and using the `RTE_MEMPOOL_REGISTER_OPS` macro.

- Using the new API to call `rte_mempool_create_empty()` and
`rte_mempool_set_ops_byname()` to create a new mempool and specifying which
ops to use.


Several different mempool handlers may be used in the same application. A new
mempool can be created by using the `rte_mempool_create_empty()` function,
then using `rte_mempool_set_ops_byname()` to point the mempool to the
relevant mempool handler callback (ops) structure.

Legacy applications may continue to use the old `rte_mempool_create()` API
call, which uses a ring based mempool handler by default. These applications
will need to be modified to use a new mempool handler.

For applications that use `rte_pktmbuf_create()`, there is a config setting
( `RTE_MBUF_DEFAULT_MEMPOOL_OPS`) that allows the application to make use of
an alternative mempool handler.

> Note
>
> When running a DPDK application with shared libraries, mempool handler
> shared objects specified with the ‘-d’ EAL command-line parameter are
> dynamically loaded. When running a multi-process application with shared
> libraries, the -d arguments for mempool handlers _must be specified in the_
> _same order for all processes_ to ensure correct operation.

## 2.6. Use Cases

All allocations that require a high level of performance should use a pool-based memory allocator.
Below are some examples:

- [Packet (Mbuf) Library](https://doc.dpdk.org/guides-25.03/prog_guide/mbuf_lib.html)

- Any application that needs to allocate fixed-sized objects in the data plane and that will be continuously utilized by the system.


================================================================================
FILE: doc.dpdk.org_guides-25.03_prog_guide_metrics_lib.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/prog_guide/metrics_lib.html"
title: "9. Metrics Library — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Programmer’s Guide](https://doc.dpdk.org/guides-25.03/prog_guide/index.html)
- 9\. Metrics Library
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/prog_guide/metrics_lib.rst.txt)

* * *

# 9\. Metrics Library

The Metrics library implements a mechanism by which _producers_ can
publish numeric information for later querying by _consumers_. In
practice producers will typically be other libraries or primary
processes, whereas consumers will typically be applications.

Metrics themselves are statistics that are not generated by PMDs. Metric
information is populated using a push model, where producers update the
values contained within the metric library by calling an update function
on the relevant metrics. Consumers receive metric information by querying
the central metric data, which is held in shared memory.

For each metric, a separate value is maintained for each port id, and
when publishing metric values the producers need to specify which port is
being updated. In addition there is a special id `RTE_METRICS_GLOBAL`
that is intended for global statistics that are not associated with any
individual device. Since the metrics library is self-contained, the only
restriction on port numbers is that they are less than `RTE_MAX_ETHPORTS`
\- there is no requirement for the ports to actually exist.

## 9.1. Initializing the library

Before the library can be used, it has to be initialized by calling
`rte_metrics_init()` which sets up the metric store in shared memory.
This is where producers will publish metric information to, and where
consumers will query it from.

```
rte_metrics_init(rte_socket_id());

```

This function **must** be called from a primary process, but otherwise
producers and consumers can be in either primary or secondary processes.

## 9.2. Registering metrics

Metrics must first be _registered_, which is the way producers declare
the names of the metrics they will be publishing. Registration can either
be done individually, or a set of metrics can be registered as a group.
Individual registration is done using `rte_metrics_reg_name()`:

```
id_1 = rte_metrics_reg_name("mean_bits_in");
id_2 = rte_metrics_reg_name("mean_bits_out");
id_3 = rte_metrics_reg_name("peak_bits_in");
id_4 = rte_metrics_reg_name("peak_bits_out");

```

or alternatively, a set of metrics can be registered together using
`rte_metrics_reg_names()`:

```
const char * const names[] = {
    "mean_bits_in", "mean_bits_out",
    "peak_bits_in", "peak_bits_out",
};
id_set = rte_metrics_reg_names(&names[0], 4);

```

If the return value is negative, it means registration failed. Otherwise
the return value is the _key_ for the metric, which is used when updating
values. A table mapping together these key values and the metrics’ names
can be obtained using `rte_metrics_get_names()`.

## 9.3. Updating metric values

Once registered, producers can update the metric for a given port using
the `rte_metrics_update_value()` function. This uses the metric key
that is returned when registering the metric, and can also be looked up
using `rte_metrics_get_names()`.

```
rte_metrics_update_value(port_id, id_1, values[0]);
rte_metrics_update_value(port_id, id_2, values[1]);
rte_metrics_update_value(port_id, id_3, values[2]);
rte_metrics_update_value(port_id, id_4, values[3]);

```

if metrics were registered as a single set, they can either be updated
individually using `rte_metrics_update_value()`, or updated together
using the `rte_metrics_update_values()` function:

```
rte_metrics_update_value(port_id, id_set, values[0]);
rte_metrics_update_value(port_id, id_set + 1, values[1]);
rte_metrics_update_value(port_id, id_set + 2, values[2]);
rte_metrics_update_value(port_id, id_set + 3, values[3]);

rte_metrics_update_values(port_id, id_set, values, 4);

```

Note that `rte_metrics_update_values()` cannot be used to update
metric values from _multiple_ _sets_, as there is no guarantee two
sets registered one after the other have contiguous id values.

## 9.4. Querying metrics

Consumers can obtain metric values by querying the metrics library using
the `rte_metrics_get_values()` function that return an array of
`struct rte_metric_value`. Each entry within this array contains a metric
value and its associated key. A key-name mapping can be obtained using the
`rte_metrics_get_names()` function that returns an array of
`struct rte_metric_name` that is indexed by the key. The following will
print out all metrics for a given port:

```
void print_metrics() {
    struct rte_metric_value *metrics;
    struct rte_metric_name *names;
    int len;

    len = rte_metrics_get_names(NULL, 0);
    if (len < 0) {
        printf("Cannot get metrics count\n");
        return;
    }
    if (len == 0) {
        printf("No metrics to display (none have been registered)\n");
        return;
    }
    metrics = malloc(sizeof(struct rte_metric_value) * len);
    names =  malloc(sizeof(struct rte_metric_name) * len);
    if (metrics == NULL || names == NULL) {
        printf("Cannot allocate memory\n");
        free(metrics);
        free(names);
        return;
    }
    ret = rte_metrics_get_values(port_id, metrics, len);
    if (ret < 0 || ret > len) {
        printf("Cannot get metrics values\n");
        free(metrics);
        free(names);
        return;
    }
    printf("Metrics for port %i:\n", port_id);
    for (i = 0; i < len; i++)
        printf("  %s: %"PRIu64"\n",
            names[metrics[i].key].name, metrics[i].value);
    free(metrics);
    free(names);
}

```

## 9.5. Deinitialising the library

Once the library usage is done, it must be deinitialized by calling
`rte_metrics_deinit()` which will free the shared memory reserved
during initialization.

```
err = rte_metrics_deinit(void);

```

If the return value is negative, it means deinitialization failed.
This function **must** be called from a primary process.

## 9.6. Bit-rate statistics library

The bit-rate library calculates the exponentially-weighted moving
average and peak bit-rates for each active port (i.e. network device).
These statistics are reported via the metrics library using the
following names:

> - `mean_bits_in`: Average inbound bit-rate
>
> - `mean_bits_out`: Average outbound bit-rate
>
> - `ewma_bits_in`: Average inbound bit-rate (EWMA smoothed)
>
> - `ewma_bits_out`: Average outbound bit-rate (EWMA smoothed)
>
> - `peak_bits_in`: Peak inbound bit-rate
>
> - `peak_bits_out`: Peak outbound bit-rate

Once initialised and clocked at the appropriate frequency, these
statistics can be obtained by querying the metrics library.

### 9.6.1. Initialization

Before the library can be used, it has to be initialised by calling
`rte_stats_bitrate_create()`, which will return a bit-rate
calculation object. Since the bit-rate library uses the metrics library
to report the calculated statistics, the bit-rate library then needs to
register the calculated statistics with the metrics library. This is
done using the helper function `rte_stats_bitrate_reg()`.

```
struct rte_stats_bitrates *bitrate_data;

bitrate_data = rte_stats_bitrate_create();
if (bitrate_data == NULL)
    rte_exit(EXIT_FAILURE, "Could not allocate bit-rate data.\n");
rte_stats_bitrate_reg(bitrate_data);

```

### 9.6.2. Controlling the sampling rate

Since the library works by periodic sampling but does not use an
internal thread, the application has to periodically call
`rte_stats_bitrate_calc()`. The frequency at which this function
is called should be the intended sampling rate required for the
calculated statistics. For instance if per-second statistics are
desired, this function should be called once a second.

```
tics_datum = rte_rdtsc();
tics_per_1sec = rte_get_timer_hz();

while( 1 ) {
    /* ... */
    tics_current = rte_rdtsc();
    if (tics_current - tics_datum >= tics_per_1sec) {
        /* Periodic bitrate calculation */
        for (idx_port = 0; idx_port < cnt_ports; idx_port++)
                rte_stats_bitrate_calc(bitrate_data, idx_port);
            tics_datum = tics_current;
        }
    /* ... */
}

```

## 9.7. Latency statistics library

The latency statistics library calculates the latency of packet
processing by a DPDK application, reporting the minimum, average,
and maximum nano-seconds that packet processing takes, as well as
the jitter in processing delay. These statistics are then reported
via the metrics library using the following names:

> - `min_latency_ns`: Minimum processing latency (nano-seconds)
>
> - `avg_latency_ns`: Average processing latency (nano-seconds)
>
> - `mac_latency_ns`: Maximum processing latency (nano-seconds)
>
> - `jitter_ns`: Variance in processing latency (nano-seconds)

Once initialised and clocked at the appropriate frequency, these
statistics can be obtained by querying the metrics library.

### 9.7.1. Initialization

Before the library can be used, it has to be initialised by calling
`rte_latencystats_init()`.

```
lcoreid_t latencystats_lcore_id = -1;

int ret = rte_latencystats_init(1, NULL);
if (ret)
    rte_exit(EXIT_FAILURE, "Could not allocate latency data.\n");

```

### 9.7.2. Triggering statistic updates

The `rte_latencystats_update()` function needs to be called
periodically so that latency statistics can be updated.

```
if (latencystats_lcore_id == rte_lcore_id())
    rte_latencystats_update();

```

### 9.7.3. Library shutdown

When finished, `rte_latencystats_uninit()` needs to be called to
de-initialise the latency library.

```
rte_latencystats_uninit();

```

### 9.7.4. Timestamp and latency calculation

The Latency stats library marks the time in the timestamp field of the
mbuf for the ingress packets and sets the `RTE_MBUF_F_RX_TIMESTAMP` flag of
`ol_flags` for the mbuf to indicate the marked time as a valid one.
At the egress, the mbufs with the flag set are considered having valid
timestamp and are used for the latency calculation.


================================================================================
FILE: doc.dpdk.org_guides-25.03_prog_guide_overview.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/prog_guide/overview.html"
title: "2. Overview — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Programmer’s Guide](https://doc.dpdk.org/guides-25.03/prog_guide/index.html)
- 2\. Overview
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/prog_guide/overview.rst.txt)

* * *

# 2\. Overview

This section gives a global overview of the architecture of Data Plane Development Kit (DPDK).

The main goal of the DPDK is to provide a simple,
complete framework for fast packet processing in data plane applications.
Users may use the code to understand some of the techniques employed,
to build upon for prototyping or to add their own protocol stacks.
Alternative ecosystem options that use the DPDK are available.

The framework creates a set of libraries for specific environments
through the creation of an Environment Abstraction Layer (EAL),
which may be specific to a mode of the Intel® architecture (32-bit or 64-bit),
Linux\* user space compilers or a specific platform.
These environments are created through the use of meson files and configuration files.
Once the EAL library is created, the user may link with the library to create their own applications.
Other libraries, outside of EAL, including the Hash,
Longest Prefix Match (LPM) and rings libraries are also provided.
Sample applications are provided to help show the user how to use various features of the DPDK.

The DPDK implements a run to completion model for packet processing,
where all resources must be allocated prior to calling Data Plane applications,
running as execution units on logical processing cores.
The model does not support a scheduler and all devices are accessed by polling.
The primary reason for not using interrupts is the performance overhead imposed by interrupt processing.

In addition to the run-to-completion model,
a pipeline model may also be used by passing packets or messages between cores via the rings.
This allows work to be performed in stages and may allow more efficient use of code on cores.

## 2.1. Development Environment

The DPDK project installation requires Linux and the associated toolchain,
such as one or more compilers, assembler, meson utility,
editor and various libraries to create the DPDK components and libraries.

Once these libraries are created for the specific environment and architecture,
they may then be used to create the user’s data plane application.

When creating applications for the Linux user space, the glibc library is used.

See the _DPDK Getting Started Guide_ for information on setting up the development environment.

## 2.2. Environment Abstraction Layer

The Environment Abstraction Layer (EAL) provides a generic interface
that hides the environment specifics from the applications and libraries.
The services provided by the EAL are:

- DPDK loading and launching

- Support for multi-process and multi-thread execution types

- Core affinity/assignment procedures

- System memory allocation/de-allocation

- Atomic/lock operations

- Time reference

- PCI bus access

- Trace and debug functions

- CPU feature identification

- Interrupt handling

- Alarm operations

- Memory management (malloc)


The EAL is fully described in [Environment Abstraction Layer (EAL) Library](https://doc.dpdk.org/guides-25.03/prog_guide/env_abstraction_layer.html).

## 2.3. Core Components

The _core components_ are a set of libraries that provide all the elements needed
for high-performance packet processing applications.

![../_images/architecture-overview.svg](https://doc.dpdk.org/guides-25.03/_images/architecture-overview.svg)

Fig. 2.1 Core Components Architecture

### 2.3.1. Ring Manager (librte\_ring)

The ring structure provides a lockless multi-producer, multi-consumer FIFO API in a finite size table.
It has some advantages over lockless queues; easier to implement, adapted to bulk operations and faster.
A ring is used by the [Memory Pool Library](https://doc.dpdk.org/guides-25.03/prog_guide/mempool_lib.html)
and may be used as a general communication mechanism between cores
and/or execution blocks connected together on a logical core.

This ring buffer and its usage are fully described in [Ring Library](https://doc.dpdk.org/guides-25.03/prog_guide/ring_lib.html).

### 2.3.2. Memory Pool Manager (librte\_mempool)

The Memory Pool Manager is responsible for allocating pools of objects in memory.
A pool is identified by name and uses a ring to store free objects.
It provides some other optional services,
such as a per-core object cache and an alignment helper to ensure that objects are padded to spread them equally on all RAM channels.

This memory pool allocator is described in [Memory Pool Library](https://doc.dpdk.org/guides-25.03/prog_guide/mempool_lib.html).

### 2.3.3. Network Packet Buffer Management (librte\_mbuf)

The mbuf library provides the facility to create and destroy buffers
that may be used by the DPDK application to store message buffers.
The message buffers are created at startup time and stored in a mempool, using the DPDK mempool library.

This library provides an API to allocate/free mbufs, manipulate
packet buffers which are used to carry network packets.

Network Packet Buffer Management is described in [Packet (Mbuf) Library](https://doc.dpdk.org/guides-25.03/prog_guide/mbuf_lib.html).

### 2.3.4. Timer Manager (librte\_timer)

This library provides a timer service to DPDK execution units,
providing the ability to execute a function asynchronously.
It can be periodic function calls, or just a one-shot call.
It uses the timer interface provided by the Environment Abstraction Layer (EAL)
to get a precise time reference and can be initiated on a per-core basis as required.

The library documentation is available in [Timer Library](https://doc.dpdk.org/guides-25.03/prog_guide/timer_lib.html).

## 2.4. Ethernet\* Poll Mode Driver Architecture

The DPDK includes Poll Mode Drivers (PMDs) for 1 GbE, 10 GbE and 40GbE, and para virtualized virtio
Ethernet controllers which are designed to work without asynchronous, interrupt-based signaling mechanisms.

## 2.5. Packet Forwarding Algorithm Support

The DPDK includes Hash (librte\_hash) and Longest Prefix Match (LPM,librte\_lpm)
libraries to support the corresponding packet forwarding algorithms.

See [Hash Library](https://doc.dpdk.org/guides-25.03/prog_guide/hash_lib.html) and [Longest Prefix Match (LPM) Library](https://doc.dpdk.org/guides-25.03/prog_guide/lpm_lib.html) for more information.

## 2.6. librte\_net

The librte\_net library is a collection of IP protocol definitions and convenience macros.
It is based on code from the FreeBSD\* IP stack and contains protocol numbers (for use in IP headers),
IP-related macros, IPv4/IPv6 header structures and TCP, UDP and SCTP header structures.


================================================================================
FILE: doc.dpdk.org_guides-25.03_prog_guide_power_man.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/prog_guide/power_man.html"
title: "2. Power Management — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Programmer’s Guide](https://doc.dpdk.org/guides-25.03/prog_guide/index.html)
- 2\. Power Management
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/prog_guide/power_man.rst.txt)

* * *

# 2\. Power Management

The DPDK Power Management feature allows users space applications to save power
by dynamically adjusting CPU frequency or entering into different C-States.

- Adjusting the CPU frequency dynamically according to the utilization of RX queue.

- Entering into different deeper C-States according to the adaptive algorithms to speculate
brief periods of time suspending the application if no packets are received.


The interfaces for adjusting the operating CPU frequency are in the power management library.
C-State control is implemented in applications according to the different use cases.

## 2.1. CPU Frequency Scaling

The Linux kernel provides a cpufreq module for CPU frequency scaling for each lcore.
For example, for cpuX, /sys/devices/system/cpu/cpuX/cpufreq/ has the following sys files for frequency scaling:

- affected\_cpus

- bios\_limit

- cpuinfo\_cur\_freq

- cpuinfo\_max\_freq

- cpuinfo\_min\_freq

- cpuinfo\_transition\_latency

- related\_cpus

- scaling\_available\_frequencies

- scaling\_available\_governors

- scaling\_cur\_freq

- scaling\_driver

- scaling\_governor

- scaling\_max\_freq

- scaling\_min\_freq

- scaling\_setspeed


In the DPDK, scaling\_governor is configured in user space.
Then, a user space application can prompt the kernel by writing scaling\_setspeed to adjust the CPU frequency
according to the strategies defined by the user space application.

## 2.2. Core-load Throttling through C-States

Core state can be altered by speculative sleeps whenever the specified lcore has nothing to do.
In the DPDK, if no packet is received after polling,
speculative sleeps can be triggered according the strategies defined by the user space application.

## 2.3. Per-core Turbo Boost

Individual cores can be allowed to enter a Turbo Boost state on a per-core
basis. This is achieved by enabling Turbo Boost Technology in the BIOS, then
looping through the relevant cores and enabling/disabling Turbo Boost on each
core.

## 2.4. Use of Power Library in a Hyper-Threaded Environment

In the case where the power library is in use on a system with Hyper-Threading enabled,
the frequency on the physical core is set to the highest frequency of the Hyper-Thread siblings.
So even though an application may request a scale down, the core frequency will
remain at the highest frequency until all Hyper-Threads on that core request a scale down.

## 2.5. API Overview of the Power Library

The main methods exported by power library are for CPU frequency scaling and include the following:

- **Freq up**: Prompt the kernel to scale up the frequency of the specific lcore.

- **Freq down**: Prompt the kernel to scale down the frequency of the specific lcore.

- **Freq max**: Prompt the kernel to scale up the frequency of the specific lcore to the maximum.

- **Freq min**: Prompt the kernel to scale down the frequency of the specific lcore to the minimum.

- **Get available freqs**: Read the available frequencies of the specific lcore from the sys file.

- **Freq get**: Get the current frequency of the specific lcore.

- **Freq set**: Prompt the kernel to set the frequency for the specific lcore.

- **Enable turbo**: Prompt the kernel to enable Turbo Boost for the specific lcore.

- **Disable turbo**: Prompt the kernel to disable Turbo Boost for the specific lcore.


## 2.6. User Cases

The power management mechanism is used to save power when performing L3 forwarding.

## 2.7. PM QoS

The `/sys/devices/system/cpu/cpuX/power/pm_qos_resume_latency_us`
sysfs interface is used to set and get the resume latency limit
on the cpuX for userspace.
Each cpuidle governor in Linux selects which idle state to enter
based on this CPU resume latency in their idle task.

The deeper the idle state, the lower the power consumption,
but the longer the resume time.
Some services are latency sensitive and request a low resume time,
like interrupt packet receiving mode.

Applications can set and get the CPU resume latency with
`rte_power_qos_set_cpu_resume_latency()`
and `rte_power_qos_get_cpu_resume_latency()` respectively.
Applications can set a strict resume latency (zero value)
to lower the resume latency and get better performance
(instead, the power consumption of platform may increase).

## 2.8. Ethernet PMD Power Management API

### 2.8.1. Abstract

Existing power management mechanisms require developers to change application
design or change code to make use of it. The PMD power management API provides a
convenient alternative by utilizing Ethernet PMD RX callbacks, and triggering
power saving whenever empty poll count reaches a certain number.

- Monitor

This power saving scheme will put the CPU into optimized power state and
monitor the Ethernet PMD RX descriptor address, waking the CPU up whenever
there’s new traffic. Support for this scheme may not be available on all
platforms, and further limitations may apply (see below).

- Pause

This power saving scheme will avoid busy polling by either entering
power-optimized sleep state with `rte_power_pause()` function, or, if it’s
not supported by the underlying platform, use `rte_pause()`.

- Frequency scaling

This power saving scheme will use `librte_power` library functionality to
scale the core frequency up/down depending on traffic volume.
The reaction time of the frequency scaling mode is longer
than the pause and monitor mode.


The “monitor” mode is only supported in the following configurations and scenarios:

- On Linux\* x86\_64, rte\_power\_monitor() requires WAITPKG instruction set being
supported by the CPU, while rte\_power\_monitor\_multi() requires WAITPKG and
RTM instruction sets being supported by the CPU. RTM instruction set may also
require booting the Linux with tsx=on command line parameter. Please refer
to your platform documentation for further information.

- If `rte_cpu_get_intrinsics_support()` function indicates that
`rte_power_monitor_multi()` function is supported by the platform, then
monitoring multiple Ethernet Rx queues for traffic will be supported.

- If `rte_cpu_get_intrinsics_support()` function indicates that only
`rte_power_monitor()` is supported by the platform, then monitoring will be
limited to a mapping of 1 core 1 queue (thus, each Rx queue will have to be
monitored from a different lcore).

- If `rte_cpu_get_intrinsics_support()` function indicates that neither of the
two monitoring functions are supported, then monitor mode will not be supported.

- Not all Ethernet drivers support monitoring, even if the underlying
platform may support the necessary CPU instructions. Please refer to
[Overview of Networking Drivers](https://doc.dpdk.org/guides-25.03/nics/overview.html) for more information.


### 2.8.2. API Overview for Ethernet PMD Power Management

- **Queue Enable**: Enable specific power scheme for certain queue/port/core.

- **Queue Disable**: Disable power scheme for certain queue/port/core.

- **Get Emptypoll Max**: Get the configured number of empty polls to wait before
entering sleep state.

- **Set Emptypoll Max**: Set the number of empty polls to wait before entering
sleep state.

- **Get Pause Duration**: Get the configured duration (microseconds) to be used
in the Pause callback.

- **Set Pause Duration**: Set the duration of the pause (microseconds) used in
the Pause mode callback.

- **Get Scaling Min Freq**: Get the configured minimum frequency (kHz) to be used
in Frequency Scaling mode.

- **Set Scaling Min Freq**: Set the minimum frequency (kHz) to be used in Frequency
Scaling mode.

- **Get Scaling Max Freq**: Get the configured maximum frequency (kHz) to be used
in Frequency Scaling mode.

- **Set Scaling Max Freq**: Set the maximum frequency (kHz) to be used in Frequency
Scaling mode.


## 2.9. Uncore API

### 2.9.1. Abstract

Uncore is a term used by Intel to describe the functions of a microprocessor
that are not in the core, but which must be closely connected to the core
to achieve high performance: L3 cache, on-die memory controller, etc.
Significant power savings can be achieved by reducing the uncore frequency
to its lowest value.

### 2.9.2. Intel Uncore

The Linux kernel provides the driver “intel-uncore-frequency”
to control the uncore frequency limits for x86 platform.
The driver is available from kernel version 5.6 and above.
Also CONFIG\_INTEL\_UNCORE\_FREQ\_CONTROL will need to be enabled in the kernel,
which was added in 5.6.
This manipulates the context of MSR 0x620,
which sets min/max of the uncore for the SKU.

### 2.9.3. AMD EPYC Uncore

On AMD EPYC platforms, the Host System Management Port (HSMP) kernel module
facilitates user-level access to HSMP mailboxes,
which are implemented by the firmware in the System Management Unit (SMU).
The AMD HSMP driver is available starting from kernel version 5.18.
Please ensure that `CONFIG_AMD_HSMP` is enabled in your kernel configuration.

Additionally, the EPYC System Management Interface In-band Library for Linux
offers essential API, enabling user-space software
to effectively manage system functions.

### 2.9.4. Uncore API Overview

Overview of each function in the Uncore API,
with explanation of what they do.
Each function should not be called in the fast path.

Uncore Power Init

Initialize uncore power, populate frequency array
and record original min & max for die on pkg.

Uncore Power Exit

Exit uncore power, restoring original min & max for die on pkg.

Get Uncore Power Freq

Get current uncore freq index for die on pkg.

Set Uncore Power Freq

Set min & max uncore freq index for die on pkg
to specified index value (min and max will be the same).

Uncore Power Max

Set min & max uncore freq to maximum frequency index for die on pkg
(min and max will be the same).

Uncore Power Min

Set min & max uncore freq to minimum frequency index for die on pkg
(min and max will be the same).

Get Num Freqs

Get the number of frequencies in the index array.

Get Num Pkgs

Get the number of packages (CPU’s) on the system.

Get Num Dies

Get the number of die’s on a given package.

## 2.10. References

- The [L3 Forwarding with Power Management Sample Application](https://doc.dpdk.org/guides-25.03/sample_app_ug/l3_forward_power_man.html)
chapter in the [Sample Applications User Guides](https://doc.dpdk.org/guides-25.03/sample_app_ug/index.html) section.

- The [Virtual Machine Power Management Application](https://doc.dpdk.org/guides-25.03/sample_app_ug/vm_power_management.html)
chapter in the [Sample Applications User Guides](https://doc.dpdk.org/guides-25.03/sample_app_ug/index.html) section.

- The [Overview of Networking Drivers](https://doc.dpdk.org/guides-25.03/nics/overview.html) chapter in the [Network Interface Controller Drivers](https://doc.dpdk.org/guides-25.03/nics/index.html) section


================================================================================
FILE: doc.dpdk.org_guides-25.03_rel_notes_release_25_03.html.md
================================================================================

---
url: "https://doc.dpdk.org/guides-25.03/rel_notes/release_25_03.html"
title: "1. DPDK Release 25.03 — Data Plane Development Kit 25.03.0 documentation"
---

- [Home](https://doc.dpdk.org/guides-25.03/index.html)
- [Release Notes](https://doc.dpdk.org/guides-25.03/rel_notes/index.html)
- 1\. DPDK Release 25.03
- [View page source](https://doc.dpdk.org/guides-25.03/_sources/rel_notes/release_25_03.rst.txt)

* * *

# 1\. DPDK Release 25.03

## 1.1. New Features

- **Added Staged-Ordered-Ring (SORING) API to the ring library.**

Added new API to the ring library to provide a software abstraction
for ordered queues with multiple processing stages.
It is based on the conventional DPDK `rte_ring` and re-uses many of its concepts,
including substantial part of its code.
It can be viewed as an extension of `rte_ring` functionality.

- **Hardened more allocation functions.**

Added allocation attributes to functions that allocate data:


  - `rte_acl_create()`

  - `rte_comp_op_pool_create()`

  - `rte_event_ring_create()`

  - `rte_fbk_hash_create()`

  - `rte_fib_create()`

  - `rte_fib6_create()`

  - `rte_hash_create()`

  - `rte_lpm_create()`

  - `rte_lpm6_create()`

  - `rte_member_create()`

  - `rte_mempool_create()`

  - `rte_mempool_create_empty()`

  - `rte_reorder_create()`

  - `rte_rib_create()`

  - `rte_rib6_create()`

  - `rte_ring_create()`

  - `rte_sched_port_config()`

  - `rte_stats_bitrate_create()`

  - `rte_tel_data_alloc()`


This can catch some obvious bugs at compile time (with GCC 11.0 or later).
For example, calling `free` on a pointer that was allocated with one
of those functions (and vice versa), freeing the same pointer twice
in the same routine or freeing an object that was not created by allocation.

- **Updated af\_packet net driver.**

  - Added ability to configure receive packet fanout mode.

  - Added statistics for failed buffer allocation and missed packets.
- **Updated Amazon ENA (Elastic Network Adapter) net driver.**

  - Added support for mutable RSS table size based on device capabilities.
- **Updated AMD axgbe driver.**

  - Added support for the TCP Segmentation Offload (TSO).
- **Updated Intel e1000 driver.**

  - Added support for the Intel i225-series NICs (previously handled by net/igc).

  - Updated base code to the latest version.
- **Updated Intel ipdf driver.**

  - Added support for AVX2 instructions in single queue Rx and Tx path.
    (The single queue model processes all packets in order within one Rx queue,
    while the split queue model separates packet data and metadata into different queues
    for parallel processing and improved performance.)
- **Updated Marvell cnxk net driver.**

  - Added flow rules support for CN20K SoC.

  - Added inline IPsec support for CN20K SoC.
- **Updated Napatech ntnic driver.**

  - Added support for the NT400D13 adapter.
- **Updated NVIDIA mlx5 driver.**

  - Added support for NVIDIA ConnectX-8 adapters.

  - Optimized large scale port probing.
    This feature enhances the efficiency of probing VF/SFs on a large scale
    by significantly reducing the probing time.
- **Updated Wangxun ngbe driver.**

  - Added support for virtual function (VF).
- **Updated ZTE zxdh network driver.**

  - Added support for multiple queues.

  - Added support for SR-IOV VF.

  - Scatter and gather for Tx and Rx.

  - Link state and auto-negotiation.

  - MAC address filtering.

  - Multicast and promiscuous mode.

  - VLAN filtering and offload.

  - Receive Side Scaling (RSS).

  - Hardware and extended statistics.

  - Jumbo frames.

  - Checksum offload.

  - LRO and TSO.

  - Ingress metering.
- **Added Yunsilicon xsc net driver \[EXPERIMENTAL\].**

Added network driver for the Yunsilicon metaScale serials NICs.

- **Updated vhost/virtio for RSA crypto.**

  - Added RSA crypto operations to the vhost library.

  - Added RSA crypto operations to the virtio crypto driver.
- **Updated IPsec\_MB crypto driver.**

  - Added support for the SM4 GCM algorithm.
- **Added ZTE Storage Data Accelerator (ZSDA) driver.**

Added a compress driver for ZSDA devices
to support the deflate compression and decompression algorithm.

See the [ZTE Storage Data Accelerator (ZSDA) Poll Mode Driver](https://doc.dpdk.org/guides-25.03/compressdevs/zsda.html) guide for more details on the new driver.

- **Added atomic tests to the eventdev test application.**

Added two atomic tests: `atomic_queue` and `atomic_atq`.
These work in the same way as the corresponding ordered tests
but use atomic queues exclusively.
Atomicity is verified using spinlocks.


## 1.2. Removed Items

- **Dropped support for Intel\|reg\| C++ Compiler (icc) (replaced by “icx” support).**

Support for the older Intel® C++ Compiler “icc” has been dropped.
The newer Intel® oneAPI DPC++/C++ Compiler, “icx”, can be used to compile DPDK instead.


## 1.3. API Changes

- eal: The `__rte_packed` macro for packing data is replaced with
`__rte_packed_begin` / `__rte_packed_end`.

- eal: The `__rte_weak` macro is deprecated and will be removed in a future release.

- net: Changed the API for CRC calculation to be thread-safe.
An opaque context argument was introduced to the net CRC API
containing the algorithm type and length.
This argument is added to `rte_net_crc_calc`, `rte_net_crc_set_alg`
and freed with `rte_net_crc_free`.
These functions are versioned to retain binary compatibility until the next LTS release.

- build: The Intel networking drivers:
cpfl, e1000, fm10k, i40e, iavf, ice, idpf, ipn3ke and ixgbe,
have been moved from `drivers/net` to a new `drivers/net/intel` directory.
The resulting build output, including the driver filenames, is the same,
but to enable/disable these drivers via Meson option requires the use of the new paths.
For example, `-Denable_drivers=/net/i40e` becomes `-Denable_drivers=/net/intel/i40e`.

- build: The Intel IGC networking driver was merged with the e1000 driver
and is no longer provided as a separate driver.
The resulting build output will no longer have the `librte_net_igc.*` driver files,
but the `librte_net_e1000.*` driver files will provide support
for all of the devices and features of the old driver.
In addition, to enable/disable the driver via Meson option,
the path has changed from `-Denable_drivers=net/igc`
to `-Denable_drivers=net/intel/e1000`.

- build: The driver `common/idpf` has been merged into the `net/intel/idpf` driver.
Similarly, the `common/iavf` driver has been merged into the `net/intel/iavf` driver.
These changes should have no impact to end applications, but,
when specifying the `idpf` or `cpfl` net drivers to Meson via `-Denable_drivers` option,
there is no longer any need to also specify the `common/idpf` driver.
In the same way, when specifying the `iavf` or `ice` net drivers,
there is no need to also specify the `common/iavf` driver.
Note, however, `net/intel/cpfl` driver now depends upon the `net/intel/idpf` driver.


## 1.4. ABI Changes

- No ABI change that would break compatibility with 24.11.


## 1.5. Tested Platforms

- Intel® platforms with Intel® NICs combinations

  - CPU

    - Intel® Atom™ x7835RE

    - Intel® Xeon® CPU E5-2699 v4 @ 2.20GHz

    - Intel® Xeon® Gold 6139 CPU @ 2.30GHz

    - Intel® Xeon® Gold 6140M CPU @ 2.30GHz

    - Intel® Xeon® Gold 6252N CPU @ 2.30GHz

    - Intel® Xeon® Gold 6348 CPU @ 2.60GHz

    - Intel® Xeon® Platinum 8180 CPU @ 2.50GHz

    - Intel® Xeon® Platinum 8280M CPU @ 2.70GHz

    - Intel® Xeon® Platinum 8358 CPU @ 2.60GHz

    - Intel® Xeon® Platinum 8380 CPU @ 2.30GHz

    - Intel® Xeon® Platinum 8468H

    - Intel® Xeon® Platinum 8490H
  - OS:

    - Microsoft Azure Linux 3.0

    - Fedora 41

    - FreeBSD 14.2

    - OpenAnolis OS 8.9

    - openEuler 24.03 (LTS)

    - Red Hat Enterprise Linux Server release 9.0

    - Red Hat Enterprise Linux Server release 9.4

    - Red Hat Enterprise Linux Server release 9.5

    - Ubuntu 22.04.3 LTS

    - Ubuntu 22.04.4 LTS

    - Ubuntu 22.04.5 LTS

    - Ubuntu 24.04.1 LTS
  - NICs:

    - Intel® Ethernet Controller E810-C for SFP (4x25G)

      - Firmware version: 4.70 0x8001f79e 1.3755.0

      - Device id (pf/vf): 8086:1593 / 8086:1889

      - Driver version(out-tree): 1.16.3 (ice)

      - Driver version(in-tree): 6.8.0-48-generic (Ubuntu24.04.1) /
        5.14.0-503.11.1.el9\_5.x86\_64 (RHEL9.5) (ice)

      - OS Default DDP: 1.3.39.1

      - COMMS DDP: 1.3.53.0

      - Wireless Edge DDP: 1.3.14.0
    - Intel® Ethernet Controller E810-C for QSFP (2x100G)

      - Firmware version: 4.70 0x8001f7b8 1.3755.0

      - Device id (pf/vf): 8086:1592 / 8086:1889

      - Driver version(out-tree): 1.16.3 (ice)

      - Driver version(in-tree): 6.6.12.1-1.azl3+ice+ (Microsoft Azure Linux 3.0) (ice)

      - OS Default DDP: 1.3.39.1

      - COMMS DDP: 1.3.53.0

      - Wireless Edge DDP: 1.3.14.0
    - Intel® Ethernet Controller E810-XXV for SFP (2x25G)

      - Firmware version: 4.70 0x8001f7ba 1.3755.0

      - Device id (pf/vf): 8086:159b / 8086:1889

      - Driver version: 1.16.3 (ice)

      - OS Default DDP: 1.3.39.1

      - COMMS DDP: 1.3.53.0
    - Intel® Ethernet Network Adapter E830-XXVDA2 for OCP

      - Firmware version: 1.00 0x800107e3 1.3766.0

      - Device id (pf/vf): 8086:12d3 / 8086:1889

      - Driver version: 1.16.3 (ice)

      - OS Default DDP: 1.3.39.1
    - Intel® Ethernet Network Adapter E830-CQDA2

      - Firmware version: 1.00 0x800107ca 1.3766.0

      - Device id (pf/vf): 8086:12d2 / 8086:1889

      - Driver version: 1.16.3 (ice)

      - OS Default DDP: 1.3.39.1

      - COMMS DDP: 1.3.53.0

      - Wireless Edge DDP: 1.3.19.0
    - Intel® Ethernet Connection E825-C for QSFP

      - Firmware version: 3.81 0x8000674f 1.0.0

      - Device id (pf/vf): 8086:579d / 8086:1889

      - Driver version: 2.1.0 (ice)

      - OS Default DDP: 1.3.39.2

      - COMMS DDP: 1.3.54.0

      - Wireless Edge DDP: 1.3.22.0
    - Intel® 82599ES 10 Gigabit Ethernet Controller

      - Firmware version: 0x000161bf

      - Device id (pf/vf): 8086:10fb / 8086:10ed

      - Driver version(out-tree): 6.0.6 (ixgbe)

      - Driver version(in-tree): 6.8.0-48-generic (Ubuntu24.04.1) (ixgbe)
    - Intel® Ethernet Network Adapter E610-XT2

      - Firmware version: 1.00 0x800066ae 0.0.0

      - Device id (pf/vf): 8086:57b0 / 8086:57ad

      - Driver version(out-tree): 6.0.6 (ixgbe)
    - Intel® Ethernet Network Adapter E610-XT4

      - Firmware version: 1.00 0x80004ef2 0.0.0

      - Device id (pf/vf): 8086:57b0 / 8086:57ad

      - Driver version(out-tree): 6.0.6 (ixgbe)
    - Intel® Ethernet Converged Network Adapter X710-DA4 (4x10G)

      - Firmware version: 9.53 0x8000f8f5 1.3755.0

      - Device id (pf/vf): 8086:1572 / 8086:154c

      - Driver version(out-tree): 2.27.8 (i40e)
    - Intel® Corporation Ethernet Connection X722 for 10GbE SFP+ (2x10G)

      - Firmware version: 6.50 0x80004216 1.3597.0

      - Device id (pf/vf): 8086:37d0 / 8086:37cd

      - Driver version(out-tree): 2.27.8 (i40e)

      - Driver version(in-tree): 5.14.0-427.13.1.el9\_4.x86\_64 (RHEL9.4)(i40e)
    - Intel® Ethernet Converged Network Adapter XXV710-DA2 (2x25G)

      - Firmware version: 9.53 0x8000f912 1.3755.0

      - Device id (pf/vf): 8086:158b / 8086:154c

      - Driver version(out-tree): 2.27.8 (i40e)

      - Driver version(in-tree): 6.8.0-45-generic (Ubuntu24.04.1) /
        5.14.0-427.13.1.el9\_4.x86\_64 (RHEL9.4)(i40e)
    - Intel® Ethernet Converged Network Adapter XL710-QDA2 (2X40G)

      - Firmware version(PF): 9.53 0x8000f8f5 1.3755.0

      - Device id (pf/vf): 8086:1583 / 8086:154c

      - Driver version(out-tree): 2.27.8 (i40e)
    - Intel® Ethernet Controller I225-LM

      - Firmware version: 1.3, 0x800000c9

      - Device id (pf): 8086:15f2

      - Driver version(in-tree): 6.8.0-48-generic (Ubuntu24.04.1)(igc)
    - Intel® Ethernet Controller (2) I225-IT

      - Firmware version: 1.82, 0x8000026C

      - Device id (pf): 8086:0d9f

      - Driver version(in-tree): 6.6.25-lts-240422t024020z (Ubuntu22.04.4 LTS)(igc)
    - Intel® Ethernet Controller I226-LM

      - Firmware version: 2.14, 0x8000028c

      - Device id (pf): 8086:125b

      - Driver version(in-tree): 6.8.0-45-generic (Ubuntu24.04.1)(igc)
    - Intel® Corporation Ethernet Server Adapter I350-T4

      - Firmware version: 1.63, 0x80001001

      - Device id (pf/vf): 8086:1521 /8086:1520

      - Driver version: 6.6.25-lts-240422t024020z(igb)
    - Intel® Infrastructure Processing Unit (Intel® IPU) E2100

      - Firmware version: CI-9231

      - Device id (idpf/cpfl): 8086:1452/8086:1453

      - Driver version: 0.0.754 (idpf)
- Intel® platforms with NVIDIA® NICs combinations

  - CPU:

    - Intel® Xeon® Gold 6154 CPU @ 3.00GHz

    - Intel® Xeon® CPU E5-2697A v4 @ 2.60GHz

    - Intel® Xeon® CPU E5-2697 v3 @ 2.60GHz

    - Intel® Xeon® CPU E5-2680 v2 @ 2.80GHz

    - Intel® Xeon® CPU E5-2670 0 @ 2.60GHz

    - Intel® Xeon® CPU E5-2650 v4 @ 2.20GHz

    - Intel® Xeon® CPU E5-2650 v3 @ 2.30GHz

    - Intel® Xeon® CPU E5-2640 @ 2.50GHz

    - Intel® Xeon® CPU E5-2650 0 @ 2.00GHz

    - Intel® Xeon® CPU E5-2620 v4 @ 2.10GHz
  - OS:

    - Red Hat Enterprise Linux release 9.1 (Plow)

    - Red Hat Enterprise Linux release 8.6 (Ootpa)

    - Red Hat Enterprise Linux release 8.4 (Ootpa)

    - Ubuntu 22.04

    - Ubuntu 20.04

    - SUSE Enterprise Linux 15 SP2
  - DOCA:

    - DOCA 2.10.0-0.5.3 and above
  - upstream kernel:

    - Linux 6.12.0 and above
  - rdma-core:

    - rdma-core-54.0 and above
  - NICs

    - NVIDIA® ConnectX®-6 Dx EN 100G MCX623106AN-CDAT (2x100G)

      - Host interface: PCI Express 4.0 x16

      - Device ID: 15b3:101d

      - Firmware version: 22.44.1036 and above
    - NVIDIA® ConnectX®-6 Lx EN 25G MCX631102AN-ADAT (2x25G)

      - Host interface: PCI Express 4.0 x8

      - Device ID: 15b3:101f

      - Firmware version: 26.44.1036 and above
    - NVIDIA® ConnectX®-7 200G CX713106AE-HEA\_QP1\_Ax (2x200G)

      - Host interface: PCI Express 5.0 x16

      - Device ID: 15b3:1021

      - Firmware version: 28.44.1036 and above
    - NVIDIA® ConnectX®-8 SuperNIC 400G MT4131 - 900-9X81Q-00CN-STA (2x400G)

      - Host interface: PCI Express 6.0 x16

      - Device ID: 15b3:1023

      - Firmware version: 40.44.1036 and above
- NVIDIA® BlueField® SmartNIC

  - NVIDIA® BlueField®-2 SmartNIC MT41686 - MBF2H332A-AEEOT\_A1 (2x25G)

    - Host interface: PCI Express 3.0 x16

    - Device ID: 15b3:a2d6

    - Firmware version: 24.44.1036 and above
  - NVIDIA® BlueField®-3 P-Series DPU MT41692 - 900-9D3B6-00CV-AAB (2x200G)

    - Host interface: PCI Express 5.0 x16

    - Device ID: 15b3:a2dc

    - Firmware version: 32.44.1036 and above
  - Embedded software:

    - Ubuntu 22.04

    - MLNX\_OFED 25.01-0.6.0.0

    - bf-bundle-2.10.0-147\_25.01\_ubuntu-22.04

    - DPDK application running on ARM cores
- IBM Power 9 platforms with NVIDIA® NICs combinations


  - CPU:

    - POWER9 2.2 (pvr 004e 1202)
  - OS:

    - Ubuntu 20.04
  - NICs:

    - NVIDIA® ConnectX®-6 Dx 100G MCX623106AN-CDAT (2x100G)

      - Host interface: PCI Express 4.0 x16

      - Device ID: 15b3:101d

      - Firmware version: 22.44.1036 and above
    - NVIDIA® ConnectX®-7 200G CX713106AE-HEA\_QP1\_Ax (2x200G)

      - Host interface: PCI Express 5.0 x16

      - Device ID: 15b3:1021

      - Firmware version: 28.44.1036 and above

> - DOCA:
>
>
> > - DOCA 2.10.0-0.5.3 and above

